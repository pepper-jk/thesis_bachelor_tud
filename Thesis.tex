%% This is file `DEMO-TUDaThesis.tex' version 2.09 (2020/03/13),
%% it is part of
%% TUDa-CI -- Corporate Design for TU Darmstadt
%% ----------------------------------------------------------------------------
%%
%%  Copyright (C) 2018--2020 by Marei Peischl <marei@peitex.de>
%%
%% ============================================================================
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2008/05/04 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainers of this work are
%%   Marei Peischl <tuda-ci@peitex.de>
%%   Markus Lazanowski <latex@ce.tu-darmstadt.de>
%%
%% The development respository can be found at
%% https://github.com/tudace/tuda_latex_templates
%% Please use the issue tracker for feedback!
%%
%% ============================================================================
%%
% !TeX program = lualatex
%%

\documentclass[
	ngerman,
	ruledheaders=section,%Ebene bis zu der die Überschriften mit Linien abgetrennt werden, vgl. DEMO-TUDaPub
	class=report,% Basisdokumentenklasse. Wählt die Korrespondierende KOMA-Script Klasse
	thesis={type=bachelor},% Dokumententyp Thesis, für Dissertationen siehe die Demo-Datei DEMO-TUDaPhd
	accentcolor=9c,% Auswahl der Akzentfarbe
	custommargins=true,% Ränder werden mithilfe von typearea automatisch berechnet
	marginpar=false,% Kopfzeile und Fußzeile erstrecken sich nicht über die Randnotizspalte
	%BCOR=5mm,%Bindekorrektur, falls notwendig
	parskip=half-,%Absatzkennzeichnung durch Abstand vgl. KOMA-Sript
	fontsize=11pt,%Basisschriftgröße laut Corporate Design ist mit 9pt häufig zu klein
%	logofile=example-image, %Falls die Logo Dateien nicht vorliegen
]{tudapub}

% Der folgende Block ist nur bei pdfTeX auf Versionen vor April 2018 notwendig
\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}%kompatibilität mit TeX Versionen vor April 2018
\fi

%%%%%%%%%%%%%%%%%%%
%Sprachanpassung & Verbesserte Trennregeln
%%%%%%%%%%%%%%%%%%%
\usepackage[main=english, ngerman]{babel}
\usepackage[autostyle]{csquotes}% Anführungszeichen vereinfacht
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
%Literaturverzeichnis
%%%%%%%%%%%%%%%%%%%
\usepackage{biblatex}   % Literaturverzeichnis
%\bibliography{DEMO-TUDaBibliography}

% TODO: fix biblatex cites
\addbibresource{research.bib}
%\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Tabellen
%%%%%%%%%%%%%%%%%%%
%\usepackage{array}     % Basispaket für Tabellenkonfiguration, wird von den folgenden automatisch geladen
\usepackage{tabularx}   % Tabellen, die sich automatisch der Breite anpassen
%\usepackage{longtable} % Mehrseitige Tabellen
%\usepackage{xltabular} % Mehrseitige Tabellen mit anpassarer Breite
\usepackage{booktabs}   % Verbesserte Möglichkeiten für Tabellenlayout über horizontale Linien
\usepackage{multirow}
\usepackage{hhline}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Mathematik
%%%%%%%%%%%%%%%%%%%
%\usepackage{mathtools} % erweiterte Fassung von amsmath
%\usepackage{amssymb}   % erweiterter Zeichensatz
%\usepackage{siunitx}   % Einheiten

%Formatierungen für Beispiele in diesem Dokument. Im Allgemeinen nicht notwendig!
\let\file\texttt
\let\code\texttt
\let\tbs\textbackslash

\usepackage{pifont}% Zapf-Dingbats Symbole
\newcommand*{\FeatureTrue}{\ding{52}}
\newcommand*{\FeatureFalse}{\ding{56}}

\begin{document}

\Metadata{
	title=Discriminating if a network flow could have been created from a given sequence of network packets,
	author=Jens Keim
}

\title{Discriminating if a network flow could have been created from a given sequence of network packets}
\subtitle{subtitle}
\author[J. Keim]{Jens Keim}%optionales Argument ist die Signatur,
\birthplace{Worms}%Geburtsort, bei Dissertationen zwingend notwendig
\reviewer{Prof. Dr. Max M{\"u}hlh{\"a}user \and Dr. Carlos G. Cordero}%Gutachter

%Diese Felder erden untereinander auf der Titelseite platziert.
%\department ist eine notwendige Angabe, siehe auch dem Abschnitt `Abweichung von den Vorgaben für die Titelseite'
% TODO: trusted systems? as carlos or in matrix
\department{inf} % Das Kürzel wird automatisch ersetzt und als Studienfach gewählt, siehe Liste der Kürzel im Dokument.
\institute{Telecooperation}
\group{SPIN}

\submissiondate{\today}
\examdate{\today}

%	\tuprints{urn=1234,printid=12345}
%	\dedication{Für alle, die \TeX{} nutzen.}

\maketitle

\affidavit

\tableofcontents

\chapter{Abstract}

\chapter{Introduction}

\section{Motivation}

Network intrusion detection systems (NIDSs) require datasets to be trained and tested.
However, appropriate datasets for NIDSs are hard to come by
since creating authentic synthetic datasets is difficult and time-consuming
and most real traffic is rarely shared due to privacy concerns \cite{ringFlowbasedNetworkTraffic2019a} or copyright \cite{corderoID2TDIYDataset2015}.
This is why researchers in the field of NIDSs are restricted to use datasets with known defects or to create their own datasets.
% Remember that the last sentence of each paragraph should ease the way for the next paragraph.

% Creating synthetic network traffic is essential to provide this kind of datasets.
A researcher that chooses to create their own datasets may use real network traffic, synthetic network traffic, or a mixture of both.
Using real traffic to create a dataset, although desirable, has many disadvantages.
The capture might be too old to represent current networks or
% The network topology recorded, e.g. NAT, % Why is this a problem?
% or the used bandwidth. % not clear what you mean
it may contain artifacts.
Synthetic network traffic is hard to create because network traffic is diverse.
It is influenced by many factors, like
the countless terminals communicating,
the interim devices, e.g. switches, and their bandwidth,
gateways, subnets, and churn of terminals.
To generate appropriate datasets one would need to simulate all those factors and their interactions.
Since this is particularly complicated,
this is where the mixture of real and synthetic traffic comes in.
With this method, one takes an already existing network packet capture and modifies it to fit their needs.
%The idea is good, but the sentence needs work.
% You also need to be carefull and not mix these two ideas:
% 1) Mixing synthetic data with real to create a new dataset.
% 2) Create a complete synthetic dataset from scratch using real traffic as reference.
% Remember that with the GAN technique, we may create completely synthetic traffic that is not mixed with "real" traffic.
This modification can range from adding specific packets, e.g. of an network attack\cite{corderoID2TDIYDataset2015},
to modifying existing packets to fit a new network topology
or altering other significant features of the network behavior by modifying each packet.
Even though this method is quite effective, its design and implementation are also time-consuming.
Since it is difficult to create realistic network packet captures from scratch,
one could propose to create these traffic captures based on network flows.

The paper ''Flow-based Network Traffic Generation using Generative Adversarial Networks'' by Markus Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} provides a method to create synthetic network flows that mimic a given set of network flows.
%This enables researchers to create network flows based on real network flows.
Building on top of this method one could synthetically create labeled datasets of authentic network flows for NIDSs.
However, it is still desirable to have traffic based datasets, since they provide more information than flows.
Instead one could build on top of the method to create synthetic network flows proposed by Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} and
use the already authentic flows to create authentic synthetic network traffic at the packet level.
One approach to this could be a Generative Adversarial Network (GAN) that generates network packet captures based on network flows.

A GAN consists of two NNs that compete against each other to maximize the quality of the generated data:
a generator network (GN) and a discriminative network (DN).
The GN is constantly trying to fool the DN with data it generates synthetically,
while the DN is constantly trying to figure out if the data it was given is real or synthetic.
If correctly optimized, a GAN may create synthetic data indistinguishable from real data.

\section{Problem}

If we would want to propose a GAN that can create the packets from which a network flow was constructed,
three major challenges need to be addressed.
One, the DN needs to be defined such that it can determine if a network flow could have been created by a sequence of network packets.
Two, a GN needs to be able to create network packet captures that are indistinguishable (in some statistical sense) from the packets the network flow was created from.
Three, the training of the GAN and the required datasets.
To create a GAN, both the DN and GN need to perform well on their own.

In this research, we focus on building a NN that can be used in future work as the DN of a GAN.
It is not obvious, however, how to create a DN that
can verify that a network flow was created from a sequence of packets.
This leads to the research questions presented next.

\section{Research Questions}

This thesis will attempt to answer the following research questions.

\begin{itemize}
  \item How can we develop a neural network that acts as a discriminator for a GAN
  that can determine if a network flow could have been created by a sequence of network packets.
  \item Which features of network flows and packets are (most) significant to determine if a network flow could have been created by a sequence of network packets?
  \item Which NN architecture would be most suitable to distinguish if network packets could have created some network flow?
\end{itemize}

\section{Goals and Objectives}

% Was wird bei der Diplomarbeit inhaltlich erwartet und bewertet?

% I picture three main goals for your thesis (you need to either agree or propose something different):
% x 1) Create a dataset (or program to make datasets) to test your theories.
% x 2) Figure out what is needed to distinguish if packets belong to a certain flow, and
% x 3) Come up with an architecture capable of making correct decisions.

This thesis focuses on creating a Neural Network (NN) model
that can be used as the DN of a GAN
that takes network flows as input and produces network packets as output.
The NN gets an array of network packets and at least one network flow as input
and provides the probability that a specific network flow was created from the given sequence of packets.
To achieve this goal, we might need to consider additional input,
such as labels for either the network flows, the network packets, or both.
Therefore the goal of this thesis is to create a DN model that distinguishes
if a specific network flow could have been created from a given sequence of packets, or not.

It will not only be necessary to create the NN architecture,
but also the datasets needed to train and test the system.
For this, we need suitable representations of network flows and packets that a NN can process.
To test the reliability of our DN, diverse datasets from sufficiently distinct networks need to be constructed.
% During the development of the NN model, these features will be adjusted to create appropriate datasets.
So to reach the goal, we need to reach the following objectives:

\begin{itemize}
  \item Collecting diverse packet captures, representing a wide range of networks.
  \item Extracting flows from said packet captures.
  \item Creating fake flows by modifying the extracted ones based on those features.
  \item Extracting features from packets and flows alike,
which can be used to distinguish if packets belong to a certain flow.
  \item Creating a NN model based on those features.
  \item Choosing a NN architecture that is capable of making correct decisions.
  \item Testing the effectiveness of the NN architectures to work as discriminators.
\end{itemize}

%%% THESIS TOPIC END

%\chapter{Requirements}

% Anmerkung von Prof. Mühlhäuser: "Voraussetzungen":
% kann ruhig Selbstverständlichkeiten wie erwünschte Programmierkenntnisse
% beinhalten. Man glaubt nicht, was man bei jedem 10ten (oder so) Diplomanden
% alles irgendwann vermissen wird :-).
% Außerdem kann man "Grundkenntnisse in ... (Themengebiet) wünschenswert
% und förderlich, aber nicht zwingend" hinschreiben, dann kann man
% (z.B. mündlich) argumentieren, dass ein Teil der Literatur vor
% Startschuss zu den 6 Monaten gelesen werden muss.

%\section{for the student}

%\begin{itemize}
%  \item basic familiarity with Python
%  \item basic familiarity with ID2T and Traffic Statistic Extraction
%  \item basic familiarity with yaf and Flow Extraction
%  \item basic familiarity with Tensorflow 2
%  \item basic knowledge of Neural Networks
%  \item broad understanding of RNNs
%\end{itemize}

%\section{for the architecture}

% how much detail?
% regarding the training of GANs/Discriminator
% OR later in Thesis?

%\subsection{Functional Requirements}
% what it should do

%\subsection{Non Functional Requirements}
% how it works

\chapter{Background}

% In every background "item", always start stating why the topic is important to know in the context of your thesis.

%\section{Network intrusion detection systems}
%To understand the motivation behind this thesis we must understand how NIDSs work.
%Intrusion detection systems (IDSs) describe hard- or software that monitors a network or system for malicious activity or policy violations.
%One differentiates between NIDSs and host-based intrusion detection systems (HIDSs).
%The former monitor a network, while the later a single system.
%
%Today NIDSs are crucial for network security.
%Therefore we focus on them.
%There are two different classifications of NIDSs:
%signature-based NIDSs and anomaly-based NIDSs.
%Signature-based systems are limited for several reasons:
%the availability of signatures,
%the growing threat of federated attacks that split the malicious signature between multiple gateways.
%
%This is where anomaly-based detection comes in.
%They classify network behavior as either normal or anomalous.
%This way even new attacks, for which no signature is yet provided,
%can be recognized by the system, since it is anomalous to the network behavior it knows about.
%These systems have two main sources of knowledge:
%the network they observe all the time
%and the datasets they get trained with.

\section{network flows}
% TODO

\section{Network traffic datasets}

This thesis aims to lay the groundwork for a new method of the creation of authentic synthetic network packet captures
that can be used for labeled datasets for training and testing NIDSs.
Anomaly-based NIDSs (ANIDS) need network traffic datasets for testing and training.
Those datasets need to be appropriate to the threat and labeled accordingly.
However appropriate datasets are hard to come by.
Most real traffic recordings are rarely shared due to privacy concerns \cite{ringFlowbasedNetworkTraffic2019a} or copyright \cite{corderoID2TDIYDataset2015}.
The datasets including network attacks,
which are shared publicly,
tend to either be snapshots from real attacks or synthetically created traces from an isolated network.
The later does not provide realistic non-attack traffic, or background traffic.
The former present the problem of non-attack traffic being of a unique network in a unique time frame and therefore not representative of every network.
Also, they most likely get anonymized and are therefore do not represent the ground truth.
Thus these datasets are not sufficient to train or test NIDS.
And no matter the origin the dataset will only describe the network and the behavior of its traffic at the time of recording or creation, hence it might not be applicable in future networks \cite{ringFlowbasedNetworkTraffic2019a}.

If we want to extract features from network flows
we first need to understand the difference between network flows and network traffic caputres.
A network packet capture is a recording of network traffic at the packet level.
It is limited by a start and end time.
Each packet within the capture contains all headers it contained during actual transmission.
The payload of the packets is omitted if its size gets too large to be reasonably stored otherwise, the packets are unaltered.
A network flow is an artificial logical equivalent to only one network connection \cite{brownleeTrafficFlowMeasurement}.
This connection may be between two terminals, a multicast group or a terminal and a broadcast address \cite{rajahalmeIPv6FlowLabel}.
Like a network packet capture, a flow is limited by a start and end time,
but it does not contain information on individual packets.
It describes the connection on a more abstract level.

% packets > flow - possible
% flow > packets - not possible yet

% NN, decision making, sequential (network) data

\section{Neural Networks}

\subsection{IP2Vec}

Network flows consist of multiple features, some of which are categorical, e.g. IP addresses, ports, and protocols.
However, NNs work best with continuous and numerical data.
To effectively train a NN we, therefore, need to represent those categorical features as numerical data.
For this we plan on using IP2Vec proposed by Ring et al. \cite{ringIP2VecLearningSimilarities2017}.
This solution produces similarity values for IP addresses based on the behavior of the host.

They based their work on the concept of word2Vec from the field of natural language processing.
There the word is observed in the context of its neighbors instead of alone.
Similarly, in IP2Vec IPs are not observed on their own, but on in the context of other features they correlate with.
But instead of words IP2Vec processes network flow attributes,
specifically source IP, destination IP, destination port and protocol.
However one could change those attributes and determine the difference between IPs based on different attributes.
With this approach they are able to determine the differences between IPs based on their communication patterns (aka similar behavior).
For example, a destination IP that correlates with port 80 often can be easily classified as an HTTP server.
Their experiments show that provided the right dataset, the data produced by IP2Vec can be used to distinguish between infected and non-infected hosts, as well as between servers, clients and even printers.

\subsection{Recurrent Neural Networks}

To solve the problem stated in this thesis we are required to find an appropriate NN architecture
that is capable of making correct decisions about network flows created from network packet captures.
Neural Networks usually process each input independently from previous inputs.
This behavior is not necessarily what you want depending on your problem.
Recurrent Neural Networks (RNNs) store previous results (output) of the NN and uses them to process the new input.
Natural language processing usually profits from this.
In theory, this ''memory'' can hold information about all previous calculations,
but in practice, it only keeps track of the last few steps.
% new hidden state = tanh(input + previous hidden state)
This is due to the vanishing gradient problem \cite{hochreiterLongShortTermMemory1997}.
%A basic RNN uses the hyperbolic tangent function

\subsection{Long Short Term Memory \& Gated Recurrent Units}

This is where Long Short Term Memory (LSTM) networks \cite{hochreiterLongShortTermMemory1997} or Gated Recurrent Units (GRUs), formerly known as gated recursive convolutional neural network (grConv) \cite{bahdanauNeuralMachineTranslation2016}, come in.
Both similarly tackle the short-term memory issue,
also known as the vanishing gradient problem \cite{hochreiterLongShortTermMemory1997}.
Where the weight of the memory is changed so little, that it becomes insignificant.
Both models provide a gated approach.
They also both rely on the Sigmoid function in addition to the hyperbolic tangent function.
While the hyperbolic tangent function produces values between -1 and 1,
the Sigmoid function produces values from 0 to 1.
This benefits the memory since low impact data is multiplied by 0 and therefore left out of the equation.

% LSTM
% new cell state = (previous cell state * forget gate) + input gate
% new hidden state = output gate * tanh(new cell state)
% gates:
% \begin{itemize}
%   \item forget gate: sigmoid(input + previous hidden state)
%   \item input gate: sigmoid(input + previous hidden state) * tanh(input + previous hidden state)
%   \item output gate: sigmoid(input + previous hidden state)
% \end{itemize}

% GRU
% new hidden state = (previous hidden state * update gate) + output gate
% gates:
% \begin{itemize}
%   \item reset gate: sigmoid(input + previous hidden state) * previous hidden state
%   \item update gate: 1 - sigmoid(input + previous hidden state)
%   \item ''output gate'': tanh(input + reset gate) * sigmoid(input + previous hidden state)
% \end{itemize}

\subsection{GANs}
% TODO

\chapter{Related Work}

\section{Flow-based Network Traffic Generation using Generative Adversarial Networks}

Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} introduce a GAN to generate synthetic network flows.
Not only is their work interesting as the source of input for the proposed GAN at a later time,
but as a resource regarding feature extraction for network flows,
which is needed by this thesis.
So even though Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} do not create network packet captures,
their choice of network flow features is immensely valuable to this thesis.

\section{Related Neural Network Models}

In addition, we will need to look into related work regarding NNs, RNNs and GANs, specifically DNs.
Since some approaches for other problems might be applicable to the problem of discriminating if a network flow could have been created from a given sequence of network packets.
This will become clearer once the feature sets of network flows and packets are defined.

% Approaches to generate synthetic network packets that did not use NNs are not really related work,
% since they will not face the same problems as this thesis, therefore they were omitted.

% Bidirectional RNN?

% TODO: search and add some more GANs network flow papaers

\chapter{Approach}

\section{Methodology}

For our DN, we assume that some form of Recurrent Neural Network (RNN) will be suitable as a discriminator of a GAN that can create synthetic network packet captures from network flows.
Since ''basic'' RNNs have the problem of short-term memory, we propose the use of either Long Short Term Memory (LSTM) networks \cite{hochreiterLongShortTermMemory1997} or Gated Recurrent Units (GRUs) \cite{bahdanauNeuralMachineTranslation2016}.
GRUs tend to use fewer operations and states to produce similar or better results than LSTMs.
But depending on the use case, LSTMs might still produce better results.
It is to be determined, which of the two neural networks provides better results for the problem in this thesis.

% TODO: how do we do this?
% TODO: what is the baseline for the datasets?
% TODO: what is the baseline for the NN?

\section{Dataset}

For our dataset we will extract packet sequences from publicly available and well-known network packet captures and create network flows based on these sequences.
Each network packet sequence will be paired with its network flow so that our NN can learn the connection between the two data structures.

Since the end goal in the future work GAN is to generate the complete packet sequences based of the flows,
each sequence of packets will contain as much of the original header information of each packet as needed to reconstruct the network packet they were extracted from.
This way, the DN is prepared to handle whole packet captures and compare them to flows.

Each header attribute is converted into an appropriate representation depending on its data type.
Since the NNs are most efficient with continuous and numerical data, we will convert all fields accordingly, if possible.
Also, we will need to implement a way to create negative samples from the ground truth, which builds our positive samples.
Both will be discussed further in the dataset preprocessing section.

\subsection{Dataset Preparation}

\subsubsection{Sampling}

% FIXME: wording matching/mismatching
The NN needs to be trained with a significant amount of data, both matching and mismatching samples, so that it can make informed decisions.
This will be provided by combining datasets.
As stated above we will take Data from popular datasets,
such as: CDX, DARPA, MACCDC, MAWI, Simpleweb and UNSW.

However instead of combining all of them into one unmanagable blob,
we will sample each dataset with multiple patterns.
This way we can decide later, which pattern yields the best results.
For our purpose there are basically two approaches to sampling a network packet caputre for flows.
Either by splitting the capture into intervals,
or by sampling through the packet sequences within the caputres.

% Splitting the capture into intervals is the less time intense alternative,
% however it yields the problem of truncated flows.
% However this should not be a problem,
% since a capture itself is truncated at beginning and end anyway.
% The second issue is, the length of the intervals.
% Should they be determined relative to the size of the capture or by a fixed length,
% While a fixed length has better comparability,
% the percentage is the less time-consuming.

% Extracting the packet sequences from a complete capture is the more time consuming,
% but might yield deeper results.
% In the sense that some flows might actually be complete
% and the variety of sequence length might yield better results later on.

% For either of the above options it is to be decided,
% if the sampling will be done sequentially or randomly.
% And if it is done randomly, it is to be decided, to which degree the sampling is randomized.
% Do we pick random intervals, or do we pick the sequence of packets within each interval at random.
% Do we pick every 20th packet sequence, or do we pick each packet sequence at random.

% We suggest a variety of patterns to choose the packet sequences taken into account,
% when creating the dataset.
% With adding a new meta boolean attribute \textbf{truncated},
% we also can mark the truncated flows.
% This will help the NN to separate the two categories of packet sequences.

\subsubsection{Creating Flows}

%To ensure contious datasets for our NN, we used IP2Vec to generate source IP based vectors that represent the similarity between IP addresses within a dataset.
We used YAF and yafscii to create flow based csv files, which we used as an input for IP2Vecs preprocessing.
% FIXME
This is the procedure for network flows as well as for packet data.

\subsection{Dataset Criteria}

We will focus only on IPv4 traffic to keep it simple.
Also we will focus on the TCP and UDP protocols for now.

\subsubsection{packet sequence attributes}

In this section we discuess, which network traffic attributes will be of importance to our neural network,
and therefore decide the criteria of the dataset input of our neural network.

Our main focus for the dataset attributes lies on the attributes already present in flows and packet captures.
For this we propose that we can focus on network flows only for the beginning, since they hold all information that is directly comparable.
For this every packet capture will first turned into flows.

FLows created by YAF contain the following attributes:
% start-time, end-time, duration, rtt, proto, sip, sp, dip, dp, iflags, uflags, riflags, ruflags, isn, risn, tag, rtag, pkt, oct, rpkt, roct, end-reason
Timestamps for start and end time, duration, round trip time, protocol, source IP, source port, destination IP, destination port, TCP flags (forward and reverse), initial TCP sequence number (forward and reverse), first-packet 802.1q VLAN tag (forward and reverse), packet count (forward and reverse), octet/byte count (forward and reverse), the reason for the end of the packet sequence.

% FIXME: remove IP2Vec
%Out of those the following are used by IP2Vec:
%Duration, protocol, source IP, source port, destination IP, destination port, packet count (forward), octet/byte count (forward)

% FIXME: maybe flags? YES

% FIXME: remove ip2vec
% Additionally IP2Vec extracts the attriubtes Day and Time from the start timestamp,
% where Day is a boolean represening weekday or weekend
% and Time is the time of day, striped from the date.
% However IP2Vec only uses them temporarly.
% We will extract those attributes from IP2Vecs preprocessing and use them as opposed to the start timestamp,
% since they are more generic, therefore provide better comparability and still anker the flow to a specific time.
% % TODO: export these features for our NN

% We now need to decide, which other attributes of a network flow are of value to our neural network.
% Since IP2Vec only helps us represent the categorical attributes, there are still bool, numerical and continous attributes left for us to use.
% We just need to look at their value for the NN.

% After we decided over the flow attributes %(and they work with the NN)
% we might take a closer look at the packet level attributes, a flow does not provide and decide if the extra implementation is worth it.

% TODO: compare single flow from packet capture? or even extract "single flow" packet captures from the whole packet capture

\subsection{Network Packet Captures used}

For the creation of our datasets we need network packet captures.
For this we take a look at intrusion detection datasets,
as well as %TODO: what?

Those most well known is DARPA, but since it is old and not representative of real world traffic we will only use it as a baseline.
We will also use the
MAWI,
Simpleweb,
etc. datasets.

% TODO: maybe add a pcap from my personal network

We chose these datasets because of their significant statistical differences, which have been shown in the paper
"On generating network trafic datasets with synthetic atacks for intrusion detection" by Cordero et al..
% TODO: OR double check with ID2T

\section{Model(s)}

% TODO: what are the tweaks

For our model we will most likely use a form of Recurrent Neural Network (RNN).
This will help with the vanishing gradient problem and make sure all our previous results will be taken into account.
However it is important to establish a baseline, which will help us establish a measure of progress. % or even success.
This is why for our first model we choose % TODO: WHAT?

Afterwards we will compare the results of various RRNs, such as LSTMs and GRUs, to our baseline.
If there is time left we might take a look into other NN architectures. % like DNN and CNN?

% FIXME: depending on the size of the training split into seperate sections, like before

\section{Training}

\section{Testing}

\section{Results}

\chapter{Evaluation}

\section{Experiments}

\section{Discussion}

\chapter{Conclusion}

% TODO: answer the following question
% "what can you do now, what you couldn't do before?"

\section{Summary}

\section{Research Contribution}

\section{Future Work}

\subsection{Generation of network traffic captures with GANs}

\subsubsection{Federated Learning}

In the last couple of years, federated learning (FL) has become a widely used method to protect user data while training ML models.
It still has flaws regarding privacy, but multiple researchers are working on improving the privacy of FL for example, with differential privacy.

Once the GAN proposed above produces good results, one could combine it with FL.
This way, multiple users could add their traffic datasets without exposing their data.
One real-world application for the GAN could be to deploy it with NIDSs, routers of big ISPs, or the routers of personal users.
This way, the GAN could be trained with real-world traffic without ever needing to see the data.
Some libraries, e.g. PySyft, also provide homomorphic encryption, so the end-user might not have to train on their data themselves.
Instead, they could send the encrypted data to an intermediate server, which computes the model with the encrypted data.
Therefore also never seeing the data itself.

With a GAN that also respects appropriately labeled intrusion detection network captures,
combining this approach with FL could have the potential to solve the issue of creating synthetic network traffic for self learning anomaly based NIDSs.

\printbibliography

\end{document}

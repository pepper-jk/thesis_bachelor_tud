%% This is file `DEMO-TUDaThesis.tex' version 2.09 (2020/03/13),
%% it is part of
%% TUDa-CI -- Corporate Design for TU Darmstadt
%% ----------------------------------------------------------------------------
%%
%%  Copyright (C) 2018--2020 by Marei Peischl <marei@peitex.de>
%%
%% ============================================================================
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2008/05/04 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainers of this work are
%%   Marei Peischl <tuda-ci@peitex.de>
%%   Markus Lazanowski <latex@ce.tu-darmstadt.de>
%%
%% The development respository can be found at
%% https://github.com/tudace/tuda_latex_templates
%% Please use the issue tracker for feedback!
%%
%% ============================================================================
%%
% !TeX program = lualatex
%%

\documentclass[
	ngerman,
	ruledheaders=section,%Ebene bis zu der die Überschriften mit Linien abgetrennt werden, vgl. DEMO-TUDaPub
	class=report,% Basisdokumentenklasse. Wählt die Korrespondierende KOMA-Script Klasse
	thesis={type=bachelor},% Dokumententyp Thesis, für Dissertationen siehe die Demo-Datei DEMO-TUDaPhd
	accentcolor=9c,% Auswahl der Akzentfarbe
	custommargins=true,% Ränder werden mithilfe von typearea automatisch berechnet
	marginpar=false,% Kopfzeile und Fußzeile erstrecken sich nicht über die Randnotizspalte
	%BCOR=5mm,%Bindekorrektur, falls notwendig
	parskip=half-,%Absatzkennzeichnung durch Abstand vgl. KOMA-Sript
	fontsize=11pt,%Basisschriftgröße laut Corporate Design ist mit 9pt häufig zu klein
%	logofile=example-image, %Falls die Logo Dateien nicht vorliegen
]{tudapub}

% Der folgende Block ist nur bei pdfTeX auf Versionen vor April 2018 notwendig
\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}%kompatibilität mit TeX Versionen vor April 2018
\fi

%%%%%%%%%%%%%%%%%%%
%Sprachanpassung & Verbesserte Trennregeln
%%%%%%%%%%%%%%%%%%%
\usepackage[main=english, ngerman]{babel}
\usepackage[autostyle]{csquotes}% Anführungszeichen vereinfacht
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
%Literaturverzeichnis
%%%%%%%%%%%%%%%%%%%
\usepackage{biblatex}   % Literaturverzeichnis
\addbibresource{research.bib}


%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Tabellen
%%%%%%%%%%%%%%%%%%%
%\usepackage{array}     % Basispaket für Tabellenkonfiguration, wird von den folgenden automatisch geladen
\usepackage{tabularx}   % Tabellen, die sich automatisch der Breite anpassen
%\usepackage{longtable} % Mehrseitige Tabellen
%\usepackage{xltabular} % Mehrseitige Tabellen mit anpassarer Breite
\usepackage{booktabs}   % Verbesserte Möglichkeiten für Tabellenlayout über horizontale Linien
\usepackage{multirow}
\usepackage{hhline}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Mathematik
%%%%%%%%%%%%%%%%%%%
%\usepackage{mathtools} % erweiterte Fassung von amsmath
%\usepackage{amssymb}   % erweiterter Zeichensatz
%\usepackage{siunitx}   % Einheiten

%Formatierungen für Beispiele in diesem Dokument. Im Allgemeinen nicht notwendig!
\let\file\texttt
\let\code\texttt
\let\tbs\textbackslash

\usepackage{pifont}% Zapf-Dingbats Symbole
\newcommand*{\FeatureTrue}{\ding{52}}
\newcommand*{\FeatureFalse}{\ding{56}}

\begin{document}

\Metadata{
	title=Discriminating if a network flow could have been created from a given sequence of network packets,
	author=Jens Keim
}

\title{Discriminating if a network flow could have been created from a given sequence of network packets}
\subtitle{subtitle}
\author[J. Keim]{Jens Keim}%optionales Argument ist die Signatur,
\birthplace{Worms}%Geburtsort, bei Dissertationen zwingend notwendig
\reviewer{Prof. Dr. Max M{\"u}hlh{\"a}user \and Dr. Carlos G. Cordero}%Gutachter

%Diese Felder erden untereinander auf der Titelseite platziert.
%\department ist eine notwendige Angabe, siehe auch dem Abschnitt `Abweichung von den Vorgaben für die Titelseite'
% TODO: trusted systems? as carlos or in matrix
\department{inf} % Das Kürzel wird automatisch ersetzt und als Studienfach gewählt, siehe Liste der Kürzel im Dokument.
\institute{Telecooperation}
\group{SPIN}

\submissiondate{\today}
\examdate{\today}

%	\tuprints{urn=1234,printid=12345}
%	\dedication{Für alle, die \TeX{} nutzen.}

\maketitle

\affidavit

\tableofcontents

\chapter{Abstract}

This thesis aims to design a neural network (NN), that is capable of discriminating if a network flow could have been created based on a sequence of packets and can be used as a discriminative network (DN) for a Generative Adversarial Network (GAN) in future work.

For this we first determined the features of network flows and packets a like, which are relevant to this task.
We then created a dataset by extracting the relevant features from well-known network traffic datasets from the field of intrusion detection, as well as falsifying said datapoints to provide negative samples.

For our NN model we compared multiple architectures of recurrent neural networks (RNNs).
Further our model uses a special kind of RNN called a conditional RNN (condRNN) \cite{remyPhilipperemyCondRnn2020}, which already has provided good results for a mixture of conditional and sequential input in the field of image region classification \cite{karpathyDeepVisualSemanticAlignments2015} \cite{vinyalsShowTellNeural2015}.
This is necessary as a flow is the conditional counterpart to a sequence of packets.

% During the phase of testing we optimized dataset and model to be most effective.

\chapter{Introduction}

\section{Motivation}

Network intrusion detection systems (NIDSs) require datasets to be trained and tested.
However, appropriate datasets for NIDSs are hard to come by
since creating authentic synthetic datasets is difficult and time-consuming
and most real traffic is rarely shared due to privacy concerns \cite{ringFlowbasedNetworkTraffic2019a} or copyright \cite{corderoID2TDIYDataset2015}.
This is why researchers in the field of NIDSs are restricted to use datasets with known defects or to create their own datasets.
% Remember that the last sentence of each paragraph should ease the way for the next paragraph.

% Creating synthetic network traffic is essential to provide this kind of datasets.
A researcher that chooses to create their own datasets may use real network traffic, synthetic network traffic, or a mixture of both.
Using real traffic to create a dataset, although desirable, has many disadvantages.
The capture might be too old to represent current networks or
% The network topology recorded, e.g. NAT, % Why is this a problem?
% or the used bandwidth. % not clear what you mean
it may contain artifacts.
Synthetic network traffic is hard to create because network traffic is diverse.
It is influenced by many factors, like
the countless terminals communicating,
the interim devices, e.g. switches, and their bandwidth,
gateways, subnets, and churn of terminals.
To generate appropriate datasets one would need to simulate all those factors and their interactions.
Since this is particularly complicated,
this is where the mixture of real and synthetic traffic comes in.
With this method, one takes an already existing network packet capture and modifies it to fit their needs.
%The idea is good, but the sentence needs work.
% You also need to be carefull and not mix these two ideas:
% 1) Mixing synthetic data with real to create a new dataset.
% 2) Create a complete synthetic dataset from scratch using real traffic as reference.
% Remember that with the GAN technique, we may create completely synthetic traffic that is not mixed with "real" traffic.
This modification can range from adding specific packets, e.g. of an network attack\cite{corderoID2TDIYDataset2015},
to modifying existing packets to fit a new network topology
or altering other significant features of the network behavior by modifying each packet.
Even though this method is quite effective, its design and implementation are also time-consuming and might need to be repeated for each use case.
Since it is difficult to create realistic network packet captures from scratch,
one could propose to create these traffic captures based on network flows, as those are more commonly availabilable.

The paper ''Flow-based Network Traffic Generation using Generative Adversarial Networks'' by Markus Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} provides a method to create synthetic network flows that mimic a given set of network flows.
%This enables researchers to create network flows based on real network flows.
Building on top of this method one could synthetically create labeled datasets of authentic network flows for NIDSs.
However, it is still desirable to have traffic based datasets, since they provide more information than flows.
Instead one could create synthetic network flows like proposed by Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} and
use these already authentic flows to create authentic synthetic network traffic at the packet level.
One approach to this could be a GAN that generates network packet captures based on network flows.

\section{Problem}

If we would want to create a GAN that is able to create the packets from which a network flow was constructed,
three major challenges need to be addressed.
One, the DN of the GAN needs to be defined such that it can determine if a network flow could have been created by a sequence of network packets.
Two, the generative network (GN) of the GAN needs to be able to create network packet captures that are indistinguishable (in some statistical sense) from the packets the network flow was created from.
Three, the training of the GAN and the required datasets.
To create a GAN, both the DN and GN need to perform well on their own.

In this research, we focus on building a NN that can be used as the DN of a GAN in future work.
It is not obvious, however, how to create a DN that
can verify that a network flow was created from a sequence of packets.
This leads to the research questions presented next.

\section{Research Questions}

This thesis will attempt to answer the following research questions.

\begin{itemize}
  \item How can we develop a neural network that acts as a discriminator for a GAN
  that can determine if a network flow could have been created by a sequence of network packets.
  \item Which features of network flows and packets are (most) significant to determine if a network flow could have been created by a sequence of network packets?
  \item Which NN architecture would be most suitable to distinguish if network packets could have created some network flow?
\end{itemize}

\section{Goals and Objectives}

% Was wird bei der Diplomarbeit inhaltlich erwartet und bewertet?

% I picture three main goals for your thesis (you need to either agree or propose something different):
% x 1) Create a dataset (or program to make datasets) to test your theories.
% x 2) Figure out what is needed to distinguish if packets belong to a certain flow, and
% x 3) Come up with an architecture capable of making correct decisions.

This thesis focuses on creating a NN model
that can be used as the DN of a GAN
that takes network flows as input and produces network packets as output.
The NN gets an array of network packets and at least one network flow as input
and provides the probability that a specific network flow was created from the given sequence of packets.
To achieve this goal, we might need to consider additional input,
such as labels for either the network flows, the network packets, or both.
Therefore the goal of this thesis is to create a NN model that distinguishes
if a specific network flow could have been created from a given sequence of packets, or not.

It will not only be necessary to create the NN architecture,
but also the datasets needed to train and test the system.
For this, we need suitable representations of network flows and packets that a NN can process.
To test the reliability of our model, diverse datasets from sufficiently distinct networks need to be constructed.
% During the development of the NN model, these features will be adjusted to create appropriate datasets.
So to reach the goal, we need to reach the following objectives:

\begin{itemize}
  \item Collecting diverse packet captures, representing a wide range of networks.
  \item Extracting flows from said packet captures.
  \item Extracting features from packets and flows alike,
which can be used to distinguish if packets belong to a certain flow.
  \item Creating fake flows by modifying the extracted ones based on those features.
  \item Creating a NN model that is able to compute those features.
  \item Testing the effectiveness of NN architectures to work as discriminators.
  \item Deciding which NN architecture is capable of making correct decisions.
\end{itemize}

%%% THESIS TOPIC END

%\chapter{Requirements}

% Anmerkung von Prof. Mühlhäuser: "Voraussetzungen":
% kann ruhig Selbstverständlichkeiten wie erwünschte Programmierkenntnisse
% beinhalten. Man glaubt nicht, was man bei jedem 10ten (oder so) Diplomanden
% alles irgendwann vermissen wird :-).
% Außerdem kann man "Grundkenntnisse in ... (Themengebiet) wünschenswert
% und förderlich, aber nicht zwingend" hinschreiben, dann kann man
% (z.B. mündlich) argumentieren, dass ein Teil der Literatur vor
% Startschuss zu den 6 Monaten gelesen werden muss.

%\section{for the student}

%\begin{itemize}
%  \item basic familiarity with Python
%  \item basic familiarity with ID2T and Traffic Statistic Extraction
%  \item basic familiarity with yaf and Flow Extraction
%  \item basic familiarity with Tensorflow 2
%  \item basic knowledge of Neural Networks
%  \item broad understanding of RNNs
%\end{itemize}

%\section{for the architecture}

% how much detail?
% regarding the training of GANs/Discriminator
% OR later in Thesis?

%\subsection{Functional Requirements}
% what it should do

%\subsection{Non Functional Requirements}
% how it works

\section{Terminology}
During this work we will use certain phrases and wording that may seem vague but simplifies more complex expressions.

When we say \textit{a sequence of (network) packets belongs to a (network) flow}, we mean that \textit{the (network) flow could have been created from this sequence of (network) packets}.

\chapter{Background}

% In every background "item", always start stating why the topic is important to know in the context of your thesis.

%\section{Network intrusion detection systems}
%To understand the motivation behind this thesis we must understand how NIDSs work.
%Intrusion detection systems (IDSs) describe hard- or software that monitors a network or system for malicious activity or policy violations.
%One differentiates between NIDSs and host-based intrusion detection systems (HIDSs).
%The former monitor a network, while the later a single system.
%
%Today NIDSs are crucial for network security.
%Therefore we focus on them.
%There are two different classifications of NIDSs:
%signature-based NIDSs and anomaly-based NIDSs.
%Signature-based systems are limited for several reasons:
%the availability of signatures,
%the growing threat of federated attacks that split the malicious signature between multiple gateways.
%
%This is where anomaly-based detection comes in.
%They classify network behavior as either normal or anomalous.
%This way even new attacks, for which no signature is yet provided,
%can be recognized by the system, since it is anomalous to the network behavior it knows about.
%These systems have two main sources of knowledge:
%the network they observe all the time
%and the datasets they get trained with.

\section{network flows}

If we want to extract features from network flows
we first need to understand the difference between network flows and network traffic caputres.
A network packet capture is a recording of network traffic at the packet level.
It is limited by a start and end time.
Each packet within the capture contains all headers it contained during actual transmission.
The payload of the packets is omitted if its size gets too large to be reasonably stored otherwise, the packets are unaltered.
A network flow is an artificial logical equivalent to only one network connection \cite{brownleeTrafficFlowMeasurement}.
This connection may be between two terminals, a multicast group or a terminal and a broadcast address \cite{rajahalmeIPv6FlowLabel}.
Like a network packet capture, a flow is limited by a start and end time,
but it does not contain information on individual packets.
It describes the connection on a more abstract level.

For example it will only keep track of a minimum of header information, like source and destination address and port
It will contain the flags used by the first and last packet of the communication,
but it will not contain the falgs of each individual packet within the communication.
It will know the duration and average round trip time of the communicating, but not the time intervals between two packets.
Additionally it might have information the packets themselves do not provide, e.g. the reason for the end of the communication.

\section{Network traffic datasets}

This thesis aims to lay the groundwork for a new method of the creation of authentic synthetic network packet captures
that can be used for labeled datasets for training and testing NIDSs.
Anomaly-based NIDSs (ANIDS) need network traffic datasets for testing and training.
Those datasets need to be appropriate to the threat and labeled accordingly.
However appropriate datasets are hard to come by.
Most real traffic recordings are rarely shared due to privacy concerns \cite{ringFlowbasedNetworkTraffic2019a} or copyright \cite{corderoID2TDIYDataset2015}.
The datasets including network attacks,
which are shared publicly,
tend to either be snapshots from real attacks or synthetically created traces from an isolated network.
The later does not provide realistic non-attack traffic, or background traffic.
The former present the problem of non-attack traffic being of a unique network in a unique time frame and therefore not representative of every network.
Also, they most likely get anonymized and are therefore do not represent the ground truth.
Thus these datasets are not sufficient to train or test NIDS.
And no matter the origin the dataset will only describe the network and the behavior of its traffic at the time of recording or creation, hence it might not be applicable in future networks \cite{ringFlowbasedNetworkTraffic2019a}.

% packets > flow - possible
% flow > packets - not possible yet

% NN, decision making, sequential (network) data

\section{Neural Networks}

\subsection{IP2Vec}

Network flows consist of multiple features, some of which are categorical, e.g. IP addresses, ports, and protocols.
However, NNs work best with continuous and numerical data.
To effectively train a NN we, therefore, need to represent those categorical features as numerical data.
For this we planned on using IP2Vec proposed by Ring et al. \cite{ringIP2VecLearningSimilarities2017}.
This solution produces similarity values for IP addresses based on the behavior of the host.

They based their work on the concept of word2Vec from the field of natural language processing.
There the word is observed in the context of its neighbors instead of alone.
Similarly, in IP2Vec IPs are not observed on their own, but on in the context of other features they correlate with.
But instead of words IP2Vec processes network flow attributes,
specifically source IP, destination IP, destination port and protocol.
However one could change those attributes and determine the difference between IPs based on different attributes.
With this approach they are able to determine the differences between IPs based on their communication patterns (aka similar behavior).
For example, a destination IP that correlates with port 80 often can be easily classified as an HTTP server.
Their experiments show that provided the right dataset, the data produced by IP2Vec can be used to distinguish between infected and non-infected hosts, as well as between servers, clients and even printers.

\subsection{Recurrent Neural Networks}

To solve the problem stated in this thesis we are required to find an appropriate NN architecture
that is capable of making correct decisions about network flows created from network packet captures.
Neural Networks usually process each input independently from previous inputs.
This behavior is not necessarily what you want depending on your problem.
Recurrent Neural Networks (RNNs) store previous results (output) of the NN and uses them to process the new input.
Natural language processing usually profits from this.
In theory, this ''memory'' can hold information about all previous calculations,
but in practice, it only keeps track of the last few steps.
% new hidden state = tanh(input + previous hidden state)
This is due to the vanishing gradient problem \cite{hochreiterLongShortTermMemory1997}.
%A basic RNN uses the hyperbolic tangent function

\subsection{Long Short Term Memory \& Gated Recurrent Units}

This is where Long Short Term Memory (LSTM) networks \cite{hochreiterLongShortTermMemory1997} or Gated Recurrent Units (GRUs), formerly known as gated recursive convolutional neural network (grConv) \cite{bahdanauNeuralMachineTranslation2016}, come in.
Both similarly tackle the short-term memory issue,
also known as the vanishing gradient problem \cite{hochreiterLongShortTermMemory1997}.
Where the weight of the memory is changed so little, that it becomes insignificant.
% TODO: maybe talk about vanishing gradiant problem more?
Both models provide a gated approach.
They also both rely on the Sigmoid function in addition to the hyperbolic tangent function.
While the hyperbolic tangent function produces values between -1 and 1,
the Sigmoid function produces values from 0 to 1.
This benefits the memory since low impact data is multiplied by 0 and therefore left out of the equation.

% LSTM
% new cell state = (previous cell state * forget gate) + input gate
% new hidden state = output gate * tanh(new cell state)
% gates:
% \begin{itemize}
%   \item forget gate: sigmoid(input + previous hidden state)
%   \item input gate: sigmoid(input + previous hidden state) * tanh(input + previous hidden state)
%   \item output gate: sigmoid(input + previous hidden state)
% \end{itemize}

% GRU
% new hidden state = (previous hidden state * update gate) + output gate
% gates:
% \begin{itemize}
%   \item reset gate: sigmoid(input + previous hidden state) * previous hidden state
%   \item update gate: 1 - sigmoid(input + previous hidden state)
%   \item ''output gate'': tanh(input + reset gate) * sigmoid(input + previous hidden state)
% \end{itemize}

\subsection{GANs}

To create a NN that could later be used as a DN we first need to understand how a GAN operates.
A GAN consists of two NNs that compete against each other to maximize the quality of the generated data:
a generator network (GN) and a discriminative network (DN).
The GN is constantly trying to fool the DN with data it generates synthetically,
while the DN is constantly trying to figure out if the data it was given is real or synthetic.
If correctly optimized, a GAN may create synthetic data indistinguishable from real data.
% TODO: more

\chapter{Related Work}

\section{Flow-based Network Traffic Generation using Generative Adversarial Networks}

Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} introduce a GAN to generate synthetic network flows.
% FIXME: this is no longer true
% Not only is their work interesting as the source of input for the proposed GAN at a later time,
% but as a resource regarding feature extraction for network flows,
% which is needed by this thesis.
So even though Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} do not create network packet captures,
their choice of network flow features is immensely valuable to this thesis.
% TODO: explain more?

\section{Related Neural Network Models}

In addition, we will need to look into related work regarding NNs, RNNs and GANs, specifically DNs.
Since some approaches for other problems might be applicable to the problem of discriminating if a network flow could have been created from a given sequence of network packets.
This will become clearer once the feature sets of network flows and packets are defined.

% Approaches to generate synthetic network packets that did not use NNs are not really related work,
% since they will not face the same problems as this thesis, therefore they were omitted.

% Bidirectional RNN?

% TODO: search and add some more GANs network flow papaers

\chapter{Approach}

\section{Methodology}

For our NN model, we assume that some form of Recurrent Neural Network (RNN) will be suitable as a discriminator of a GAN that can create synthetic network packet captures from network flows.
Since simple RNNs have the problem of short-term memory, we propose the use of either Long Short Term Memory (LSTM) networks \cite{hochreiterLongShortTermMemory1997} or Gated Recurrent Units (GRUs) \cite{bahdanauNeuralMachineTranslation2016}.
GRUs tend to use fewer operations and states to produce similar or better results than LSTMs.
But depending on the use case, LSTMs might still produce better results.
It is to be determined, which of the two neural networks provides better results for the problem in this thesis.

For this we will run experiments with the \textbf{SimpleRNN}, \textbf{LSTM} and \textbf{GRU} layers provided in Tensorflow.
\textbf{SimpleRNN} will provide the baseline for the real contendors \textbf{LSTM} and \textbf{GRU} during the experiments, as it will most likely provide the least optimal results.
Before we can persue the experiments however we need data to train an test with.

% TODO: what is the baseline for the datasets?

\section{Dataset}

For our dataset we will extract packet sequences from publicly available and well-known network packet captures and create network flows based on these sequences.
Each network packet sequence will be paired with its network flow so that our NN can learn the connection between the two data structures.
Since the end goal in the future work GAN is to generate the complete packet sequences based of the flows,
each sequence of packet features will contain as much of the original header information as needed to reconstruct a packet sequence from them.
This way, no crucial information gets lost and the DN is prepared to handle whole packet captures and compare them to flows.

Each header attribute is converted into an appropriate representation depending on its data type and meaning.
Since the NNs are most efficient with continuous and numerical data, we will convert all fields accordingly, whereever possible.
We will also implement a way to create negative samples from the ground truth, which builds our positive samples.
Both will be discussed further in the dataset preprocessing section.

\subsection{Dataset Preparation}

The NN needs to be trained with a significant amount of data, both positive and negative datapoints so that it can make informed decisions.
This will be provided in two steps.
First we sample flows from multiple packet captures we extracted from popular intrusion detection datasets with network traffic packets.
Those samples represent the ground truth and will be our positive datapoints.
Later we randomly apply modifications to those samples to create negative datapoints.

\subsubsection{Sampling}

% FIXME: one capture each might be bad (sound bad)
Instead of combining all flows from our source captures, we sample sequences of packets from only one packet capture of each dataset.
The sampled packet sequences and flows are stored separately as csv files for further preprocessing, as network packet captures are usually slow to process.
For our purpose we considered two approaches for sampling a network packet capture for flows.
Either by sampling through the packet sequences within the captures or by splitting the capture into intervals and sampling through those partial captures.
Splitting the capture into intervals is less compute-heavy and therefore time-intensive, since the resulting packet captures are smaller and much quicker to parse.
Truncated flows should not be a problem, since a capture itself is truncated at beginning and end anyway.
Additionally we also do not need any flow longer than the intervals themselves,
as we use timeouts for the flow collecting.
Later in the preprocessing we also drop any flow longer than 500 packets, to ensure a maixmum length and therefore an equal length for the packet sequences.

The question however is the length of the intervals.
Should they be determined relative to the size of the capture or should they have a fixed length?
A fixed-length may provide better comparability between the flows of different captures,
but the variation between packet captures regarding packet rate (frequency) may proof to be problematic.
As setting the fixed length to low may yield the problem that some captures return empty intervals
and setting it too high might result in too large intervals and therefore partial captures to be efficiently parsed later on.
An interval length based on the percentage of each individual capture does not have those faults.
Even though it might shrink the flow duration of smaller capures, it appears to be the better approach.
In addition, extracting packet sequences with a relative offset within a capture might provide sequences with a wider range of flow behavior.

Now that we know we want to split the capture first, it is to be decided, how we sample the packet sequences within an interval.
We can make an educated guess that randomly selected samples will yield better results than sequential sampling, as it again provides a wider range of flow behavior.
Still the degree to which the sampling is randomized should be determined.
% FIXME: drop everything below here?
Do we pick random intervals, or do we pick the sequence of packets within each interval at random.
Do we pick every 20th packet sequence, or do we pick each packet sequence at random.
We might test a variety of those methods to choose the packet sequences, when improving the dataset, once we have some results from the NN.
% to here

For the first iteration of our dataset we decided, that splitting the capture into intervals first would be the right approach.
We split the captures into intervals relative to its size.
This way we will preserve the behavior of the sampled flows within the larger network within each specific interval.
Afterwards, we will randomly sample up to 250 of TCP and UDP packet sequences alike within those intervals.
% TODO: mention that not all sources provide "enough" flows per protocol, specifically UDP

Each sampled packet sequence will be exported as a capture first.
The header fields of the packets belonging to the sequence will be stored as a csv file for easier parsing.
Then we use YAF to extract the flow information and yafscii to create flow-based csv files form the exported capture.

\subsubsection{A Padding supported Conversion from UDP to TCP}

Each datapoint in our dataset can either be a TCP or a UDP packet.
Compared to UDP packets, TCP packets have much more header fields and therefore more (derived) features.
This results in an uneven timestamp size (length of packet entries) between datapoints, if one is based on a UDP packet and one is a TCP packet.
To make packets of both protocols comparable, we will add padding to the UDP packets.
We add the TCP flags and options as well as the sequence number and window size with a masking value (-15) , more on that in the model section.
To round it up we convert the UDP length to TCP header length and TCP segment size (aka payload length).
The last part is especially important to keep the datapoints comparable.
This way, the NN can make the right decisions based on the data.

\subsubsection{Creation of negative samples}

To create negative samples we use multiple different methods.
The first is falsifying packet and flow features by either sampling them from another datapoint of the dataset or generating random values, which lie within a rough frame of expectation for the feature.
Both processes are randomized.
And both produce three different types of negative samples:

\begin{itemize}
    \item a datapoint, which has modified (falsified) packets
    \item a datapoint, which has a modified (falsified) flow
    \item a datapoint, which has both
\end{itemize}

The second method is randomizing the packet order within the packet sequence of a given datapoint.
The third is to decrease or increase the number of packets of a sequence in a given datapoint.
To decrease the number of packets we just remove a random number of packets at a random position.
To increase the number of packets we again sample from another datapoint in the dataset and add some of its packets to the current sequence.
And the fourth and final method is mismatching network flows with a different sequence of packets.

\subsection{Conclusion on IP2Vec}

% We had planned to adapt the IP2Vec approach and extend it to handle not only flow attributes but all packet header attributes.
% However since this approach seems fundamentally flawed.

% used IP2Vec to generate source IP based vectors that represent the similarity between IP addresses within a dataset.

% Out of the YAF flow features the following are used by IP2Vec:
% Duration, protocol, source IP, source port, destination IP, destination port, packet count (forward), octet/byte count (forward)

After due consideration, we decided not to use the IP2Vec approach.
This is mainly to the fact that IPs do not influence the flow behavior too much.
The only feature of them that influences a flow significantly is if they are public or local (aka private).
Additionally, IP2Vec sees IPs in the context of a series of packet flows, which represent a network traffic capture,
meaning that it relates the IPs to one another depending on the behavior of their flows within one network.

This is not applicable to our use case, since we use traffic of multiple network packet capture datasets.
To use IP2Vec we would either have to apply it before extracting the flows and mixing the results,
or mix the flows and then apply IP2Vec to it.
The former bares two problems:
First we would risk introducing false correlations between IPs from different datasets.
Second we would assume that for our GAN in the future work, we have a whole set of flows for each IP address, that could provide the data necessary to apply IP2Vec before using the flow as input to the DN.
% FIXME: is this really the problem? THINK ABOUT IT
The later has the problem, that IPs will correlated to each other without ever having been in the same network.
Both approaches have the problem of IPs that might be present in multiple source datasets.

\subsection{Dataset Criteria}

In this section we discuss, which features will be part of your dataset. % and how the NN input will look like.
Network traffic features influencing network flow behavior will be of importance to our neural network.
This will therefore decide the criteria of the dataset features.
Network flows and packet captures of packet sequences will be the source of our datapoints.

% FIXME: move this UP?
We will generally focus only on IPv4 traffic to keep it simple.
Also we will focus on the TCP and UDP protocols for now.

% Network flows created by YAF contain the following features:
% % start-time, end-time, duration, rtt, proto, sip, sp, dip, dp, iflags, uflags, riflags, ruflags, isn, risn, tag, rtag, pkt, oct, rpkt, roct, end-reason
% Timestamps for start and end time, duration, round trip time, protocol, source IP, source port, destination IP, destination port, TCP flags (forward and reverse), initial TCP sequence number (forward and reverse), first-packet 802.1q VLAN tag (forward and reverse), packet count (forward and reverse), octet/byte count (forward and reverse), the reason for the end of the packet sequence.

\subsubsection{Packet sequence features}

\definecolor{not}{HTML}{949698} % Gray
\definecolor{trivial}{HTML}{ed1b23} % Red
\definecolor{derivation}{HTML}{fff200} % Yellow
\definecolor{feature}{HTML}{41b0e4} % CornflowerBlue
\definecolor{dropped}{HTML}{99479b} % purple

Each packet contains a variety of header fields of varying importance to our work.
One could argue that to properly correlate between flows and packets we would need most of the header fields of each packet.
%Ideally all of them, but some can be ignored and others simplified for the NN.
However most of the header fields can be trivially compared to a flow or verified.

The checksums are a perfect example of the later.
It does not matter to the NN what the checksum reads.
At most it matters if the checksum matches the header.
So instead of feeding the NN the raw checksum and therefore teaching it how checksums work, we would provide its validity with a simple boolean.
The same goes for the header length.
% FIXME: wording
Some header fields are insubstantial for our use case, for example the addresses of the communicating end points.
This includes MAC and IP addresses.

\paragraph{packet header fields}
% TODO: link header tables
Tables 5.1 to 5.5 presents an overview of the packet headers and their fields.
We color code them in five categories:
\colorbox{not}{\textbf{{Not Used} (gray)}},
useful but \colorbox{trivial}{\textbf{Trivial} (red)},
\colorbox{dropped}{\textbf{Dropped} (purple)},
\colorbox{derivation}{\textbf{Feature Derivations} (yellow)},
and raw \colorbox{feature}{\textbf{Features} (blue)}.

\paragraph{\colorbox{not}{\textbf{{Not Used} (gray)}}} are header fields that do not change either never or in our scenario.
This includes the Version of the IP Header, since we only take IPv4 traffic into account,
and the reserved bits in the IP flags and the TCP header.

\paragraph{\colorbox{trivial}{\textbf{Trivial} (red)}} fields might be useful, but should not be fed to the NN.
Either because they can be easily checked/validated outside the NN and might slow the NN down for no benefit.
Or they do not alter the behavior of a given network flow.
% TODO QUESTION: what about IP class? this could be an interesting feature
% No, but difference between public and local.
This would include checksums, header lengths, urgent pointer, as well as MAC and IP addresses.
% The checksum and header lengths are just verified and passed to the NN as a boolean.

\paragraph{\colorbox{dropped}{\textbf{Dropped} (purple)}} fields might have been \colorbox{derivation}{\textbf{Feature Derivations} (yellow)}, but were omitted due to a lack of data.
A NN needs a large amount of data to make proper decisions, if the data is not sufficient and it encounters a feature for the first time it would at best classify the datapoint correctly with a 50\%.
For IP options and the TCP flag NS, the ECN-nonce, the data is inconclusive.
As both are rarely used in practice.
Our dataset, although constructed from numerous network packet captures, does not contain any flows with IP options and only three with the NS flag set.
Additionally network flows do contain neither of these features.
IP options might still influence the behavior of a flow, but without sufficient data, we can not train our NN.
It would just be a dead feature.
The ECN-nonce is the more acute problem since there are three datapoints, which use it.
But it is less problematic to omit since it should not impact the behavior of the flow.

% FIXME: where do we put these? I still want them to be features
The acknowledgement number can be easily checked outside the NN and can be omitted as well.
However the sequence number validity or even position in the flow can make a flow special.

\paragraph{\colorbox{derivation}{\textbf{Feature Derivations} (yellow)}} are fields, which are used as input to the NN,
but are derived in some way beforehand.
Their raw data representation either needs to be adjusted, since it is not optimal as NN input, or can be abstracted in a certain way.
The type of derivation depends on the header field.
The flags are concatenated in a binary representation.
The data offset and IP header length (IHL) are also used as an indicating boolean for TCP and IP options respectively.
The protocol will be represented as a single boolean as we are only looking at TCP and UDP and can set it to 1 for TCP and 0 for UDP.
The ports are categorical and might need to be abstracted in a useful way.
As TCP options such as maximum segment size (MSS) and selective acknowledgment (SACK) may impact a network flows behavior,
while others may never occure, just like the IP options described above,
it is important that we derivate TCP options to remove dead features and preserve the information important to us.
We collect the MSS, SACK and the timestamp options.
As they are the most prominent and frequent options.

The Explicit Congestion Notification (ECN) \cite{floydAdditionExplicitCongestion} header field is a simple example.
It consists of two bits that are interpreted as an integer from wireshark.
However the bits are a categorical feature comparable with the IP and TCP Flags.
If neither bit is set, ECN is not supported by the sender.
If either one bit is set by itself, ECN is supported by the sender.
If both bits are set, there is a network congestion on the route between source and destination.
This leaves us with two boolean features as input for our NN: ECN capable transport and congestion encountered.
If we look into the documentation of ECN, we will see that those are the only attributes the ECN provides.
% TODO: reference the ECN doc (RFC)

% TODO: explain categorical feature abstraction

The Differentiated Services Code Point (DSCP), formerly Type of Service (ToS), header field is the more complicated example.
Its specification is open for experimental and future standard values.
% TODO: reference the ECN docs (RFCs)
% https://www.iana.org/assignments/dscp-registry/dscp-registry.xhtml
It is first divided into 3 pools of values, specified by the values last two bits \cite{DifferentiatedServicesField}.
The first pool (xxxxx0) contains standard codepoints and still unassigned values, reserved for later standards.
The second pool (xxxx11) is reserved for local use or experimental codepoints.
The third pool (xxxx01) contains codepoints for local use or experimental, but will be used for standard codepoints once the first pool is exhausted.
We abstract this behavior with two features: local (or experimental) and standard.
This allows us to keep the parameters for the pool category minimal.
But the pools are not the only scheme by which DSCPs are categorized.
There are five major categories of standardized DSCPs, of which two have ranked subcategories.
The five categories Low Effort \cite{blessLowerEffortPerHopBehavior}, Class Selector \cite{nicholsDefinitionDifferentiatedServices}, Assured Forwarding \cite{wroclawskiAssuredForwardingPHB}, Voice Admit \cite{bakerDifferentiatedServicesCode}, and Expedited Forwarding \cite{firoiuExpeditedForwardingPHB} will again be used as bool features for the NN input.
However Class Selector and Assured Forwarding are again ranked within their own.
Both have are given an IP precedence class, which is ranked from 1 to 7, where packets of class 7 have the highest priority and packets with class 1 have the lowest.
Assured Forwarding only allows the first 4 classes (1-4).
% higher relative order (in the queue?)
Assured Forwarding also has the drop precedence probability, which is ranked "Low", "Medium", and "High", or in our abstraction 1 to 3, and is used within one class, so to not overwrite the priority set by the IP precedence.
As the name suggests the higher the class the more likely the packet is dropped, so here the higher number has the least priority.
% Two solutions: one hot or integer % which?
Both of these classifications will be represented as an integer value.

\paragraph{\colorbox{feature}{\textbf{Features} (blue)}} are fields that we consider unmodified or raw input.
Most of them are numerical and are therefore easily interpretable by the NN.
This goes for the total Length, ID, TTL, and Window Size.

We also consider the relative time offset of the packets, this feature will match with the duration of the flow.

\paragraph{Network flow features} will be classified in the same categories.
Table 5.6 shows the color-coded flow features.
In this work, we want to compare flow behavior to a sequence of packets.
IP addresses and timestamps might mislead our classifier.
Additionally if they match the header fields in the packet sequence could be easily checked without using a NN.
Therefore both are marked \colorbox{trivial}{\textbf{trivial} (red)}.
Duration as well as packet and byte count are already present as numerical data, therefore, they can be used as raw \colorbox{feature}{\textbf{features} (blue)}.
The rest of the features are to be \colorbox{derivation}{\textbf{feature derivations} (yellow)} and will be derived in a way that fits the derivations of their packet field counterparts.
There are less parts of the flow that are omitted.
This is because flows are a much higher abstraction of a packet sequence and we try to use as many of its data in order to make up for that.
Please note however that the reverse features of a YAF flow will be omitted due to the fact that it does not relate to the packet sequence of the flow, but to the counter part of it.
Meaning the flow from destination to source.

% Later: "data preprocessing"

% FIXME: remove ip2vec
% Additionally IP2Vec extracts the attriubtes Day and Time from the start timestamp,
% where Day is a boolean represening weekday or weekend
% and Time is the time of day, striped from the date.
% However IP2Vec only uses them temporarly.
% We will extract those attributes from IP2Vecs preprocessing and use them as opposed to the start timestamp,
% since they are more generic, therefore provide better comparability and still anker the flow to a specific time.
% % TODO: export these features for our NN

% We now need to decide, which other attributes of a network flow are of value to our neural network.
% Since IP2Vec only helps us represent the categorical attributes, there are still bool, numerical and continous attributes left for us to use.
% We just need to look at their value for the NN.

% After we decided over the flow attributes %(and they work with the NN)
% we might take a closer look at the packet level attributes, a flow does not provide and decide if the extra implementation is worth it.

% TODO: compare single flow from packet capture? or even extract "single flow" packet captures from the whole packet capture

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|}
        \hline
        bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 \\
        \hline
        0   & \multicolumn{6}{|c|}{\cellcolor{trivial} Desitnation} & \multicolumn{6}{|c|}{\cellcolor{trivial} Source} & \multicolumn{2}{|c|}{\cellcolor{not} Type} \\
        \hline
    \end{tabular}
    \caption{MAC Header (Ethernet II)}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|}
        %\hline
        %bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\
        \hline
        %bit & \multicolumn{8}{|l|}{0} & \multicolumn{8}{|l|}{8} & \multicolumn{8}{|l|}{16} & \multicolumn{8}{|l|}{24} \\
        bit & \multicolumn{8}{|l|}{0} & \multicolumn{8}{|l|}{8} & \multicolumn{3}{|l|}{16} & \multicolumn{13}{|l|}{19} \\
        \hline % \multicolumn{3}{|c|}{Flags} % \makecell{E\\C\\N}
        0   & \multicolumn{4}{|c|}{\cellcolor{not} Version} & \multicolumn{4}{|c|}{\cellcolor{derivation} IHL} & \multicolumn{6}{|c|}{\cellcolor{derivation} DSCP} & \multicolumn{2}{|c|}{\cellcolor{derivation} ECN} & \multicolumn{16}{|c|}{\cellcolor{feature} Total Length} \\
        \hline % \multicolumn{3}{|c|}{Flags} | \makecell{D\\F} \makecell{M\\F}
        32  & \multicolumn{16}{|c|}{\cellcolor{feature} Identification} & \cellcolor{not} 0 & \cellcolor{derivation} DF & \cellcolor{derivation} MF & \multicolumn{13}{|c|}{\cellcolor{derivation} Fragment Offset} \\
        \hline
        64  & \multicolumn{8}{|c|}{\cellcolor{feature} Time To Live} & \multicolumn{8}{|c|}{\cellcolor{derivation} Protocol} & \multicolumn{16}{|c|}{\cellcolor{trivial} Header Checksum} \\
        \hline
        96  & \multicolumn{32}{|c|}{\cellcolor{trivial} Source IP Address} \\
        \hline
        128 & \multicolumn{32}{|c|}{\cellcolor{trivial} Destination IP Address} \\
        \hline
        160 & \multicolumn{32}{|c|}{\cellcolor{dropped}}\\\cline{1-1}
        192 & \multicolumn{32}{|c|}{\cellcolor{dropped}}\\\cline{1-1}
        224 & \multicolumn{32}{|c|}{\cellcolor{dropped}}\\\cline{1-1}
        256 & \multicolumn{32}{|c|}{\multirow{-4}{*}{\cellcolor{dropped} Options (if IHL > 5)}} \\
        \hline
    \end{tabular}
    \caption{IPv4 Header}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|} % m{0.2pt}
        \hline
        %bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\
        bit & \multicolumn{4}{|l|}{0} & \multicolumn{3}{|l|}{4} & \multicolumn{9}{|l|}{7} & \multicolumn{16}{|l|}{16} \\
        \hline
        0   & \multicolumn{16}{|c|}{\cellcolor{derivation} Source Port} & \multicolumn{16}{|c|}{\cellcolor{derivation} Desitnation Port} \\
        \hline % \cellcolor{derivation}
        32  & \multicolumn{32}{|c|}{\cellcolor{derivation} Sequence Number} \\
        \hline % \cellcolor{derivation}
        64  & \multicolumn{32}{|c|}{\cellcolor{trivial} Acknowledgement Number} \\
        \hline % \makecell{Data \\Offset}
        96  & \multicolumn{4}{|c|}{\cellcolor{derivation} Data Offset} & \multicolumn{3}{|c|}{\cellcolor{not} Reserved} & \multicolumn{9}{|c|}{\cellcolor{derivation} Flags} & \multicolumn{16}{|c|}{\cellcolor{feature} Windows Size} \\
        \hline
        128 & \multicolumn{16}{|c|}{\cellcolor{trivial} Checksum} & \multicolumn{16}{|c|}{\cellcolor{trivial} Urgent Pointer (if URG set)} \\
        \hline
        160 & \multicolumn{32}{|c|}{\cellcolor{derivation}}\\\cline{1-1}
        ... & \multicolumn{32}{|c|}{\multirow{-2}{*}{\cellcolor{derivation} Options (if data offset > 5. Padded at the end with "0" bytes if necessary.)}} \\
        \hline
    \end{tabular}
    \caption{TCP Header}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{*{9}{|c}|}
        \hline
        \cellcolor{dropped} \makecell{N\\S} & \cellcolor{derivation} \makecell{C\\W\\R} & \cellcolor{derivation} \makecell{E\\C\\E} & \cellcolor{derivation} \makecell{U\\R\\G} & \cellcolor{derivation} \makecell{A\\C\\K} & \cellcolor{derivation} \makecell{P\\S\\H} & \cellcolor{derivation} \makecell{R\\S\\T} & \cellcolor{derivation} \makecell{S\\Y\\N} & \cellcolor{derivation} \makecell{F\\I\\N} \\
        \hline
    \end{tabular}
    \caption{TCP Flags}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|}
        \hline
        %bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\
        bit & \multicolumn{16}{|l|}{0} & \multicolumn{16}{|l|}{16} \\
        \hline
        0   & \multicolumn{16}{|c|}{\cellcolor{derivation} Source Port} & \multicolumn{16}{|c|}{\cellcolor{derivation} Desitnation Port} \\
        \hline
        32  & \multicolumn{16}{|c|}{\cellcolor{trivial} Length} & \multicolumn{16}{|c|}{\cellcolor{trivial} Checksum} \\
        \hline
    \end{tabular}
    \caption{UDP Header}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|}
        \hline
        % FIXME: remove demo stuff, adjust for YAF
        % start-time, end-time, duration, rtt, proto, sip, sp, dip, dp, iflags, uflags, riflags, ruflags, isn, risn, tag, rtag, pkt, oct, rpkt, roct, end-reason
        \cellcolor{trivial} Start Timestamps \\
        \hline
        \cellcolor{trivial} End Timestamps \\
        \hline
        \cellcolor{feature} Duration \\
        \hline
        % OR trivial?
        \cellcolor{derivation} round trip time \\
        \hline
        \cellcolor{derivation} protocol \\
        \hline
        \cellcolor{trivial} source IP \\
        \hline
        \cellcolor{derivation} source port \\
        \hline
        \cellcolor{trivial} destination IP \\
        \hline
        \cellcolor{derivation} destination port \\
        \hline
        \cellcolor{derivation} TCP flags (forward only) \\
        \hline
        \cellcolor{derivation} initial TCP sequence number (forward only) \\
        \hline
        % TODO: what exactly does this store/return; is it useful?
        \cellcolor{derivation} first-packet 802.1q VLAN tag (forward only) \\
        \hline
        \cellcolor{feature} packet count (forward only) \\
        \hline
        \cellcolor{feature} octet/byte count (forward only) \\
        \hline
        % TODO: what exactly does this store/return; is it useful?
        \cellcolor{derivation} the reason for the end of the packet sequence. \\
        \hline
    \end{tabular}
    \caption{YAF IPFIX Template}
\end{table}

\subsection{Network Packet Captures used}

For the creation of our datasets we need network packet captures.
For this we take a look at publically available intrusion detection datasets.
Since the goal of our future work is to produce network packet captures, that can be used in the field of NIDSs.
As stated above we will extracted data from multiple popular datasets, such as: CDX, DARPA, MACCDC, MAWI, Simpleweb and UNSW.
% TODO: maybe add a pcap from my personal network
We chose these datasets because of their significant statistical differences, which has been shown in the paper "On generating network traffic datasets with synthetic attacks for intrusion detection" by Cordero et al.
% TODO: OR double check with ID2T

\paragraph{CDX:} the data capture from the U.S. National Security Agency (NSA) published in 2009.

\paragraph{DARPA:} the Intrusion Detection Data Sets from 1998 and 1999.

\paragraph{MACCDC:} the U.S. National CyberWatch Mid-Atlantic Collegiate Cyber Defense Competition (MACCDC) dataset from 2012

\paragraph{MAWI:} backbone traffic from the MAWI labs in Japan? from 2015.

\paragraph{Simpleweb:}

\paragraph{UNSW,} specifically UNSW-NB15, with multiple attacks.

\section{Neural Network}

\subsection{Input}

To use our dataset with a NN we need to convert the datapoints into a data structure that suitable as NN input.
As NNs works best with continuous data we face the following problem when converting the data.
The data that is only available as bytes and strings.
Those features need to be converted to integer, floats or what data researchers call a binary representation, which is basically a sequence of boolean values that describe a categorical feature.
This is already solved in the preprocessing.
% Second those sequences of binary representations need to be converted into continuous sequences, before being concatenated with the features, which are continuous by default.
% For this we will apply a naive approach, which has proven to be very effective.
% We will create two RNN layers, one of which will convert the categorical data into a continuous vector.
% The other one will get a vector of continuous data, which consists of the original continuous data and the newly created conversion vector.

Furthermore we need to add padding to the packet sequences of each datapoint to the maximum sequence size, 500 in case of our initial dataset.
% TODO: mention the decision to cut data > 500 out
Therefore the model needs two Masking layers.
One for ignoring the padding of UDP packets described in the previous section on dataset preparation.
And one for ignoring the padding of the packet sequence length.

Each datapoint has a general structure of a list of flow features and a sequence of packets, where each packet consists of a list of attributes derived from the data.
As RNNs work with sequential data and in our case only the sequence of packets is sequential, we need a more sophisticated solution to take our flow data into account.
There are multiple approaches to this problem.
One is the append the flow data to each packet in the sequence.
But this creates an enormous amount of overhead and has also proven to be an ineffective solution in the past.
Both Vinyals et al. \cite{vinyalsShowTellNeural2015} and Karpathy and Fei-Fei \cite{karpathyDeepVisualSemanticAlignments2015} claim that passing the condition to the NN with each timestamp of the sequence was less effective in providing results.
Another approach would be to use two different NN layers in parallel to learn about the flow attributes and the packet sequence and merge the results afterwards.
In our case the  packet sequence can only be verified as belonging to a given flow if the layer knows the flow attributes.
Therefore the flow data should directly impact the packet sequence, so separating them would not be a good idea.
The only liable approach is to modify the initial state of the RNN with the flow data as proposed by Vinyals et al. \cite{vinyalsShowTellNeural2015} and by Karpathy and Fei-Fei \cite{karpathyDeepVisualSemanticAlignments2015}.
Both used the output of a CNN, that provided a continuous vector representation of an image, as a conditional input for their RNN.

% The image I is only input once, at t = −1, to inform the LSTM about the image contents. We empirically verified that feeding the image at each time step as an extra input yields inferior results, as the network can explicitly exploit noise in the image and overfits more easily.
% - Vinyals et al.

% All weights were randomly initialized except for the CNN weights, which we left unchanged because changing them had a negative impact. We used 512 dimensions for the embeddings and the size of the LSTM memory.
% - Vinyals et al.

% We first describe neural networks that map words and image regions into a common, multimodal embedding.
% Then we introduce our novel objective, which learns the embedding representations so that semantically similar concepts across the two modalities occupy nearby regions of the space.
% - Karpathy and Fei-Fei

% We explore a simple but effective extension that additionally conditions the generative process on the content of an input image.
% More formally, during training our Multimodal RNN takes the image pixels I and a sequence of input vectors (x1, . . . , xT ).
% Note that we provide the image context vector bv to the RNN only at the first iteration, which we found to work better than at each time step.
% In practice we also found that it can help to also pass both bv, (Whxxt) through the activation function.
% A typical size of the hidden layer of the RNN is 512 neurons.
% - Karpathy and Fei-Fei

% We condition the RNN’s predictions on the image information (bv) via bias interactions on the first step.
% - Karpathy and Fei-Fei

% sentence. The RNN is conditioned on the image information at the first time step.
% - Karpathy and Fei-Fei

\subsection{Model(s)}

% TODO: what are the tweaks

For our NN model we will (most likely) use a form of Recurrent Neural Network (RNN).
This will help with the vanishing gradient problem and make sure all our previous results will be taken into account.
However it is important to establish a baseline, which will help us establish a measure of progress. % or even success.
This is why for our first model we can use a Softmax Layer.

Afterwards we will compare the results of various RNNs, such as LSTMs and GRUs, to our baseline.
If there is time left we might take a look into other NN architectures. % like DNN and CNN?

% TODO: talk about UDP/TCP splitting
A RNN needs sequences of data that have the same input shape (length, number of features) in each batch.
Since UDP and TCP packets have a different number of features associated with them UDP packets need to be padded in order to coexist with TCP packets within one dataset.
As this allows us to compare the behavior of UDP and TCP packets this is our goto solution.
Additionally a NN handling both protocols allows us to have negative samples where the protocol was changed.
However it might be interesting to compare our results to a NN model that consists of two RNNs, where one is handling UDP and one is handling TCP packet sequences later on.

As described in the Input section above, we will use a NN model, which accepts flows as conditional input and packets as sequential input.
This conditional RNN \cite{remyPhilipperemyCondRnn2020} will empower us to diced if a network flow could have been created by a sequence of network packets.


% % TODO: talk about flow+packet concat as another "sanity check" experiment?
% As described in the Input section above, we will try to avoid a concatenation of each packet with the flow data.
% However it might be interesting to see if it even has an impact on the performance.

% % TODO: masking with one or two layers?
% As described in the dataset preparation section of this work, our NN will need two masking layers.
% One for the UDP padding and one for the padding of the packet sequence length.
% To decrease calculation overhead, we propose to use only one masking layer and only one masking value instead of two.
% This idea might be flawed so experiments comparing both approaches are in order.

% % TODO: another experiment would be to not convert the categorical data, but instead just use a int/bool mix vector and see where it goes
% %       also easy todo as this is what we did so far anyway
% Also skipping the conversion of categorical data into a continuous vector could yield interesting results.

% FIXME: depending on the size of the training split into separate sections, like before

\section{Training}

\section{Testing}

\section{Results}

\chapter{Evaluation}

\section{Experiments}

\section{Discussion}

\chapter{Conclusion}

% TODO: answer the following question
% "what can you do now, what you couldn't do before?"

\section{Summary}

\section{Research Contribution}

We discussed the features of packets and flows and their importance to the flow behavior and therefore the classifier.
We created a pipeline that creates datasets of network flows and network packet sequences for NNs.
We proofed that it is possible to classify if a flow could have been created by a sequence of packets.
We created a NN that can distinguish if a flow could have been created from a given sequence of packets and can be used as a DN for a GAN.
We also evaluated the effectiveness of different NN architectures for this task.

\section{Future Work}

\subsection{Generation of network traffic captures with GANs}

As stated many times throughout this thesis, we created a NN that can serve as a DN for a GAN that creates synthetic and authentic network packets from a network flow.
The next step for this would be implement a GN that can be run against our DN.
It is also advised to gather more data to be more representative of the real world.
The dataset we created is limited by its source data, which was fine for a proof of concept, but will not be sufficient when applied to real-world problems.

\subsubsection{Federated Learning}

In the last couple of years, federated learning (FL) has become a widely used method to protect user data while training ML models.
It still has flaws regarding privacy, but multiple researchers are working on improving the privacy of FL for example, with differential privacy.

Once the GAN proposed above produces good results, one could combine it with FL.
This way, multiple users could add their traffic datasets without exposing their data.
One real-world application for the GAN could be to deploy it with NIDSs, routers of big ISPs, or the routers of personal users.
This way, the GAN could be trained with real-world traffic without ever needing to see the data.
Some libraries, e.g. PySyft, also provide homomorphic encryption, so the end-user might not have to train on their data themselves.
Instead, they could send the encrypted data to an intermediate server, which computes the model with the encrypted data.
Therefore also never seeing the data itself.

With a GAN that also respects appropriately labeled intrusion detection network captures,
combining this approach with FL could have the potential to solve the issue of creating synthetic network traffic for self learning anomaly based NIDSs.

\printbibliography

\end{document}

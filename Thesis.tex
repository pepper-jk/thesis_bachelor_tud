%% This is file `DEMO-TUDaThesis.tex' version 2.09 (2020/03/13),
%% it is part of
%% TUDa-CI -- Corporate Design for TU Darmstadt
%% ----------------------------------------------------------------------------
%%
%%  Copyright (C) 2018--2020 by Marei Peischl <marei@peitex.de>
%%
%% ============================================================================
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2008/05/04 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainers of this work are
%%   Marei Peischl <tuda-ci@peitex.de>
%%   Markus Lazanowski <latex@ce.tu-darmstadt.de>
%%
%% The development respository can be found at
%% https://github.com/tudace/tuda_latex_templates
%% Please use the issue tracker for feedback!
%%
%% ============================================================================
%%
% !TeX program = lualatex
%%

\documentclass[
	ngerman,
	ruledheaders=section,%Ebene bis zu der die Überschriften mit Linien abgetrennt werden, vgl. DEMO-TUDaPub
	class=report,% Basisdokumentenklasse. Wählt die Korrespondierende KOMA-Script Klasse
	thesis={type=bachelor},% Dokumententyp Thesis, für Dissertationen siehe die Demo-Datei DEMO-TUDaPhd
	accentcolor=9c,% Auswahl der Akzentfarbe
	custommargins=true,% Ränder werden mithilfe von typearea automatisch berechnet
	marginpar=false,% Kopfzeile und Fußzeile erstrecken sich nicht über die Randnotizspalte
	%BCOR=5mm,%Bindekorrektur, falls notwendig
	parskip=half-,%Absatzkennzeichnung durch Abstand vgl. KOMA-Sript
	fontsize=11pt,%Basisschriftgröße laut Corporate Design ist mit 9pt häufig zu klein
%	logofile=example-image, %Falls die Logo Dateien nicht vorliegen
]{tudapub}

% Der folgende Block ist nur bei pdfTeX auf Versionen vor April 2018 notwendig
\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}%kompatibilität mit TeX Versionen vor April 2018
\fi

%%%%%%%%%%%%%%%%%%%
%Sprachanpassung & Verbesserte Trennregeln
%%%%%%%%%%%%%%%%%%%
\usepackage[main=english, ngerman]{babel}
\usepackage[autostyle]{csquotes}% Anführungszeichen vereinfacht
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
%Literaturverzeichnis
%%%%%%%%%%%%%%%%%%%
\usepackage{biblatex}   % Literaturverzeichnis
%\bibliography{DEMO-TUDaBibliography}

% TODO: fix biblatex cites
\addbibresource{research.bib}
%\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Tabellen
%%%%%%%%%%%%%%%%%%%
%\usepackage{array}     % Basispaket für Tabellenkonfiguration, wird von den folgenden automatisch geladen
\usepackage{tabularx}   % Tabellen, die sich automatisch der Breite anpassen
%\usepackage{longtable} % Mehrseitige Tabellen
%\usepackage{xltabular} % Mehrseitige Tabellen mit anpassarer Breite
\usepackage{booktabs}   % Verbesserte Möglichkeiten für Tabellenlayout über horizontale Linien
\usepackage{multirow}
\usepackage{hhline}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Mathematik
%%%%%%%%%%%%%%%%%%%
%\usepackage{mathtools} % erweiterte Fassung von amsmath
%\usepackage{amssymb}   % erweiterter Zeichensatz
%\usepackage{siunitx}   % Einheiten

%Formatierungen für Beispiele in diesem Dokument. Im Allgemeinen nicht notwendig!
\let\file\texttt
\let\code\texttt
\let\tbs\textbackslash

\usepackage{pifont}% Zapf-Dingbats Symbole
\newcommand*{\FeatureTrue}{\ding{52}}
\newcommand*{\FeatureFalse}{\ding{56}}

\begin{document}

\Metadata{
	title=Discriminating if a network flow could have been created from a given sequence of network packets,
	author=Jens Keim
}

\title{Discriminating if a network flow could have been created from a given sequence of network packets}
\subtitle{subtitle}
\author[J. Keim]{Jens Keim}%optionales Argument ist die Signatur,
\birthplace{Worms}%Geburtsort, bei Dissertationen zwingend notwendig
\reviewer{Prof. Dr. Max M{\"u}hlh{\"a}user \and Dr. Carlos G. Cordero}%Gutachter

%Diese Felder erden untereinander auf der Titelseite platziert.
%\department ist eine notwendige Angabe, siehe auch dem Abschnitt `Abweichung von den Vorgaben für die Titelseite'
% TODO: trusted systems? as carlos or in matrix
\department{inf} % Das Kürzel wird automatisch ersetzt und als Studienfach gewählt, siehe Liste der Kürzel im Dokument.
\institute{Telecooperation}
\group{SPIN}

\submissiondate{\today}
\examdate{\today}

%	\tuprints{urn=1234,printid=12345}
%	\dedication{Für alle, die \TeX{} nutzen.}

\maketitle

\affidavit

\tableofcontents

\chapter{Abstract}

This thesis aims to design a neural network (NN), that is capable of discriminating if a network flow could have been created based on a sequence of packets and can later be used as a discriminative network (DN) for a GAN.
For this we first determined the features of network flows and packets a like, which are relevant to this task.
We then created a dataset with extracting the relevant features from well-known network traffic datasets from the field of intrusion detection.

... about the NN model we used ...

% During the phase of testing we optimized dataset and model to be most effective.

\chapter{Introduction}

\section{Motivation}

Network intrusion detection systems (NIDSs) require datasets to be trained and tested.
However, appropriate datasets for NIDSs are hard to come by
since creating authentic synthetic datasets is difficult and time-consuming
and most real traffic is rarely shared due to privacy concerns \cite{ringFlowbasedNetworkTraffic2019a} or copyright \cite{corderoID2TDIYDataset2015}.
This is why researchers in the field of NIDSs are restricted to use datasets with known defects or to create their own datasets.
% Remember that the last sentence of each paragraph should ease the way for the next paragraph.

% Creating synthetic network traffic is essential to provide this kind of datasets.
A researcher that chooses to create their own datasets may use real network traffic, synthetic network traffic, or a mixture of both.
Using real traffic to create a dataset, although desirable, has many disadvantages.
The capture might be too old to represent current networks or
% The network topology recorded, e.g. NAT, % Why is this a problem?
% or the used bandwidth. % not clear what you mean
it may contain artifacts.
Synthetic network traffic is hard to create because network traffic is diverse.
It is influenced by many factors, like
the countless terminals communicating,
the interim devices, e.g. switches, and their bandwidth,
gateways, subnets, and churn of terminals.
To generate appropriate datasets one would need to simulate all those factors and their interactions.
Since this is particularly complicated,
this is where the mixture of real and synthetic traffic comes in.
With this method, one takes an already existing network packet capture and modifies it to fit their needs.
%The idea is good, but the sentence needs work.
% You also need to be carefull and not mix these two ideas:
% 1) Mixing synthetic data with real to create a new dataset.
% 2) Create a complete synthetic dataset from scratch using real traffic as reference.
% Remember that with the GAN technique, we may create completely synthetic traffic that is not mixed with "real" traffic.
This modification can range from adding specific packets, e.g. of an network attack\cite{corderoID2TDIYDataset2015},
to modifying existing packets to fit a new network topology
or altering other significant features of the network behavior by modifying each packet.
Even though this method is quite effective, its design and implementation are also time-consuming.
Since it is difficult to create realistic network packet captures from scratch,
one could propose to create these traffic captures based on network flows.

The paper ''Flow-based Network Traffic Generation using Generative Adversarial Networks'' by Markus Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} provides a method to create synthetic network flows that mimic a given set of network flows.
%This enables researchers to create network flows based on real network flows.
Building on top of this method one could synthetically create labeled datasets of authentic network flows for NIDSs.
However, it is still desirable to have traffic based datasets, since they provide more information than flows.
Instead one could build on top of the method to create synthetic network flows proposed by Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} and
use the already authentic flows to create authentic synthetic network traffic at the packet level.
One approach to this could be a Generative Adversarial Network (GAN) that generates network packet captures based on network flows.

\section{Problem}

If we would want to propose a GAN that can create the packets from which a network flow was constructed,
three major challenges need to be addressed.
One, the DN needs to be defined such that it can determine if a network flow could have been created by a sequence of network packets.
Two, a GN needs to be able to create network packet captures that are indistinguishable (in some statistical sense) from the packets the network flow was created from.
Three, the training of the GAN and the required datasets.
To create a GAN, both the DN and GN need to perform well on their own.

In this research, we focus on building a NN that can be used in future work as the DN of a GAN.
It is not obvious, however, how to create a DN that
can verify that a network flow was created from a sequence of packets.
This leads to the research questions presented next.

\section{Research Questions}

This thesis will attempt to answer the following research questions.

\begin{itemize}
  \item How can we develop a neural network that acts as a discriminator for a GAN
  that can determine if a network flow could have been created by a sequence of network packets.
  \item Which features of network flows and packets are (most) significant to determine if a network flow could have been created by a sequence of network packets?
  \item Which NN architecture would be most suitable to distinguish if network packets could have created some network flow?
\end{itemize}

\section{Goals and Objectives}

% Was wird bei der Diplomarbeit inhaltlich erwartet und bewertet?

% I picture three main goals for your thesis (you need to either agree or propose something different):
% x 1) Create a dataset (or program to make datasets) to test your theories.
% x 2) Figure out what is needed to distinguish if packets belong to a certain flow, and
% x 3) Come up with an architecture capable of making correct decisions.

This thesis focuses on creating a Neural Network (NN) model
that can be used as the DN of a GAN
that takes network flows as input and produces network packets as output.
The NN gets an array of network packets and at least one network flow as input
and provides the probability that a specific network flow was created from the given sequence of packets.
To achieve this goal, we might need to consider additional input,
such as labels for either the network flows, the network packets, or both.
Therefore the goal of this thesis is to create a DN model that distinguishes
if a specific network flow could have been created from a given sequence of packets, or not.

It will not only be necessary to create the NN architecture,
but also the datasets needed to train and test the system.
For this, we need suitable representations of network flows and packets that a NN can process.
To test the reliability of our DN, diverse datasets from sufficiently distinct networks need to be constructed.
% During the development of the NN model, these features will be adjusted to create appropriate datasets.
So to reach the goal, we need to reach the following objectives:

\begin{itemize}
  \item Collecting diverse packet captures, representing a wide range of networks.
  \item Extracting flows from said packet captures.
  \item Creating fake flows by modifying the extracted ones based on those features.
  \item Extracting features from packets and flows alike,
which can be used to distinguish if packets belong to a certain flow.
  \item Creating a NN model based on those features.
  \item Choosing a NN architecture that is capable of making correct decisions.
  \item Testing the effectiveness of the NN architectures to work as discriminators.
\end{itemize}

%%% THESIS TOPIC END

%\chapter{Requirements}

% Anmerkung von Prof. Mühlhäuser: "Voraussetzungen":
% kann ruhig Selbstverständlichkeiten wie erwünschte Programmierkenntnisse
% beinhalten. Man glaubt nicht, was man bei jedem 10ten (oder so) Diplomanden
% alles irgendwann vermissen wird :-).
% Außerdem kann man "Grundkenntnisse in ... (Themengebiet) wünschenswert
% und förderlich, aber nicht zwingend" hinschreiben, dann kann man
% (z.B. mündlich) argumentieren, dass ein Teil der Literatur vor
% Startschuss zu den 6 Monaten gelesen werden muss.

%\section{for the student}

%\begin{itemize}
%  \item basic familiarity with Python
%  \item basic familiarity with ID2T and Traffic Statistic Extraction
%  \item basic familiarity with yaf and Flow Extraction
%  \item basic familiarity with Tensorflow 2
%  \item basic knowledge of Neural Networks
%  \item broad understanding of RNNs
%\end{itemize}

%\section{for the architecture}

% how much detail?
% regarding the training of GANs/Discriminator
% OR later in Thesis?

%\subsection{Functional Requirements}
% what it should do

%\subsection{Non Functional Requirements}
% how it works

\chapter{Background}

% In every background "item", always start stating why the topic is important to know in the context of your thesis.

%\section{Network intrusion detection systems}
%To understand the motivation behind this thesis we must understand how NIDSs work.
%Intrusion detection systems (IDSs) describe hard- or software that monitors a network or system for malicious activity or policy violations.
%One differentiates between NIDSs and host-based intrusion detection systems (HIDSs).
%The former monitor a network, while the later a single system.
%
%Today NIDSs are crucial for network security.
%Therefore we focus on them.
%There are two different classifications of NIDSs:
%signature-based NIDSs and anomaly-based NIDSs.
%Signature-based systems are limited for several reasons:
%the availability of signatures,
%the growing threat of federated attacks that split the malicious signature between multiple gateways.
%
%This is where anomaly-based detection comes in.
%They classify network behavior as either normal or anomalous.
%This way even new attacks, for which no signature is yet provided,
%can be recognized by the system, since it is anomalous to the network behavior it knows about.
%These systems have two main sources of knowledge:
%the network they observe all the time
%and the datasets they get trained with.

\section{network flows}
% TODO

\section{Network traffic datasets}

This thesis aims to lay the groundwork for a new method of the creation of authentic synthetic network packet captures
that can be used for labeled datasets for training and testing NIDSs.
Anomaly-based NIDSs (ANIDS) need network traffic datasets for testing and training.
Those datasets need to be appropriate to the threat and labeled accordingly.
However appropriate datasets are hard to come by.
Most real traffic recordings are rarely shared due to privacy concerns \cite{ringFlowbasedNetworkTraffic2019a} or copyright \cite{corderoID2TDIYDataset2015}.
The datasets including network attacks,
which are shared publicly,
tend to either be snapshots from real attacks or synthetically created traces from an isolated network.
The later does not provide realistic non-attack traffic, or background traffic.
The former present the problem of non-attack traffic being of a unique network in a unique time frame and therefore not representative of every network.
Also, they most likely get anonymized and are therefore do not represent the ground truth.
Thus these datasets are not sufficient to train or test NIDS.
And no matter the origin the dataset will only describe the network and the behavior of its traffic at the time of recording or creation, hence it might not be applicable in future networks \cite{ringFlowbasedNetworkTraffic2019a}.

If we want to extract features from network flows
we first need to understand the difference between network flows and network traffic caputres.
A network packet capture is a recording of network traffic at the packet level.
It is limited by a start and end time.
Each packet within the capture contains all headers it contained during actual transmission.
The payload of the packets is omitted if its size gets too large to be reasonably stored otherwise, the packets are unaltered.
A network flow is an artificial logical equivalent to only one network connection \cite{brownleeTrafficFlowMeasurement}.
This connection may be between two terminals, a multicast group or a terminal and a broadcast address \cite{rajahalmeIPv6FlowLabel}.
Like a network packet capture, a flow is limited by a start and end time,
but it does not contain information on individual packets.
It describes the connection on a more abstract level.

% packets > flow - possible
% flow > packets - not possible yet

% NN, decision making, sequential (network) data

\section{Neural Networks}

\subsection{IP2Vec}

Network flows consist of multiple features, some of which are categorical, e.g. IP addresses, ports, and protocols.
However, NNs work best with continuous and numerical data.
To effectively train a NN we, therefore, need to represent those categorical features as numerical data.
For this we plan on using IP2Vec proposed by Ring et al. \cite{ringIP2VecLearningSimilarities2017}.
This solution produces similarity values for IP addresses based on the behavior of the host.

They based their work on the concept of word2Vec from the field of natural language processing.
There the word is observed in the context of its neighbors instead of alone.
Similarly, in IP2Vec IPs are not observed on their own, but on in the context of other features they correlate with.
But instead of words IP2Vec processes network flow attributes,
specifically source IP, destination IP, destination port and protocol.
However one could change those attributes and determine the difference between IPs based on different attributes.
With this approach they are able to determine the differences between IPs based on their communication patterns (aka similar behavior).
For example, a destination IP that correlates with port 80 often can be easily classified as an HTTP server.
Their experiments show that provided the right dataset, the data produced by IP2Vec can be used to distinguish between infected and non-infected hosts, as well as between servers, clients and even printers.

\subsection{Recurrent Neural Networks}

To solve the problem stated in this thesis we are required to find an appropriate NN architecture
that is capable of making correct decisions about network flows created from network packet captures.
Neural Networks usually process each input independently from previous inputs.
This behavior is not necessarily what you want depending on your problem.
Recurrent Neural Networks (RNNs) store previous results (output) of the NN and uses them to process the new input.
Natural language processing usually profits from this.
In theory, this ''memory'' can hold information about all previous calculations,
but in practice, it only keeps track of the last few steps.
% new hidden state = tanh(input + previous hidden state)
This is due to the vanishing gradient problem \cite{hochreiterLongShortTermMemory1997}.
%A basic RNN uses the hyperbolic tangent function

\subsection{Long Short Term Memory \& Gated Recurrent Units}

This is where Long Short Term Memory (LSTM) networks \cite{hochreiterLongShortTermMemory1997} or Gated Recurrent Units (GRUs), formerly known as gated recursive convolutional neural network (grConv) \cite{bahdanauNeuralMachineTranslation2016}, come in.
Both similarly tackle the short-term memory issue,
also known as the vanishing gradient problem \cite{hochreiterLongShortTermMemory1997}.
Where the weight of the memory is changed so little, that it becomes insignificant.
Both models provide a gated approach.
They also both rely on the Sigmoid function in addition to the hyperbolic tangent function.
While the hyperbolic tangent function produces values between -1 and 1,
the Sigmoid function produces values from 0 to 1.
This benefits the memory since low impact data is multiplied by 0 and therefore left out of the equation.

% LSTM
% new cell state = (previous cell state * forget gate) + input gate
% new hidden state = output gate * tanh(new cell state)
% gates:
% \begin{itemize}
%   \item forget gate: sigmoid(input + previous hidden state)
%   \item input gate: sigmoid(input + previous hidden state) * tanh(input + previous hidden state)
%   \item output gate: sigmoid(input + previous hidden state)
% \end{itemize}

% GRU
% new hidden state = (previous hidden state * update gate) + output gate
% gates:
% \begin{itemize}
%   \item reset gate: sigmoid(input + previous hidden state) * previous hidden state
%   \item update gate: 1 - sigmoid(input + previous hidden state)
%   \item ''output gate'': tanh(input + reset gate) * sigmoid(input + previous hidden state)
% \end{itemize}

\subsection{GANs}

To create a NN that could later be used as a DN we first need to understand how a GAN operates.
A GAN consists of two NNs that compete against each other to maximize the quality of the generated data:
a generator network (GN) and a discriminative network (DN).
The GN is constantly trying to fool the DN with data it generates synthetically,
while the DN is constantly trying to figure out if the data it was given is real or synthetic.
If correctly optimized, a GAN may create synthetic data indistinguishable from real data.

\chapter{Related Work}

\section{Flow-based Network Traffic Generation using Generative Adversarial Networks}

Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} introduce a GAN to generate synthetic network flows.
Not only is their work interesting as the source of input for the proposed GAN at a later time,
but as a resource regarding feature extraction for network flows,
which is needed by this thesis.
So even though Ring et al. \cite{ringFlowbasedNetworkTraffic2019a} do not create network packet captures,
their choice of network flow features is immensely valuable to this thesis.

\section{Related Neural Network Models}

In addition, we will need to look into related work regarding NNs, RNNs and GANs, specifically DNs.
Since some approaches for other problems might be applicable to the problem of discriminating if a network flow could have been created from a given sequence of network packets.
This will become clearer once the feature sets of network flows and packets are defined.

% Approaches to generate synthetic network packets that did not use NNs are not really related work,
% since they will not face the same problems as this thesis, therefore they were omitted.

% Bidirectional RNN?

% TODO: search and add some more GANs network flow papaers

\chapter{Approach}

\section{Methodology}

For our DN, we assume that some form of Recurrent Neural Network (RNN) will be suitable as a discriminator of a GAN that can create synthetic network packet captures from network flows.
Since ''basic'' RNNs have the problem of short-term memory, we propose the use of either Long Short Term Memory (LSTM) networks \cite{hochreiterLongShortTermMemory1997} or Gated Recurrent Units (GRUs) \cite{bahdanauNeuralMachineTranslation2016}.
GRUs tend to use fewer operations and states to produce similar or better results than LSTMs.
But depending on the use case, LSTMs might still produce better results.
It is to be determined, which of the two neural networks provides better results for the problem in this thesis.

% TODO: how do we do this?
% TODO: what is the baseline for the datasets?
% TODO: what is the baseline for the NN?

\section{Dataset}

For our dataset we will extract packet sequences from publicly available and well-known network packet captures and create network flows based on these sequences.
Each network packet sequence will be paired with its network flow so that our NN can learn the connection between the two data structures.

Since the end goal in the future work GAN is to generate the complete packet sequences based of the flows,
each sequence of packets will contain as much of the original header information of each packet as needed to reconstruct the network packet they were extracted from.
This way, the DN is prepared to handle whole packet captures and compare them to flows.

Each header attribute is converted into an appropriate representation depending on its data type.
Since the NNs are most efficient with continuous and numerical data, we will convert all fields accordingly, if possible.
Also, we will need to implement a way to create negative samples from the ground truth, which builds our positive samples.
Both will be discussed further in the dataset preprocessing section.

\subsection{Dataset Preparation}

The NN needs to be trained with a significant amount of data, both positive and negative data points so that it can make informed decisions.
This will be provided in two steps.
First we sample flows from multiple packet captures we extracted from popular intrusion detection datasets with network traffic packets.
Those samples represent the ground truth and will be our positive data points.
Then we randomly apply modifications to those samples to create negative data points.

\subsubsection{Sampling}

% FIXME: one capture each might be bad (sound bad)
Instead of combining all flows from our source datasets, we pick only one packet capture each and sample sequences of packets from it.
The sampled packet sequences and flows are stored separately as csv files, as network packet captures are usually slow to process.

For our purpose there are basically two approaches to sampling a network packet capture for flows.
Either by sampling through the packet sequences within the captures or by splitting the capture into intervals and sampling through those partial captures.
Splitting the capture into intervals is less time-intensive, since the resulting packet captures are smaller and much quicker to parse.
Truncated flows should not be a problem, since a capture itself is truncated at beginning and end anyway.
The question however is the length of the intervals.
Should they be determined relative to the size of the capture or should they have a fixed length?
While a fixed-length has better comparability, the percentage is less time-consuming.
Additionally, the variation between packet captures in packet frequency may proof to be problematic.
Setting the fixed length to low might yield the problem that some captures return empty intervals.
Setting it too high might result in too large intervals and therefore partial captures.
Extracting the packet sequences from a complete capture is more time-consuming.
In addition, extracting packet sequences with a relative offset within a capture might provide sequences with different behavior.

Now that we know we want to split the capture first, it is to be decided, how we sample the packet sequences within an interval.
We can make an educated guess that randomly selected samples will yield better results than sequential sampling.
Still the degree to which the sampling is randomized should be determined.
Do we pick random intervals, or do we pick the sequence of packets within each interval at random.
Do we pick every 20th packet sequence, or do we pick each packet sequence at random.
We might test a variety of those methods to choose the packet sequences, when improving the dataset, once we have some results from the NN.

For the first iteration of our dataset we decided, that splitting the capture into intervals first would be the right approach.
We split the captures into intervals relative to its size.
This way we will preserve the behavior of the sampled flows within the larger network within each specific interval.
Afterwards, we will randomly sample up to 250 of TCP and UDP packet sequences alike within those intervals.

Each sampled packet sequence will be exported as a capture first.
The header fields of the packets belonging to the sequence will be stored as a csv file for easier parsing.
Then we use YAF to extract the flow information and yafscii to create flow-based csv files.

\subsubsection{Creation of negative samples}

To create negative samples we use multiple different methods.
The first is falsifying packet and flow features by either sampling them from another datapoint of the dataset or generating random values, which lie within a rough frame of expectation for the feature.
Both processes are randomized.
And both produce three different types of negative samples:

\begin{itemize}
    \item a datapoint, which has modified (falsified) packets
    \item a datapoint, which has a modified (falsified) flow
    \item a datapoint, which has both
\end{itemize}

The second is randomizing the packet order within the packet sequence of a given data point.
The third is to decrease or increase the number of packets of a sequence in a given data point.
To decrease the number of packets we just remove a random number of packets at a random position.
To increase the number of packets we again sample from another datapoint in the dataset and add some of its packets to the current sequence.
And the fourth and final method is mismatching network flows with a different sequence of packets.

\subsection{Conclusion on IP2Vec}

% We had planned to adapt the IP2Vec approach and extend it to handle not only flow attributes but all packet header attributes.
% However since this approach seems fundamentally flawed.

% used IP2Vec to generate source IP based vectors that represent the similarity between IP addresses within a dataset.

% Out of the YAF flow features the following are used by IP2Vec:
% Duration, protocol, source IP, source port, destination IP, destination port, packet count (forward), octet/byte count (forward)

After due consideration, we decided not to use the IP2Vec approach.
This is mainly to the fact that IPs do not influence the flow behavior too much.
The only feature of them that influences a flow significantly is if they are public or local (aka private).
Additionally, IP2Vec sees IPs in the context of a series of packet flows, which represent a network traffic capture,
meaning that it relates the IPs to one another depending on the behavior of their flows within one network.

This is not applicable to our use case, since we use traffic of multiple network packet capture datasets.
To use IP2Vec we would either have to apply it before extracting the flows and mixing the results,
or mix the flows and then apply IP2Vec to it.
The former bares two problems:
First we would risk introducing false correlations between IPs from different datasets.
Second we would assume that for our GAN in the future work, we have a whole set of flows for each IP address, that could provide the data necessary to apply IP2Vec before using the flow as input to the DN.
% FIXME: is this really the problem? THINK ABOUT IT
The later has the problem, that IPs will correlated to each other without ever having been in the same network.
Both approaches have the problem of IPs that might be present in multiple source datasets.

\subsection{Dataset Criteria}

In this section we discuss, which features will be part of your dataset. % and how the NN input will look like.
Network traffic features influencing network flow behavior will be of importance to our neural network.
This will therefore decide the criteria of the dataset features.
Network flows and packet captures of packet sequences will be the source of our data points.

% FIXME: move this UP?
We will generally focus only on IPv4 traffic to keep it simple.
Also we will focus on the TCP and UDP protocols for now.

% Network flows created by YAF contain the following features:
% % start-time, end-time, duration, rtt, proto, sip, sp, dip, dp, iflags, uflags, riflags, ruflags, isn, risn, tag, rtag, pkt, oct, rpkt, roct, end-reason
% Timestamps for start and end time, duration, round trip time, protocol, source IP, source port, destination IP, destination port, TCP flags (forward and reverse), initial TCP sequence number (forward and reverse), first-packet 802.1q VLAN tag (forward and reverse), packet count (forward and reverse), octet/byte count (forward and reverse), the reason for the end of the packet sequence.

\subsubsection{Packet sequence features}

\definecolor{not}{HTML}{949698} % Gray
\definecolor{trivial}{HTML}{ed1b23} % Red
\definecolor{derivation}{HTML}{fff200} % Yellow
\definecolor{feature}{HTML}{41b0e4} % CornflowerBlue

Each packet contains a variety of header fields of varying importance to our work.
One could argue that to properly correlate between flows and packets we would need most of the header fields of each packet.
%Ideally all of them, but some can be ignored and others simplified for the NN.
However most of the header fields can be trivially compared to a flow or verified.

The checksums are a perfect example of the later.
It does not matter to the NN what the checksum reads.
At most it matters if the checksum matches the header.
So instead of feeding the NN the raw checksum and therefore teaching it how checksums work, we would provide its validity with a simple boolean.
The same goes for the header length.
% FIXME: wording
Some header fields are insubstantial for our use case, for example the addresses of the communicating end points.
This includes MAC and IP addresses.

\paragraph{packet header fields}
% TODO: link header tables
Figures 5.1 to 5.5 presents an overview of the packet headers and their fields.
% TODO: color the colors
We color code them in four categories:
\colorbox{not}{\textbf{{Not Used} (gray)}},
useful but \colorbox{trivial}{\textbf{Trivial} (red)},
\colorbox{derivation}{\textbf{Feature Derivations} (yellow)},
and raw \colorbox{feature}{\textbf{Features} (blue)}.

\paragraph{\colorbox{not}{\textbf{{Not Used} (gray)}}} are header fields that do not change either never or in our scenario.
This includes the Version of the IP Header, since we only take IPv4 traffic into account,
and the reserved bits in the IP flags and the TCP header.

\paragraph{\colorbox{trivial}{\textbf{Trivial} (red)}} fields might be useful, but should not be fed to the NN.
Either because they can be easily checked/validated outside the NN and might slow the NN down for no benefit.
Or they do not alter the behavior of a given network flow.
% TODO QUESTION: what about IP class? this could be an interesting feature
% No, but difference between public and local.
This would include checksums, header lengths, urgent pointer, as well as MAC and IP addresses.
% The checksum and header lengths are just verified and passed to the NN as a boolean.

\paragraph{To be decided} fields, might either add to the \colorbox{trivial}{\textbf{Trivial} (red)} or \colorbox{derivation}{\textbf{Feature Derivations} (yellow)} category.
The Sequence and Acknowledgement Numbers can be easily checked outside the NN and can be omitted as well.
However their validity or even position in the flow can make a flow special.

\paragraph{\colorbox{derivation}{\textbf{Feature Derivations} (yellow)}} are fields, which are used as input to the NN,
but are derived in some way beforehand.
Their raw data representation either needs to be adjusted, since it is not optimal as NN input, or can be abstracted in a certain way.
The type of derivation depends on the header field.
The flags are concatenated in a bit stream.
The data offset and IP header length (IHL) are also used as an indicating boolean for TCP and IP options respectively.
The ports are categorical and might need to be abstracted in a useful way.

\paragraph{\colorbox{feature}{\textbf{Features} (blue)}} are fields that we consider unmodified or raw input.
Most of them are numerical and are therefore easily interpretable by the NN.
This goes for the DSCP (ToS), ECN, total Length, ID, TTL, Protocol, and Window Size.
Were as IP and TCP Options keep their binary structure that will be represented as hexadecimal strings.

\paragraph{Network flow features} will be classified in the same categories.
Table 5.7 shows the color-coded flow features.
In this work, we want to compare flow behavior to a sequence of packets.
IP addresses and timestamps might mislead our classifier.
Additionally if they match the header fields in the packet sequence could be easily checked without using a NN.
Therefore both are marked \colorbox{trivial}{\textbf{trivial} (red)}.
Duration, protocol as well as packet and byte count are already present as numerical data, therefore, they can be used as raw \colorbox{feature}{\textbf{features} (blue)}.
The rest of the features are to be \colorbox{derivation}{\textbf{feature derivations} (yellow)}.
There are less parts of the flow that are omitted.
This is because flows are a much higher abstraction of a packet sequence and we try to use as many of its data in order to make up for that.

% Later: "data preprocessing"

% FIXME: remove ip2vec
% Additionally IP2Vec extracts the attriubtes Day and Time from the start timestamp,
% where Day is a boolean represening weekday or weekend
% and Time is the time of day, striped from the date.
% However IP2Vec only uses them temporarly.
% We will extract those attributes from IP2Vecs preprocessing and use them as opposed to the start timestamp,
% since they are more generic, therefore provide better comparability and still anker the flow to a specific time.
% % TODO: export these features for our NN

% We now need to decide, which other attributes of a network flow are of value to our neural network.
% Since IP2Vec only helps us represent the categorical attributes, there are still bool, numerical and continous attributes left for us to use.
% We just need to look at their value for the NN.

% After we decided over the flow attributes %(and they work with the NN)
% we might take a closer look at the packet level attributes, a flow does not provide and decide if the extra implementation is worth it.

% TODO: compare single flow from packet capture? or even extract "single flow" packet captures from the whole packet capture

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|}
        \hline
        bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 \\
        \hline
        0   & \multicolumn{6}{|c|}{\cellcolor{trivial} Desitnation} & \multicolumn{6}{|c|}{\cellcolor{trivial} Source} & \multicolumn{2}{|c|}{\cellcolor{not} Type} \\
        \hline
    \end{tabular}
    \caption{MAC Header (Ethernet II)}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|}
        %\hline
        %bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\
        \hline
        %bit & \multicolumn{8}{|l|}{0} & \multicolumn{8}{|l|}{8} & \multicolumn{8}{|l|}{16} & \multicolumn{8}{|l|}{24} \\
        bit & \multicolumn{8}{|l|}{0} & \multicolumn{8}{|l|}{8} & \multicolumn{3}{|l|}{16} & \multicolumn{13}{|l|}{19} \\
        \hline % \multicolumn{3}{|c|}{Flags} % \makecell{E\\C\\N}
        0   & \multicolumn{4}{|c|}{\cellcolor{not} Version} & \multicolumn{4}{|c|}{\cellcolor{derivation} IHL} & \multicolumn{6}{|c|}{\cellcolor{feature} DSCP} & \multicolumn{2}{|c|}{\cellcolor{feature} ECN} & \multicolumn{16}{|c|}{\cellcolor{feature} Total Length} \\
        \hline % \multicolumn{3}{|c|}{Flags} | \makecell{D\\F} \makecell{M\\F}
        32  & \multicolumn{16}{|c|}{\cellcolor{feature} Identification} & \cellcolor{not} 0 & \cellcolor{derivation} DF & \cellcolor{derivation} MF & \multicolumn{13}{|c|}{\cellcolor{derivation} Fragment Offset} \\
        \hline
        64  & \multicolumn{8}{|c|}{\cellcolor{feature} Time To Live} & \multicolumn{8}{|c|}{\cellcolor{feature} Protocol} & \multicolumn{16}{|c|}{\cellcolor{trivial} Header Checksum} \\
        \hline
        96  & \multicolumn{32}{|c|}{\cellcolor{trivial} Source IP Address} \\
        \hline
        128 & \multicolumn{32}{|c|}{\cellcolor{trivial} Destination IP Address} \\
        \hline
        160 & \multicolumn{32}{|c|}{\cellcolor{feature}}\\\cline{1-1}
        192 & \multicolumn{32}{|c|}{\cellcolor{feature}}\\\cline{1-1}
        224 & \multicolumn{32}{|c|}{\cellcolor{feature}}\\\cline{1-1}
        256 & \multicolumn{32}{|c|}{\multirow{-4}{*}{\cellcolor{feature} Options (if IHL > 5)}} \\
        \hline
    \end{tabular}
    \caption{IPv4 Header}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|} % m{0.2pt}
        \hline
        %bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\
        bit & \multicolumn{4}{|l|}{0} & \multicolumn{3}{|l|}{4} & \multicolumn{9}{|l|}{7} & \multicolumn{16}{|l|}{16} \\
        \hline
        0   & \multicolumn{16}{|c|}{\cellcolor{derivation} Source Port} & \multicolumn{16}{|c|}{\cellcolor{derivation} Desitnation Port} \\
        \hline % \cellcolor{derivation}
        32  & \multicolumn{32}{|c|}{\cellcolor{trivial} Sequence Number} \\
        \hline % \cellcolor{derivation}
        64  & \multicolumn{32}{|c|}{\cellcolor{trivial} Acknowledgement Number} \\
        \hline % \makecell{Data \\Offset}
        96  & \multicolumn{4}{|c|}{\cellcolor{derivation} Data Offset} & \multicolumn{3}{|c|}{\cellcolor{not} Reserved} & \multicolumn{9}{|c|}{\cellcolor{derivation} Flags} & \multicolumn{16}{|c|}{\cellcolor{feature} Windows Size} \\
        \hline
        128 & \multicolumn{16}{|c|}{\cellcolor{trivial} Checksum} & \multicolumn{16}{|c|}{\cellcolor{trivial} Urgent Pointer (if URG set)} \\
        \hline
        160 & \multicolumn{32}{|c|}{\cellcolor{feature}}\\\cline{1-1}
        ... & \multicolumn{32}{|c|}{\multirow{-2}{*}{\cellcolor{feature} Options (if data offset > 5. Padded at the end with "0" bytes if necessary.)}} \\
        \hline
    \end{tabular}
    \caption{TCP Header}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{*{9}{|c}|}
        \hline
        \cellcolor{derivation} \makecell{N\\S} & \cellcolor{derivation} \makecell{C\\W\\R} & \cellcolor{derivation} \makecell{E\\C\\E} & \cellcolor{derivation} \makecell{U\\R\\G} & \cellcolor{derivation} \makecell{A\\C\\K} & \cellcolor{derivation} \makecell{P\\S\\H} & \cellcolor{derivation} \makecell{R\\S\\T} & \cellcolor{derivation} \makecell{S\\Y\\N} & \cellcolor{derivation} \makecell{F\\I\\N} \\
        \hline
    \end{tabular}
    \caption{TCP Flags}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|r*{32}{|c}|}
        \hline
        %bit & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\
        bit & \multicolumn{16}{|l|}{0} & \multicolumn{16}{|l|}{16} \\
        \hline
        0   & \multicolumn{16}{|c|}{\cellcolor{derivation} Source Port} & \multicolumn{16}{|c|}{\cellcolor{derivation} Desitnation Port} \\
        \hline
        32  & \multicolumn{16}{|c|}{\cellcolor{trivial} Length} & \multicolumn{16}{|c|}{\cellcolor{trivial} Checksum} \\
        \hline
    \end{tabular}
    \caption{UDP Header}
\end{table}

% TODO: create table for YAF fields

\subsection{Network Packet Captures used}

For the creation of our datasets we need network packet captures.
For this we take a look at publically available intrusion detection datasets.
Since the goal of our future work is to produce network packet captures, that can be used in the field of NIDSs.
As stated above we will extracted data from multiple popular datasets, such as: CDX, DARPA, MACCDC, MAWI, Simpleweb and UNSW.
% TODO: maybe add a pcap from my personal network
We chose these datasets because of their significant statistical differences, which has been shown in the paper "On generating network traffic datasets with synthetic attacks for intrusion detection" by Cordero et al.
% TODO: OR double check with ID2T

\paragraph{CDX:} the data capture from the U.S. National Security Agency (NSA) published in 2009.

\paragraph{DARPA:} the Intrusion Detection Data Sets from 1998 and 1999.

\paragraph{MACCDC:} the U.S. National CyberWatch Mid-Atlantic Collegiate Cyber Defense Competition (MACCDC) dataset from 2012

\paragraph{MAWI:} backbone traffic from the MAWI labs in Japan? from 2015.

\paragraph{Simpleweb:}

\paragraph{UNSW,} specifically UNSW-NB15, with multiple attacks.

\section{Model(s)}

% TODO: what are the tweaks

For our model we will most likely use a form of Recurrent Neural Network (RNN).
This will help with the vanishing gradient problem and make sure all our previous results will be taken into account.
However it is important to establish a baseline, which will help us establish a measure of progress. % or even success.
This is why for our first model we choose % TODO: WHAT?

Afterwards we will compare the results of various RRNs, such as LSTMs and GRUs, to our baseline.
If there is time left we might take a look into other NN architectures. % like DNN and CNN?

% FIXME: depending on the size of the training split into seperate sections, like before

\section{Training}

\section{Testing}

\section{Results}

\chapter{Evaluation}

\section{Experiments}

\section{Discussion}

\chapter{Conclusion}

% TODO: answer the following question
% "what can you do now, what you couldn't do before?"

\section{Summary}

\section{Research Contribution}

\section{Future Work}

\subsection{Generation of network traffic captures with GANs}

\subsubsection{Federated Learning}

In the last couple of years, federated learning (FL) has become a widely used method to protect user data while training ML models.
It still has flaws regarding privacy, but multiple researchers are working on improving the privacy of FL for example, with differential privacy.

Once the GAN proposed above produces good results, one could combine it with FL.
This way, multiple users could add their traffic datasets without exposing their data.
One real-world application for the GAN could be to deploy it with NIDSs, routers of big ISPs, or the routers of personal users.
This way, the GAN could be trained with real-world traffic without ever needing to see the data.
Some libraries, e.g. PySyft, also provide homomorphic encryption, so the end-user might not have to train on their data themselves.
Instead, they could send the encrypted data to an intermediate server, which computes the model with the encrypted data.
Therefore also never seeing the data itself.

With a GAN that also respects appropriately labeled intrusion detection network captures,
combining this approach with FL could have the potential to solve the issue of creating synthetic network traffic for self learning anomaly based NIDSs.

\printbibliography

\end{document}

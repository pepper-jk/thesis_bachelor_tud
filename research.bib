
@misc{1999DARPAIntrusion,
  title = {1999 {{DARPA Intrusion Detection Evaluation Dataset}} | {{MIT Lincoln Laboratory}}},
  file = {/home/pepper-jk/Zotero/storage/UMHD4N8P/1999-darpa-intrusion-detection-evaluation-dataset.html},
  howpublished = {https://www.ll.mit.edu/r-d/datasets/1999-darpa-intrusion-detection-evaluation-dataset}
}

@misc{AdvancedSearchArXiv,
  title = {Advanced {{Search}} | {{arXiv}} E-Print Repository},
  file = {/home/pepper-jk/Zotero/storage/MGU72887/advanced.html},
  howpublished = {https://arxiv.org/search/advanced?advanced=\&terms-0-operator=AND\&terms-0-term=federated+learning+inference+attack\&terms-0-field=all\&terms-1-operator=AND\&terms-1-term=collaborative+learning+inference+attack\&terms-1-field=all\&terms-2-operator=AND\&terms-2-term=federated+learning+privacy+attack\&terms-2-field=all\&terms-3-operator=AND\&terms-3-term=collaborative+learning+privacy+attack\&terms-3-field=all\&classification-physics\_archives=all\&classification-include\_cross\_list=include\&date-filter\_by=all\_dates\&date-year=\&date-from\_date=\&date-to\_date=\&date-date\_type=submitted\_date\&abstracts=hide\&size=200\&order=-announced\_date\_first}
}

@inproceedings{akaviaLinearRegressionPackedEncrypted2019,
  title = {Linear-{{Regression}} on {{Packed Encrypted Data}} in the {{Two}}-{{Server Model}}},
  booktitle = {Proceedings of the 7th {{ACM Workshop}} on {{Encrypted Computing}} \& {{Applied Homomorphic Cryptography}}  - {{WAHC}}'19},
  author = {Akavia, Adi and Shaul, Hayim and Weiss, Mor and Yakhini, Zohar},
  year = {2019},
  pages = {21--32},
  publisher = {{ACM Press}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3338469.3358942},
  isbn = {978-1-4503-6829-2},
  language = {en}
}

@article{alarcon-aquinoMultiresolutionFIRNeuralnetworkbased2006,
  title = {Multiresolution {{FIR}} Neural-Network-Based Learning Algorithm Applied to Network Traffic Prediction},
  author = {{Alarcon-Aquino}, V. and Barria, J.A.},
  year = {2006},
  month = mar,
  volume = {36},
  pages = {208--220},
  issn = {1558-2442},
  doi = {10.1109/TSMCC.2004.843217},
  abstract = {In this paper, a multiresolution finite-impulse-response (FIR) neural-network-based learning algorithm using the maximal overlap discrete wavelet transform (MODWT) is proposed. The multiresolution learning algorithm employs the analysis framework of wavelet theory, which decomposes a signal into wavelet coefficients and scaling coefficients. The translation-invariant property of the MODWT allows alignment of events in a multiresolution analysis with respect to the original time series and, therefore, preserving the integrity of some transient events. A learning algorithm is also derived for adapting the gain of the activation functions at each level of resolution. The proposed multiresolution FIR neural-network-based learning algorithm is applied to network traffic prediction (real-world aggregate Ethernet traffic data) with comparable results. These results indicate that the generalization ability of the FIR neural network is improved by the proposed multiresolution learning algorithm.},
  file = {/home/pepper-jk/Zotero/storage/8Y8GMBDK/Alarcon-Aquino and Barria - 2006 - Multiresolution FIR neural-network-based learning .pdf;/home/pepper-jk/Zotero/storage/3GTREUAS/1624547.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  keywords = {activation function,Algorithm design and analysis,Discrete wavelet transforms,Ethernet traffic data,Finite impulse response filter,Finite-impulse-response (FIR) neural networks,finite-impulse-response neural network,learning (artificial intelligence),local area networks,maximal overlap discrete wavelet transform,Multiresolution analysis,multiresolution learning,multiresolution learning algorithm,network traffic prediction,neural nets,Signal analysis,signal decomposition,Signal resolution,telecommunication computing,telecommunication traffic,Telecommunication traffic,time series,transfer functions,Transient analysis,translation-invariant property,Wavelet analysis,Wavelet coefficients,wavelet transforms,wavelets},
  number = {2}
}

@inproceedings{allenSelfsimilaritySyntheticTraffic2003,
  title = {On the Self-Similarity of Synthetic Traffic for the Evaluation of Intrusion Detection Systems},
  booktitle = {2003 {{Symposium}} on {{Applications}} and the {{Internet}}, 2003. {{Proceedings}}.},
  author = {Allen, W.H. and Marin, G.A.},
  year = {2003},
  month = jan,
  pages = {242--248},
  issn = {null},
  doi = {10.1109/SAINT.2003.1183056},
  abstract = {The difficulty of quantifying the accuracy of intrusion detection tools against real network data mandates that researchers use simulated attack data for the partial evaluation of such tools. In 1998 and 1999 researchers at MIT Lincoln Labs produced datasets both with and without attack data specifically for use by those interested in developing intrusion detection tools. Because self-similarity has been shown to be a statistical property of real network traffic, this paper examines the attack-free datasets for the presence of self-similarity in various time periods. The results offer insight for researchers who may wish to use specific subsets of the data for testing. Where the results indicate a lack of self-similarity in the data, the likely cause was determined to be either a low activity level or traffic that was dominated by a single protocol, thus forcing the overall distribution to match its own.},
  file = {/home/pepper-jk/Zotero/storage/9IZFSJIU/Allen and Marin - 2003 - On the self-similarity of synthetic traffic for th.pdf;/home/pepper-jk/Zotero/storage/VCLPZZ36/1183056.html},
  keywords = {attack-free datasets,Character generation,computer network management,Computer science,Force measurement,Intrusion detection,intrusion detection tools,IP networks,Local area networks,partial evaluation,protocol,Protocols,real network data,security of data,self-similarity,simulated attack data,statistical property,synthetic traffic,telecommunication security,telecommunication traffic,Telecommunication traffic,testing,Testing,time periods,Traffic control}
}

@misc{ArXivOrgFull,
  title = {{{arXiv}}.Org {{Full Text Search}}},
  file = {/home/pepper-jk/Zotero/storage/935CEFER/search.arxiv.org.html},
  howpublished = {http://search.arxiv.org:8081/?query=distributed+learning+privacy+attack\&in=}
}

@misc{ArXivOrgFulla,
  title = {{{arXiv}}.Org {{Full Text Search}}},
  file = {/home/pepper-jk/Zotero/storage/BR3V58JE/search.arxiv.org.html},
  howpublished = {http://search.arxiv.org:8081/?query=\%28\%22federated+learning\%22+OR+\%22collaborative+learning\%22+OR+\%22distributed+learning\%22+OR+\%22decentralized+learning\%22\%29+AND+\%28\%22inference+attack\%22+OR+\%22privacy+attack\%22+OR+\%22defense+mechanism\%22+OR+\%22privacy-preserving\%22+OR+\%22privacy-protection\%22\%29\&in=}
}

@article{auldBayesianNeuralNetworks2007,
  title = {Bayesian {{Neural Networks}} for {{Internet Traffic Classification}}},
  author = {Auld, Tom and Moore, Andrew W. and Gull, Stephen F.},
  year = {2007},
  month = jan,
  volume = {18},
  pages = {223--239},
  issn = {1941-0093},
  doi = {10.1109/TNN.2006.883010},
  abstract = {Internet traffic identification is an important tool for network management. It allows operators to better predict future traffic matrices and demands, security personnel to detect anomalous behavior, and researchers to develop more realistic traffic models. We present here a traffic classifier that can achieve a high accuracy across a range of application types without any source or destination host-address or port information. We use supervised machine learning based on a Bayesian trained neural network. Though our technique uses training data with categories derived from packet content, training and testing were done using features derived from packet streams consisting of one or more packet headers. By providing classification without access to the contents of packets, our technique offers wider application than methods that require full packet/payloads for classification. This is a powerful advantage, using samples of classified traffic to permit the categorization of traffic based only upon commonly available information},
  file = {/home/pepper-jk/Zotero/storage/F82IEBTF/Auld et al. - 2007 - Bayesian Neural Networks for Internet Traffic Clas.pdf;/home/pepper-jk/Zotero/storage/7UFXYPDR/4049810.html},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {Algorithms,Bayes Theorem,Bayesian methods,Bayesian neural networks,belief networks,Cluster Analysis,computer network management,Computer Security,Information security,Information Storage and Retrieval,Internet,Internet traffic,Internet traffic classification,IP networks,learning (artificial intelligence),Machine learning,network management,network operations,neural nets,neural network applications,Neural networks,Neural Networks (Computer),pattern classification,pattern recognition,Pattern Recognition; Automated,Personnel,Predictive models,Signal Processing; Computer-Assisted,supervised machine learning,telecommunication traffic,Telecommunication traffic,Traffic control,traffic identification,Training data},
  number = {1}
}

@inproceedings{awanPosterReliableAccountable2019,
  title = {Poster: {{A Reliable}} and {{Accountable Privacy}}-{{Preserving Federated Learning Framework}} Using the {{Blockchain}}},
  shorttitle = {Poster},
  booktitle = {Proceedings of the 2019 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Awan, Sana and Li, Fengjun and Luo, Bo and Liu, Mei},
  year = {2019},
  month = nov,
  pages = {2561--2563},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3319535.3363256},
  isbn = {978-1-4503-6747-9},
  language = {en}
}

@article{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archivePrefix = {arXiv},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/5RKTDD3N/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/home/pepper-jk/Zotero/storage/2JXT7SG6/1409.html},
  journal = {arXiv:1409.0473 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{bakerDifferentiatedServicesCode,
  title = {A {{Differentiated Services Code Point}} ({{DSCP}}) for {{Capacity}}-{{Admitted Traffic}}},
  author = {Baker, Fred and Polk, James and Dolly, Martin},
  file = {/home/pepper-jk/Zotero/storage/9RAA7PBF/rfc5865.html},
  howpublished = {https://tools.ietf.org/html/rfc5865},
  language = {en}
}

@misc{bakerDifferentiatedServicesCodea,
  title = {A {{Differentiated Services Code Point}} ({{DSCP}}) for {{Capacity}}-{{Admitted Traffic}}},
  author = {Baker, Fred and Polk, James and Dolly, Martin},
  file = {/home/pepper-jk/Zotero/storage/PCZP4KJ9/rfc5865.html},
  howpublished = {https://tools.ietf.org/html/rfc5865},
  language = {en}
}

@inproceedings{ballePPML19Privacy2019,
  title = {{{PPML}} '19: {{Privacy Preserving Machine Learning}}},
  shorttitle = {{{PPML}} '19},
  booktitle = {Proceedings of the 2019 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Balle, Borja and Gasc{\'o}n, Adri{\`a} and Ohrimenko, Olya and Raykova, Mariana and Schoppmmann, Phillipp and Troncoso, Carmela},
  year = {2019},
  month = nov,
  pages = {2717--2718},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3319535.3353562},
  isbn = {978-1-4503-6747-9},
  language = {en}
}

@article{bhuyanGeneratingReallifeDatasets2015,
  title = {Towards {{Generating Real}}-Life {{Datasets}} for {{Network Intrusion Detection}}},
  author = {Bhuyan, Monowar and Bhattacharyya, Dhruba K and Kalita, Jugal},
  year = {2015},
  month = nov,
  volume = {17},
  pages = {675--693},
  abstract = {With exponential growth in the number of computer applications and the sizes of networks, the potential damage that can be caused by attacks launched over the Internet keeps increasing dramatically. A number of network intrusion detection methods have been developed with respective strengths and weaknesses. The majority of network intrusion detection research and development is still based on simulated datasets due to non-availability of real datasets. A simulated dataset cannot represent a real network intrusion scenario. It is important to generate real and timely datasets to ensure accurate and consistent evaluation of detection methods. In this paper, we propose a systematic approach to generate unbiased fullfeature real-life network intrusion datasets to compensate for the crucial shortcomings of existing datasets. We establish the importance of an intrusion dataset in the development and validation process of detection mechanisms, identify a set of requirements for effective dataset generation, and discuss several attack scenarios and their incorporation in generating datasets. We also establish the effectiveness of the generated dataset in the context of several existing datasets.},
  journal = {International Journal of Network Security}
}

@misc{blessLowerEffortPerHopBehavior,
  title = {A {{Lower}}-{{Effort Per}}-{{Hop Behavior}} ({{LE PHB}}) for {{Differentiated Services}}},
  author = {Bless {$<$}roland.bless@kit.edu{$>$}, Roland},
  file = {/home/pepper-jk/Zotero/storage/HZCNVGHT/rfc8622.html},
  howpublished = {https://tools.ietf.org/html/rfc8622},
  language = {en}
}

@inproceedings{bonawitzPracticalSecureAggregation2017,
  title = {Practical {{Secure Aggregation}} for {{Privacy}}-{{Preserving Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  year = {2017},
  month = oct,
  pages = {1175--1191},
  publisher = {{ACM}},
  address = {{Dallas Texas USA}},
  doi = {10.1145/3133956.3133982},
  isbn = {978-1-4503-4946-8},
  language = {en}
}

@misc{brownleeTrafficFlowMeasurement,
  title = {Traffic {{Flow Measurement}}: {{Architecture}}},
  shorttitle = {Traffic {{Flow Measurement}}},
  author = {Brownlee, Nevil and Mills, Cyndi and Ruth, Greg},
  file = {/home/pepper-jk/Zotero/storage/CCENPCKI/rfc2722.html},
  howpublished = {https://tools.ietf.org/html/rfc2722},
  language = {en}
}

@misc{CableGuyDecember,
  title = {The {{Cable Guy}} - {{December}} 2005},
  file = {/home/pepper-jk/Zotero/storage/GDXLY5AI/bb878133(v=technet.html},
  howpublished = {https://docs.microsoft.com/en-us/previous-versions/bb878133(v=technet.10)},
  language = {en-us}
}

@inproceedings{celikCuriePolicybasedSecure2019,
  title = {Curie: {{Policy}}-Based {{Secure Data Exchange}}},
  shorttitle = {Curie},
  booktitle = {Proceedings of the {{Ninth ACM Conference}} on {{Data}} and {{Application Security}} and {{Privacy}}},
  author = {Celik, Z. Berkay and Acar, Abbas and Aksu, Hidayet and Sheatsley, Ryan and McDaniel, Patrick and Uluagac, A. Selcuk},
  year = {2019},
  month = mar,
  pages = {121--132},
  publisher = {{ACM}},
  address = {{Richardson Texas USA}},
  doi = {10.1145/3292006.3300042},
  isbn = {978-1-4503-6099-9},
  language = {en}
}

@article{chenDifferentiallyPrivateData2018,
  title = {Differentially Private Data Generative Models},
  author = {Chen, Qingrong and Xiang, Chong and Xue, Minhui and Li, Bo and Borisov, Nikita and Kaarfar, Dali and Zhu, Haojin},
  year = {2018},
  file = {/home/pepper-jk/Zotero/storage/UPA27LPT/Chen et al. - 2018 - Differentially private data generative models.pdf;/home/pepper-jk/Zotero/storage/YGSNH3SG/1812.html},
  journal = {arXiv preprint arXiv:1812.02274}
}

@article{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}-{{Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archivePrefix = {arXiv},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/24QU3YKC/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf;/home/pepper-jk/Zotero/storage/8LP89C5X/1406.html},
  journal = {arXiv:1406.1078 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{choLearningPhraseRepresentations2014a,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}-{{Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archivePrefix = {arXiv},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/N2TU8H8L/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf;/home/pepper-jk/Zotero/storage/IB4QN6DQ/1406.html},
  journal = {arXiv:1406.1078 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}-{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  month = oct,
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  archivePrefix = {arXiv},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/EE3ACQZ7/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf;/home/pepper-jk/Zotero/storage/JG94JUDV/1409.html},
  journal = {arXiv:1409.1259 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archivePrefix = {arXiv},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/2ZFRTIRH/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf;/home/pepper-jk/Zotero/storage/F2DIL3WX/1412.html},
  journal = {arXiv:1412.3555 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{ciresanMulticolumnDeepNeural2012,
  title = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  author = {Cire{\c s}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"u}rgen},
  year = {2012},
  month = aug,
  volume = {32},
  pages = {333--338},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.02.023},
  abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.},
  file = {/home/pepper-jk/Zotero/storage/BPGIZKGU/Cireşan et al. - 2012 - Multi-column deep neural network for traffic sign .pdf;/home/pepper-jk/Zotero/storage/NVPHUIL9/S0893608012000524.html},
  journal = {Neural Networks},
  keywords = {Deep neural networks,Image classification,Image preprocessing,Traffic signs},
  language = {en},
  series = {Selected {{Papers}} from {{IJCNN}} 2011}
}

@misc{claiseSpecificationIPFlow,
  title = {Specification of the {{IP Flow Information Export}} ({{IPFIX}}) {{Protocol}} for the {{Exchange}} of {{IP Traffic Flow Information}}},
  author = {Claise {$<$}bclaise@cisco.com{$>$}, Benoit},
  file = {/home/pepper-jk/Zotero/storage/97F5VZSL/rfc5101.html},
  howpublished = {https://tools.ietf.org/html/rfc5101},
  language = {en}
}

@misc{CloudStorCloudStorPowered,
  title = {{{CloudStor}} - {{CloudStor}} Is Powered by {{AARNet}}},
  abstract = {UNSW-NB15 Source-Files is publicly shared},
  file = {/home/pepper-jk/Zotero/storage/9F2SFCJR/2DhnLGDdEECo4ys.html},
  howpublished = {https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys},
  journal = {CloudStor},
  language = {en\_GB}
}

@article{corderoGeneratingNetworkTraffic2019,
  title = {On Generating Network Traffic Datasets with Synthetic Attacks for Intrusion Detection},
  author = {Cordero, Carlos Garcia and Vasilomanolakis, Emmanouil and Wainakh, Aidmar and M{\"u}hlh{\"a}user, Max and {Nadjm-Tehrani}, Simin},
  year = {2019},
  month = may,
  abstract = {Most research in the area of intrusion detection requires datasets to develop, evaluate or compare systems in one way or another. In this field, however, finding suitable datasets is a challenge on to itself. Most publicly available datasets have negative qualities that limit their usefulness. In this article, we propose ID2T (Intrusion Detection Dataset Toolkit) to tackle this problem. ID2T facilitates the creation of labeled datasets by injecting synthetic attacks into background traffic. The injected synthetic attacks blend themselves with the background traffic by mimicking the background traffic's properties to eliminate any trace of ID2T's usage. This work has three core contribution areas. First, we present a comprehensive survey on intrusion detection datasets. In the survey, we propose a classification to group the negative qualities we found in the datasets. Second, the architecture of ID2T is revised, improved and expanded. The architectural changes enable ID2T to inject recent and advanced attacks such as the widespread EternalBlue exploit or botnet communication patterns. The toolkit's new functionality provides a set of tests, known as TIDED (Testing Intrusion Detection Datasets), that help identify potential defects in the background traffic into which attacks are injected. Third, we illustrate how ID2T is used in different use-case scenarios to evaluate the performance of anomaly and signature-based intrusion detection systems in a reproducible manner. ID2T is open source software and is made available to the community to expand its arsenal of attacks and capabilities.},
  archivePrefix = {arXiv},
  eprint = {1905.00304},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/82ED8JWN/Cordero et al. - 2019 - On generating network traffic datasets with synthe.pdf;/home/pepper-jk/Zotero/storage/BY3D65LX/1905.html},
  journal = {arXiv:1905.00304 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryClass = {cs}
}

@inproceedings{corderoID2TDIYDataset2015,
  title = {{{ID2T}}: {{A DIY}} Dataset Creation Toolkit for {{Intrusion Detection Systems}}},
  shorttitle = {{{ID2T}}},
  booktitle = {2015 {{IEEE Conference}} on {{Communications}} and {{Network Security}} ({{CNS}})},
  author = {Cordero, Carlos Garcia and Vasilomanolakis, Emmanouil and Milanov, Nikolay and Koch, Christian and Hausheer, David and M{\"u}hlh{\"a}user, Max},
  year = {2015},
  month = sep,
  pages = {739--740},
  issn = {null},
  doi = {10.1109/CNS.2015.7346912},
  abstract = {Intrusion Detection Systems (IDSs) are an important defense tool against the sophisticated and ever-growing network attacks. These systems need to be evaluated against high quality datasets for correctly assessing their usefulness and comparing their performance. We present an Intrusion Detection Dataset Toolkit (ID2T) for the creation of labeled datasets containing user defined synthetic attacks. The architecture of the toolkit is provided for examination and the example of an injected attack, in real network traffic, is visualized and analyzed. We further discuss the ability of the toolkit of creating realistic synthetic attacks of high quality and low bias.},
  file = {/home/pepper-jk/Zotero/storage/U75KJBHR/Cordero et al. - 2015 - ID2T A DIY dataset creation toolkit for Intrusion.pdf;/home/pepper-jk/Zotero/storage/JHAB7NJY/authors.html},
  keywords = {Computer crime,cyber-attacks,Data mining,data visualisation,Data visualization,defense tool,DIY dataset creation toolkit,Entropy,ID2T,IDS,Intrusion detection,intrusion detection dataset toolkit,intrusion detection systems,IP networks,labeled dataset creation,network attacks,network traffic,Ports (Computers),security of data,telecommunication traffic,user defined synthetic attacks}
}

@misc{CyberResearchCenter,
  title = {Cyber {{Research Center}} - {{Data Sets}} | {{United States Military Academy West Point}}},
  file = {/home/pepper-jk/Zotero/storage/XFWM9TPP/data-sets.html},
  howpublished = {https://www.westpoint.edu/centers-and-research/cyber-research-center/data-sets}
}

@article{dainottiInternetTrafficModeling2008,
  title = {Internet Traffic Modeling by Means of {{Hidden Markov Models}}},
  author = {Dainotti, Alberto and Pescap{\'e}, Antonio and Rossi, Pierluigi Salvo and Palmieri, Francesco and Ventre, Giorgio},
  year = {2008},
  month = oct,
  volume = {52},
  pages = {2645--2662},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2008.05.004},
  abstract = {In this work, we propose a Hidden Markov Model for Internet traffic sources at packet level, jointly analyzing Inter Packet Time and Packet Size. We give an analytical basis and the mathematical details regarding the model, and we test the flexibility of the proposed modeling approach with real traffic traces related to common Internet services with strong differences in terms of both applications/users and protocol behavior: SMTP, HTTP, a network game, and an instant messaging platform. The presented experimental analysis shows that, even maintaining a simple structure, the model is able to achieve good results in terms of estimation of statistical parameters and synthetic series generation, taking into account marginal distributions, mutual, and temporal dependencies. Moreover we show how, by exploiting such temporal dependencies, the model is able to perform short-term prediction by observing traffic from real sources.},
  file = {/home/pepper-jk/Zotero/storage/HAGBERTF/Dainotti et al. - 2008 - Internet traffic modeling by means of Hidden Marko.pdf;/home/pepper-jk/Zotero/storage/NTZVLLY2/S138912860800162X.html},
  journal = {Computer Networks},
  language = {en},
  number = {14}
}

@inproceedings{dasPrivacyWhatWe2019,
  title = {Privacy Is {{What We Care About}}: {{Experimental Investigation}} of {{Federated Learning}} on {{Edge Devices}}},
  shorttitle = {Privacy Is {{What We Care About}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Challenges}} in {{Artificial Intelligence}} and {{Machine Learning}} for {{Internet}} of {{Things}}  - {{AIChallengeIoT}}'19},
  author = {Das, Anirban and Brunschwiler, Thomas},
  year = {2019},
  pages = {39--42},
  publisher = {{ACM Press}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3363347.3363365},
  isbn = {978-1-4503-7013-4},
  language = {en}
}

@misc{DefaultDynamicPort,
  title = {The Default Dynamic Port Range for {{TCP}}/{{IP}} Has Changed in {{Windows Vista}} and in {{Windows Server}} 2008},
  file = {/home/pepper-jk/Zotero/storage/HWCV8JFW/the-default-dynamic-port-range-for-tcp-ip-has-changed-in-windows-vista.html},
  howpublished = {https://support.microsoft.com/en-us/help/929851/the-default-dynamic-port-range-for-tcp-ip-has-changed-in-windows-vista}
}

@misc{DefaultDynamicPorta,
  title = {The Default Dynamic Port Range for {{TCP}}/{{IP}} Has Changed in {{Windows Vista}} and in {{Windows Server}} 2008},
  file = {/home/pepper-jk/Zotero/storage/CANDI2MJ/the-default-dynamic-port-range-for-tcp-ip-has-changed-in-windows-vista.html},
  howpublished = {https://support.microsoft.com/en-us/help/929851/the-default-dynamic-port-range-for-tcp-ip-has-changed-in-windows-vista}
}

@misc{DifferentiatedServicesField,
  title = {Differentiated {{Services Field Codepoints}} ({{DSCP}})},
  file = {/home/pepper-jk/Zotero/storage/GQJHS8HK/dscp-registry.html},
  howpublished = {https://www.iana.org/assignments/dscp-registry/dscp-registry.xhtml}
}

@misc{DifferentiatedServicesFielda,
  title = {Differentiated {{Services Field Codepoints}} ({{DSCP}})},
  file = {/home/pepper-jk/Zotero/storage/USUVHHRR/dscp-registry.html},
  howpublished = {https://www.iana.org/assignments/dscp-registry/dscp-registry.xhtml}
}

@inproceedings{doluiPrivacypreservingMobileApplications2019,
  title = {Towards {{Privacy}}-Preserving {{Mobile Applications}} with {{Federated Learning}}: {{The Case}} of {{Matrix Factorization}} (Poster)},
  shorttitle = {Towards {{Privacy}}-Preserving {{Mobile Applications}} with {{Federated Learning}}},
  booktitle = {Proceedings of the 17th {{Annual International Conference}} on {{Mobile Systems}}, {{Applications}}, and {{Services}}},
  author = {Dolui, Koustabh and Cuba Gyllensten, Illapha and Lowet, Dietwig and Michiels, Sam and Hallez, Hans and Hughes, Danny},
  year = {2019},
  month = jun,
  pages = {624--625},
  publisher = {{ACM}},
  address = {{Seoul Republic of Korea}},
  doi = {10.1145/3307334.3328657},
  isbn = {978-1-4503-6661-8},
  language = {en}
}

@article{engstromIdentifyingStatisticalBias2020,
  title = {Identifying {{Statistical Bias}} in {{Dataset Replication}}},
  author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Steinhardt, Jacob and Madry, Aleksander},
  year = {2020},
  month = may,
  abstract = {Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models' ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14\%) drop in accuracy, even after controlling for a standard human-in-the-loop measure of data quality. We show that after correcting for the identified statistical bias, only an estimated \$3.6\textbackslash\% \textbackslash pm 1.5\textbackslash\%\$ of the original \$11.7\textbackslash\% \textbackslash pm 1.0\textbackslash\%\$ accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available at http://github.com/MadryLab/dataset-replication-analysis .},
  archivePrefix = {arXiv},
  eprint = {2005.09619},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/3AW2SVL2/Engstrom et al. - 2020 - Identifying Statistical Bias in Dataset Replicatio.pdf;/home/pepper-jk/Zotero/storage/EI8GV4C8/2005.html},
  journal = {arXiv:2005.09619 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{EphemeralPortRange,
  title = {The {{Ephemeral Port Range}}},
  file = {/home/pepper-jk/Zotero/storage/UIQGAIWU/ephemeral_ports.html},
  howpublished = {https://www.ncftp.com/ncftpd/doc/misc/ephemeral\_ports.html}
}

@misc{FederatedLearningCollaborative,
  title = {Federated {{Learning}}: {{Collaborative Machine Learning}} without {{Centralized Training Data}}},
  shorttitle = {Federated {{Learning}}},
  abstract = {Posted by Brendan McMahan and Daniel Ramage, Research Scientists Standard machine learning approaches require centralizing the training data...},
  file = {/home/pepper-jk/Zotero/storage/5MIEGIWH/federated-learning-collaborative.html},
  journal = {Google AI Blog},
  language = {en}
}

@article{fengPMFPrivacypreservingHuman2020,
  title = {{{PMF}}: {{A Privacy}}-Preserving {{Human Mobility Prediction Framework}} via {{Federated Learning}}},
  shorttitle = {{{PMF}}},
  author = {Feng, Jie and Rong, Can and Sun, Funing and Guo, Diansheng and Li, Yong},
  year = {2020},
  month = mar,
  volume = {4},
  pages = {1--21},
  issn = {2474-9567, 2474-9567},
  doi = {10.1145/3381006},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  language = {en},
  number = {1}
}

@misc{firoiuExpeditedForwardingPHB,
  title = {An {{Expedited Forwarding PHB}} ({{Per}}-{{Hop Behavior}})},
  author = {Firoiu, Victor and Courtney, William and Davie, Bruce and Charny, Anna},
  file = {/home/pepper-jk/Zotero/storage/DSH2XCHH/rfc3246.html},
  howpublished = {https://tools.ietf.org/html/rfc3246},
  language = {en}
}

@misc{FlowbasedNetworkTraffic,
  title = {Flow-Based Network Traffic Generation Using {{Generative Adversarial Networks}} - {{ScienceDirect}}},
  file = {/home/pepper-jk/Zotero/storage/SQ9NBRY8/S0167404818308393.html},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0167404818308393}
}

@misc{floydAdditionExplicitCongestion,
  title = {The {{Addition}} of {{Explicit Congestion Notification}} ({{ECN}}) to {{IP}}},
  author = {Floyd, Sally and Ramakrishnan, K. K. and Black, David L.},
  file = {/home/pepper-jk/Zotero/storage/LGIVL4DJ/rfc3168.html},
  howpublished = {https://tools.ietf.org/html/rfc3168},
  language = {en}
}

@article{floydDifficultiesSimulatingInternet2001,
  title = {Difficulties in Simulating the Internet},
  author = {Floyd, Sally and Paxson, Vern},
  year = {2001},
  month = aug,
  volume = {9},
  pages = {392--403},
  issn = {1063-6692},
  doi = {10.1109/90.944338},
  abstract = {Simulating how the global Internet behaves is an immensely challenging undertaking because of the network's great heterogeneity and rapid change. The heterogeneity ranges from the individual links that carry the network's traffic, to the protocols that interoperate over the links, the "mix" of different applications used at a site, and the levels of congestion seen on different links. We discuss two key strategies for developing meaningful simulations in the face of these difficulties: searching for invariants and judiciously exploring the simulation parameter space. We finish with a brief look at a collaborative effort within the research community to develop a common network simulator.},
  journal = {IEEE/ACM Transactions on Networking (TON)},
  keywords = {Internet,modeling,simulation},
  number = {4}
}

@article{floydDifficultiesSimulatingInternet2001a,
  title = {Difficulties in Simulating the {{Internet}}},
  author = {Floyd, S. and Paxson, V.},
  year = {2001},
  month = aug,
  volume = {9},
  pages = {392--403},
  issn = {1558-2566},
  doi = {10.1109/90.944338},
  abstract = {Simulating how the global Internet behaves is an immensely challenging undertaking because of the network's great heterogeneity and rapid change. The heterogeneity ranges from the individual links that carry the network's traffic, to the protocols that interoperate over the links, the "mix" of different applications used at a site, and the levels of congestion seen on different links. We discuss two key strategies for developing meaningful simulations in the face of these difficulties: searching for invariants and judiciously exploring the simulation parameter space. We finish with a look at a collaborative effort within the research community to develop a common network simulator.},
  file = {/home/pepper-jk/Zotero/storage/BZRDIBHM/Floyd and Paxson - 2001 - Difficulties in simulating the Internet.pdf;/home/pepper-jk/Zotero/storage/CKGFZV2I/944338.html},
  journal = {IEEE/ACM Transactions on Networking},
  keywords = {Analytical models,Collaboration,collaborative research,Computational modeling,digital simulation,heterogeneous network,Intelligent networks,Internet,Internet simulation,IP networks,network congestion level,network simulator,network traffic,Particle measurements,protocols,Protocols,research initiatives,reviews,simulation parameter space,telecommunication congestion control,telecommunication traffic,Telecommunication traffic,Traffic control},
  number = {4}
}

@article{fuFINEFrameworkDistributed2018,
  title = {{{FINE}}: {{A Framework}} for {{Distributed Learning}} on {{Incomplete Observations}} for {{Heterogeneous Crowdsensing Networks}}},
  shorttitle = {{{FINE}}},
  author = {Fu, Luoyi and Ma, Songjun and Kong, Lingkun and Liang, Shiyu and Wang, Xinbing},
  year = {2018},
  month = jun,
  volume = {26},
  pages = {1092--1109},
  issn = {1063-6692, 1558-2566},
  doi = {10.1109/TNET.2018.2814779},
  journal = {IEEE/ACM Transactions on Networking},
  number = {3}
}

@inproceedings{guReachingDataConfidentiality2019,
  title = {Reaching Data Confidentiality and Model Accountability on the Caltrain},
  booktitle = {2019 49th {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks}} ({{DSN}})},
  author = {Gu, Zhongshu and Jamjoom, Hani and Su, Dong and Huang, Heqing and Zhang, Jialong and Ma, Tengfei and Pendarakis, Dimitrios and Molloy, Ian},
  year = {2019},
  pages = {336--348},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/AM3S5TGD/Gu et al. - 2019 - Reaching data confidentiality and model accountabi.pdf;/home/pepper-jk/Zotero/storage/V6PU27CP/8809502.html}
}

@inproceedings{haDifferentialPrivacyDeep2019,
  title = {Differential {{Privacy}} in {{Deep Learning}}: {{An Overview}}},
  shorttitle = {Differential {{Privacy}} in {{Deep Learning}}},
  booktitle = {2019 {{International Conference}} on {{Advanced Computing}} and {{Applications}} ({{ACOMP}})},
  author = {Ha, Trung and Dang, Tran Khanh and Dang, Tran Tri and Truong, Tuan Anh and Nguyen, Manh Tuan},
  year = {2019},
  pages = {97--102},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/4MA2F6R6/9044259.html}
}

@article{haiderGeneratingRealisticIntrusion2017,
  title = {Generating Realistic Intrusion Detection System Dataset Based on Fuzzy Qualitative Modeling},
  author = {Haider, W. and Hu, J. and Slay, J. and Turnbull, B. P. and Xie, Y.},
  year = {2017},
  month = jun,
  volume = {87},
  pages = {185--192},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2017.03.018},
  abstract = {Prior to deploying any intrusion detection system, it is essential to obtain a realistic evaluation of its performance. However, the major problems currently faced by the research community is the lack of availability of any realistic evaluation dataset and systematic metric for assessing the quantified quality of realism of any intrusion detection system dataset. It is difficult to access and collect data from real-world enterprise networks due to business continuity and integrity issues. In response to this, in this paper, firstly, a metric using a fuzzy logic system based on the Sugeno fuzzy inference model for evaluating the quality of the realism of existing intrusion detection system datasets is proposed. Secondly, based on the proposed metric results, a synthetically realistic next generation intrusion detection systems dataset is designed and generated, and a preliminary analysis conducted to assist in the design of future intrusion detection systems. This generated dataset consists of both normal and abnormal reflections of current network activities occurring at critical cyber infrastructure levels in various enterprises. Finally, using the proposed metric, the generated dataset is analyzed to assess the quality of its realism, with its comparison with publicly available intrusion detection system datasets for verifying its superiority.},
  file = {/home/pepper-jk/Zotero/storage/FXIYFE28/Haider et al. - 2017 - Generating realistic intrusion detection system da.pdf;/home/pepper-jk/Zotero/storage/QDAISVQX/S1084804517301273.html},
  journal = {Journal of Network and Computer Applications},
  keywords = {Dataset evaluation,Dataset realism,Fuzzy logic,HIDS,IDS,IDS dataset,NIDS},
  language = {en}
}

@inproceedings{hanFederatedLearningbasedComputation2019,
  title = {Federated Learning-Based Computation Offloading Optimization in Edge Computing-Supported Internet of Things},
  booktitle = {Proceedings of the {{ACM Turing Celebration Conference}} - {{China}} on   - {{ACM TURC}} '19},
  author = {Han, Yiwen and Li, Ding and Qi, Haotian and Ren, Jianji and Wang, Xiaofei},
  year = {2019},
  pages = {1--5},
  publisher = {{ACM Press}},
  address = {{Chengdu, China}},
  doi = {10.1145/3321408.3321586},
  isbn = {978-1-4503-7158-2},
  language = {en}
}

@inproceedings{heModelInversionAttacks2019,
  title = {Model Inversion Attacks against Collaborative Inference},
  booktitle = {Proceedings of the 35th {{Annual Computer Security Applications Conference}}},
  author = {He, Zecheng and Zhang, Tianwei and Lee, Ruby B.},
  year = {2019},
  month = dec,
  pages = {148--162},
  publisher = {{ACM}},
  address = {{San Juan Puerto Rico}},
  doi = {10.1145/3359789.3359824},
  isbn = {978-1-4503-7628-0},
  language = {en}
}

@inproceedings{hernandez-camposHowRealCan2004,
  title = {How Real Can Synthetic Network Traffic Be},
  booktitle = {In {{Proceedings}} of {{ACM SIGCOMM}} '04 ({{Poster Session}}},
  author = {{Hern{\'a}ndez-campos}, F{\'e}lix and Donelson, F. and Jeffay, Smith Kevin},
  year = {2004},
  abstract = {At some point, most networking researchers simulate or emulate networks in order to understand the behavior or performance of some piece of networking technology. In performing these experiments, there is a clear need to have the ability to generate realistic synthetic workloads; synthetic network traffic whose statistical properties match those of traffic observed on a real network link. In our work, we are developing a method of traffic modeling and synthetic generation that can realize a new level of realism in network experiments that is not possible with existing techniques. Our approach follows the philosophy of using source-level descriptions of network traffic advocated by Floyd and Paxson [1]. We develop a new source-level model of the data exchange dynamics inside a TCP connection that is application-independent, and therefore amenable to modeling the complete mix of TCP},
  file = {/home/pepper-jk/Zotero/storage/GIFZLSRQ/Hernández-campos et al. - 2004 - How real can synthetic network traffic be.pdf;/home/pepper-jk/Zotero/storage/2QS45ESI/summary.html}
}

@inproceedings{hitajDeepModelsGAN2017,
  title = {Deep {{Models Under}} the {{GAN}}: {{Information Leakage}} from {{Collaborative Deep Learning}}},
  shorttitle = {Deep {{Models Under}} the {{GAN}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Hitaj, Briland and Ateniese, Giuseppe and {Perez-Cruz}, Fernando},
  year = {2017},
  month = oct,
  pages = {603--618},
  publisher = {{ACM}},
  address = {{Dallas Texas USA}},
  doi = {10.1145/3133956.3134012},
  file = {/home/pepper-jk/Zotero/storage/KV4S8Z52/Hitaj et al. - 2017 - Deep Models Under the GAN Information Leakage fro.pdf},
  isbn = {978-1-4503-4946-8},
  language = {en}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  volume = {9},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/pepper-jk/Zotero/storage/4QICXW5T/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf;/home/pepper-jk/Zotero/storage/BTE3YUUR/neco.1997.9.8.html},
  journal = {Neural Computation},
  number = {8}
}

@inproceedings{houGenerateImagesObfuscated2020,
  title = {Generate {{Images}} with {{Obfuscated Attributes}} for {{Private Image Classification}}},
  booktitle = {International {{Conference}} on {{Multimedia Modeling}}},
  author = {Hou, Wei and Wang, Dakui and Chen, Xiaojun},
  year = {2020},
  pages = {125--135},
  publisher = {{Springer}},
  file = {/home/pepper-jk/Zotero/storage/IPRACVY5/978-3-030-37734-2_11.html}
}

@inproceedings{ickinPrivacyPreservingQoE2019,
  title = {Privacy {{Preserving QoE Modeling}} Using {{Collaborative Learning}}},
  booktitle = {Proceedings of the 4th {{Internet}}-{{QoE Workshop}} on {{QoE}}-Based {{Analysis}} and {{Management}} of {{Data Communication Networks}}  - {{Internet}}-{{QoE}}'19},
  author = {Ickin, Selim and Vandikas, Konstantinos and Fiedler, Markus},
  year = {2019},
  pages = {13--18},
  publisher = {{ACM Press}},
  address = {{Los Cabos, Mexico}},
  doi = {10.1145/3349611.3355548},
  isbn = {978-1-4503-6927-5},
  language = {en}
}

@inproceedings{jiaMemGuardDefendingBlackBox2019,
  title = {{{MemGuard}}: {{Defending}} against {{Black}}-{{Box Membership Inference Attacks}} via {{Adversarial Examples}}},
  shorttitle = {{{MemGuard}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Jia, Jinyuan and Salem, Ahmed and Backes, Michael and Zhang, Yang and Gong, Neil Zhenqiang},
  year = {2019},
  month = nov,
  pages = {259--274},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3319535.3363201},
  isbn = {978-1-4503-6747-9},
  language = {en}
}

@inproceedings{jiangLightweightPrivacypreservingCollaborative2019,
  title = {On Lightweight Privacy-Preserving Collaborative Learning for Internet-of-Things Objects},
  booktitle = {Proceedings of the {{International Conference}} on {{Internet}} of {{Things Design}} and {{Implementation}}},
  author = {Jiang, Linshan and Tan, Rui and Lou, Xin and Lin, Guosheng},
  year = {2019},
  month = apr,
  pages = {70--81},
  publisher = {{ACM}},
  address = {{Montreal Quebec Canada}},
  doi = {10.1145/3302505.3310070},
  file = {/home/pepper-jk/Zotero/storage/F4RQPLLU/Jiang et al. - 2019 - On lightweight privacy-preserving collaborative le.pdf},
  isbn = {978-1-4503-6283-2},
  language = {en}
}

@inproceedings{jincaoStochasticModelsGenerating2004,
  title = {Stochastic Models for Generating Synthetic {{HTTP}} Source Traffic},
  booktitle = {{{IEEE INFOCOM}} 2004},
  author = {Jin Cao and Cleveland, W.S. and Yuan Gao and Jeffay, K. and Smith, F.D. and Weigle, M.},
  year = {2004},
  month = mar,
  volume = {3},
  pages = {1546-1557 vol.3},
  issn = {0743-166X},
  doi = {10.1109/INFCOM.2004.1354568},
  abstract = {New source-level models for aggregated HTTP traffic and a design for their integration with the TCP transport layer are built and validated using two large-scale collections of TCP/IP packet header traces. An implementation of the models and the design in the ns network simulator can be used to generate web traffic in network simulations},
  file = {/home/pepper-jk/Zotero/storage/SEPSHNIX/Jin Cao et al. - 2004 - Stochastic models for generating synthetic HTTP so.pdf;/home/pepper-jk/Zotero/storage/B6T7ZDJM/1354568.html},
  keywords = {Computer science,Hardware,hypermedia,Internet,IP networks,IP packet header,Network servers,ns network simulator,Peer to peer computing,Protocols,source-level models,Statistics,stochastic models,Stochastic processes,synthetic HTTP source traffic,TCP transport layer,telecommunication traffic,Telecommunication traffic,Traffic control,transport protocols,Web sites,web traffic}
}

@article{kannanFLOWBASEDANALYSIS2005,
  title = {{{FLOW BASED ANALYSIS TO IDENTIFY BOTNET INFECTED SYSTEMS}}},
  author = {Kannan, R and Ramani, A V},
  year = {2005},
  volume = {67},
  pages = {6},
  abstract = {Botnet most widespread and occurs commonly in today's cyber-attacks, resulting in serious threats to our network assets and organization's properties hence there is a high need to detect and prevent the adverse effects of bots. Botnets are collections of compromised computers (Bots) which are remotely controlled by its originator (Bot-Master) under a common Command-and-Control (C\&C) infrastructure. This paper focuses on classifying the bots and the regular hosts in the network through the classification based on their behavior. The goal is to develop a live version of the botnet detection system which identifies a botnet activity in a network, based on traffic behavior analysis and flow intervals which does not depend on packet pay load i.e., they can work on encrypted network communication protocol. The approach is to classify packets based on source IP, destination IP, number of packet, etc., using decision tree which is a classification technique in machine learning. The attribute selection is mainly based on packet attribute and does not consider the data part. The feasibility of the approach is to detect botnet activity without having seen a complete network flow by classifying behavior based on time intervals.},
  file = {/home/pepper-jk/Zotero/storage/TRWRDXVF/Kannan and Ramani - 2005 - FLOW BASED ANALYSIS TO IDENTIFY BOTNET INFECTED SY.pdf},
  journal = {. Vol.},
  language = {en}
}

@article{karpathyDeepVisualSemanticAlignments2015,
  title = {Deep {{Visual}}-{{Semantic Alignments}} for {{Generating Image Descriptions}}},
  author = {Karpathy, Andrej and {Fei-Fei}, Li},
  year = {2015},
  month = apr,
  abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
  archivePrefix = {arXiv},
  eprint = {1412.2306},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/BW6W8Y69/Karpathy and Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Ima.pdf;/home/pepper-jk/Zotero/storage/PGNU458K/1412.html},
  journal = {arXiv:1412.2306 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@incollection{khoslaUndoingDamageDataset2012,
  title = {Undoing the {{Damage}} of {{Dataset Bias}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2012},
  author = {Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A. and Torralba, Antonio},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  year = {2012},
  volume = {7572},
  pages = {158--171},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33718-5_12},
  file = {/home/pepper-jk/Zotero/storage/ZQ4HMI6H/Khosla et al. - 2012 - Undoing the Damage of Dataset Bias.pdf},
  isbn = {978-3-642-33717-8 978-3-642-33718-5},
  language = {en}
}

@article{kungCollaborativePCADCA2017,
  title = {Collaborative {{PCA}}/{{DCA Learning Methods}} for {{Compressive Privacy}}},
  author = {Kung, Sun-Yuan and Chanyaswad, Thee and Chang, J. Morris and Wu, Peiyuan},
  year = {2017},
  month = jul,
  volume = {16},
  pages = {1--18},
  issn = {1539-9087, 1558-3465},
  doi = {10.1145/2996460},
  journal = {ACM Transactions on Embedded Computing Systems},
  language = {en},
  number = {3}
}

@misc{LabeledDatasetIntrusion,
  title = {Labeled {{Dataset}} for {{Intrusion Detection}} - {{SimpleWiki}}},
  file = {/home/pepper-jk/Zotero/storage/UCSM7UB8/Labeled_Dataset_for_Intrusion_Detection.html},
  howpublished = {https://www.simpleweb.org/wiki/index.php/Labeled\_Dataset\_for\_Intrusion\_Detection}
}

@misc{LabeledDatasetIntrusiona,
  title = {Labeled {{Dataset}} for {{Intrusion Detection}} - {{SimpleWiki}}},
  file = {/home/pepper-jk/Zotero/storage/G3Z2BRJA/Labeled_Dataset_for_Intrusion_Detection.html},
  howpublished = {https://www.simpleweb.org/wiki/index.php/Labeled\_Dataset\_for\_Intrusion\_Detection}
}

@inproceedings{liuCloudenabledPrivacypreservingCollaborative2012,
  title = {Cloud-Enabled Privacy-Preserving Collaborative Learning for Mobile Sensing},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Embedded Network Sensor Systems}} - {{SenSys}} '12},
  author = {Liu, Bin and Jiang, Yurong and Sha, Fei and Govindan, Ramesh},
  year = {2012},
  pages = {57},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2426656.2426663},
  isbn = {978-1-4503-1169-4},
  language = {en}
}

@article{liuSecureFederatedLearning2020,
  title = {A {{Secure Federated Learning Framework}} for {{5G Networks}}},
  author = {Liu, Yi and Peng, Jialiang and Kang, Jiawen and Iliyasu, Abdullah M. and Niyato, Dusit and {El-Latif}, Ahmed A. Abd},
  year = {2020},
  file = {/home/pepper-jk/Zotero/storage/UU34374M/Liu et al. - 2020 - A Secure Federated Learning Framework for 5G Netwo.pdf;/home/pepper-jk/Zotero/storage/U2HNRDZK/2005.html},
  journal = {arXiv preprint arXiv:2005.05752}
}

@inproceedings{luCollaborativeLearningCloud2019,
  title = {Collaborative Learning between Cloud and End Devices: An Empirical Study on Location Prediction},
  shorttitle = {Collaborative Learning between Cloud and End Devices},
  booktitle = {Proceedings of the 4th {{ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Lu, Yan and Shu, Yuanchao and Tan, Xu and Liu, Yunxin and Zhou, Mengyu and Chen, Qi and Pei, Dan},
  year = {2019},
  month = nov,
  pages = {139--151},
  publisher = {{ACM}},
  address = {{Arlington Virginia}},
  doi = {10.1145/3318216.3363304},
  isbn = {978-1-4503-6733-2},
  language = {en}
}

@article{luoExploitingDefensesGANBased2020,
  title = {Exploiting {{Defenses}} against {{GAN}}-{{Based Feature Inference Attacks}} in {{Federated Learning}}},
  author = {Luo, Xinjian and Zhu, Xiangqi},
  year = {2020},
  file = {/home/pepper-jk/Zotero/storage/6YK2PSYD/Luo and Zhu - 2020 - Exploiting Defenses against GAN-Based Feature Infe.pdf;/home/pepper-jk/Zotero/storage/TJFLU39E/2004.html},
  journal = {arXiv preprint arXiv:2004.12571}
}

@article{luoPredictablePrivacyPreservingMobile2019,
  title = {Predictable {{Privacy}}-{{Preserving Mobile Crowd Sensing}}: {{A Tale}} of {{Two Roles}}},
  shorttitle = {Predictable {{Privacy}}-{{Preserving Mobile Crowd Sensing}}},
  author = {Luo, Chengwen and Liu, Xiao and Xue, Wanli and Shen, Yiran and Li, Jianqiang and Hu, Wen and Liu, Alex X.},
  year = {2019},
  month = feb,
  volume = {27},
  pages = {361--374},
  issn = {1063-6692, 1558-2566},
  doi = {10.1109/TNET.2019.2890860},
  journal = {IEEE/ACM Transactions on Networking},
  number = {1}
}

@inproceedings{lyuPrivacyPreservingCollaborativeDeep2017,
  title = {Privacy-{{Preserving Collaborative Deep Learning}} with {{Application}} to {{Human Activity Recognition}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Lyu, Lingjuan and He, Xuanli and Law, Yee Wei and Palaniswami, Marimuthu},
  year = {2017},
  month = nov,
  pages = {1219--1228},
  publisher = {{ACM}},
  address = {{Singapore Singapore}},
  doi = {10.1145/3132847.3132990},
  isbn = {978-1-4503-4918-5},
  language = {en}
}

@inproceedings{mandalPrivFLPracticalPrivacypreserving2019,
  title = {{{PrivFL}}: {{Practical Privacy}}-Preserving {{Federated Regressions}} on {{High}}-Dimensional {{Data}} over {{Mobile Networks}}},
  shorttitle = {{{PrivFL}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGSAC Conference}} on {{Cloud Computing Security Workshop}} - {{CCSW}}'19},
  author = {Mandal, Kalikinkar and Gong, Guang},
  year = {2019},
  pages = {57--68},
  publisher = {{ACM Press}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3338466.3358926},
  file = {/home/pepper-jk/Zotero/storage/BWJWN7EC/Mandal and Gong - 2019 - PrivFL Practical Privacy-preserving Federated Reg.pdf},
  isbn = {978-1-4503-6826-1},
  language = {en}
}

@inproceedings{maoNovelUserMembership2019,
  title = {A {{Novel User Membership Leakage Attack}} in {{Collaborative Deep Learning}}},
  booktitle = {2019 11th {{International Conference}} on {{Wireless Communications}} and {{Signal Processing}} ({{WCSP}})},
  author = {Mao, Yaoru and Zhu, Xiaoyan and Zheng, Wenbin and Yuan, Danni and Ma, Jianfeng},
  year = {2019},
  pages = {1--6},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/2UAQZY8L/8927871.html}
}

@inproceedings{maPrivacyPreservingTensorFactorization2019,
  title = {Privacy-{{Preserving Tensor Factorization}} for {{Collaborative Health Data Analysis}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Ma, Jing and Zhang, Qiuchen and Lou, Jian and Ho, Joyce C. and Xiong, Li and Jiang, Xiaoqian},
  year = {2019},
  month = nov,
  pages = {1291--1300},
  publisher = {{ACM}},
  address = {{Beijing China}},
  doi = {10.1145/3357384.3357878},
  file = {/home/pepper-jk/Zotero/storage/VJ9IPD2H/Ma et al. - 2019 - Privacy-Preserving Tensor Factorization for Collab.pdf},
  isbn = {978-1-4503-6976-3},
  language = {en}
}

@misc{MAWILabDataSet,
  title = {{{MAWILab}} - {{Data}} Set},
  file = {/home/pepper-jk/Zotero/storage/X2UAKKCB/data.html},
  howpublished = {http://www.fukuda-lab.org/mawilab/data.html}
}

@misc{MAWILabDataSeta,
  title = {{{MAWILab}} - {{Data}} Set - 2015/06/21},
  file = {/home/pepper-jk/Zotero/storage/KNGW4RW8/20150621.html},
  howpublished = {http://www.fukuda-lab.org/mawilab/v1.1/2015/06/21/20150621.html}
}

@misc{MAWILabDataSetb,
  title = {{{MAWILab}} - {{Data}} Set - 2015/06/21},
  file = {/home/pepper-jk/Zotero/storage/LW288ANB/20150621.html},
  howpublished = {http://www.fukuda-lab.org/mawilab/v1.1/2015/06/21/20150621.html}
}

@misc{MAWILabHome,
  title = {{{MAWILab}} - {{Home}}},
  file = {/home/pepper-jk/Zotero/storage/GVRPKANF/mawilab.html},
  howpublished = {http://www.fukuda-lab.org/mawilab/}
}

@article{mcmahanCommunicationEfficientLearningDeep2017,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2017},
  month = feb,
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archivePrefix = {arXiv},
  eprint = {1602.05629},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/QHPWNR68/McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf;/home/pepper-jk/Zotero/storage/MEJY8NZ2/1602.html},
  journal = {arXiv:1602.05629 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{mcmahanCommunicationEfficientLearningDeep2017a,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2017},
  month = feb,
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archivePrefix = {arXiv},
  eprint = {1602.05629},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/EBRS33L7/McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf;/home/pepper-jk/Zotero/storage/HSLPNYRQ/1602.html},
  journal = {arXiv:1602.05629 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{mcmahanLearningDifferentiallyPrivate2018,
  title = {Learning {{Differentially Private Recurrent Language Models}}},
  author = {McMahan, H. Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  year = {2018},
  month = feb,
  abstract = {We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes ``large step'' updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.},
  archivePrefix = {arXiv},
  eprint = {1710.06963},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/KLK38HZH/McMahan et al. - 2018 - Learning Differentially Private Recurrent Language.pdf},
  journal = {arXiv:1710.06963 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{melisInferenceAttacksCollaborative2018,
  title = {Inference Attacks against Collaborative Learning},
  author = {Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
  year = {2018},
  journal = {arXiv preprint arXiv:1805.04049}
}

@inproceedings{meruguDistributedLearningFramework2005,
  title = {A Distributed Learning Framework for Heterogeneous Data Sources},
  booktitle = {Proceeding of the Eleventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery in Data Mining  - {{KDD}} '05},
  author = {Merugu, Srujana and Ghosh, Joydeep},
  year = {2005},
  pages = {208},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1081870.1081896},
  isbn = {978-1-59593-135-1},
  language = {en}
}

@inproceedings{meurischPNetPrivacypreservingPersonalization2019,
  title = {\{\vphantom\}{{P}}\vphantom\{\}{{Net}}: Privacy-Preserving Personalization of {{AI}}-Based Models by Anonymous Inter-Person Similarity Networks},
  shorttitle = {\{\vphantom\}{{P}}\vphantom\{\}{{Net}}},
  booktitle = {Proceedings of the 16th {{EAI International Conference}} on {{Mobile}} and {{Ubiquitous Systems}}: {{Computing}}, {{Networking}} and {{Services}}},
  author = {Meurisch, Christian and Kauschke, Sebastian and Grube, Tim and Bayrak, Bekir and M{\"u}hlh{\"a}user, Max},
  year = {2019},
  month = nov,
  pages = {60--69},
  publisher = {{ACM}},
  address = {{Houston Texas}},
  doi = {10.1145/3360774.3360819},
  isbn = {978-1-4503-7283-1},
  language = {en}
}

@inproceedings{meurischPrivacypreservingAIServices2020,
  title = {Privacy-Preserving {{AI Services Through Data Decentralization}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Meurisch, Christian and Bayrak, Bekir and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  month = apr,
  pages = {190--200},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3366423.3380106},
  isbn = {978-1-4503-7023-3},
  language = {en}
}

@incollection{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {3111--3119},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/pepper-jk/Zotero/storage/DMZ8XBV8/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/home/pepper-jk/Zotero/storage/SUMNXUAP/5021-distributed-representations-of-words-and-phrases-and.html}
}

@misc{MITLincolnLaboratory,
  title = {{{MIT Lincoln Laboratory}}: {{DARPA Intrusion Detection Evaluation}}},
  file = {/home/pepper-jk/Zotero/storage/QMEBU5AP/index.html},
  howpublished = {https://archive.ll.mit.edu/ideval/data/1999/training/week1/index.html}
}

@inproceedings{nasrComprehensivePrivacyAnalysis,
  title = {Comprehensive {{Privacy Analysis}} of {{Deep Learning}}},
  booktitle = {2019 Ieee Symposium on Security and Privacy},
  author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  file = {/home/pepper-jk/Zotero/storage/XENMHTR8/Nasr et al. - Comprehensive Privacy Analysis of Deep Learning.pdf}
}

@article{nasrComprehensivePrivacyAnalysis2018,
  title = {Comprehensive Privacy Analysis of Deep Learning: {{Stand}}-Alone and Federated Learning under Passive and Active White-Box Inference Attacks},
  shorttitle = {Comprehensive Privacy Analysis of Deep Learning},
  author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  year = {2018},
  file = {/home/pepper-jk/Zotero/storage/BA8A2K99/Nasr et al. - 2018 - Comprehensive privacy analysis of deep learning S.pdf;/home/pepper-jk/Zotero/storage/A9XNJEVK/1812.html},
  journal = {arXiv preprint arXiv:1812.00910}
}

@inproceedings{nasrComprehensivePrivacyAnalysis2019,
  title = {Comprehensive Privacy Analysis of Deep Learning: {{Passive}} and Active White-Box Inference Attacks against Centralized and Federated Learning},
  shorttitle = {Comprehensive Privacy Analysis of Deep Learning},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  year = {2019},
  pages = {739--753},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/66UBSWTC/8835245.html}
}

@misc{Netflix,
  title = {Netflix},
  howpublished = {https://www.netflix.com/watch/81010665?trackId=200254291\&tctx=0\%2C0\%2Ccb955936-6742-48ae-be13-5e45ed50034f-583309078\%2Cb0d200af-feab-4baa-bac7-021251140014\_7706855X6XX1586788265610\%2Cb0d200af-feab-4baa-bac7-021251140014\_ROOT}
}

@misc{nicholsDefinitionDifferentiatedServices,
  title = {Definition of the {{Differentiated Services Field}} ({{DS Field}}) in the {{IPv4}} and {{IPv6 Headers}}},
  author = {Nichols, Kathleen and Black, David L. and Blake, Steven and Baker, Fred},
  file = {/home/pepper-jk/Zotero/storage/MV5GYE3V/rfc2474.html},
  howpublished = {https://tools.ietf.org/html/rfc2474},
  language = {en}
}

@inproceedings{ochejaConnectingDecentralizedLearning2018,
  title = {Connecting Decentralized Learning Records: A Blockchain Based Learning Analytics Platform},
  shorttitle = {Connecting Decentralized Learning Records},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Analytics}} and {{Knowledge}}},
  author = {Ocheja, Patrick and Flanagan, Brendan and Ogata, Hiroaki},
  year = {2018},
  month = mar,
  pages = {265--269},
  publisher = {{ACM}},
  address = {{Sydney New South Wales Australia}},
  doi = {10.1145/3170358.3170365},
  isbn = {978-1-4503-6400-3},
  language = {en}
}

@patent{ouderkirkMethodSystemGenerating2006,
  title = {Method and System for Generating Synthetic Digital Network Traffic},
  author = {Ouderkirk, Steven},
  year = {2006},
  month = dec,
  assignee = {Battelle Memorial Institute Inc},
  file = {/home/pepper-jk/Zotero/storage/5KYKBDX3/Ouderkirk - 2006 - Method and system for generating synthetic digital.pdf},
  keywords = {actor,agent,method,recited,system},
  nationality = {US},
  number = {US20060274659A1}
}

@article{papernotSemisupervisedKnowledgeTransfer2017,
  title = {Semi-Supervised {{Knowledge Transfer}} for {{Deep Learning}} from {{Private Training Data}}},
  author = {Papernot, Nicolas and Abadi, Mart{\'i}n and Erlingsson, {\'U}lfar and Goodfellow, Ian and Talwar, Kunal},
  year = {2017},
  month = mar,
  abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1610.05755},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/EP73IFRK/Papernot et al. - 2017 - Semi-supervised Knowledge Transfer for Deep Learni.pdf;/home/pepper-jk/Zotero/storage/93WARJTX/1610.html},
  journal = {arXiv:1610.05755 [cs, stat]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{papstEmbracingOpportunitiesLivestock2019,
  title = {Embracing {{Opportunities}} of {{Livestock Big Data Integration}} with {{Privacy Constraints}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on the {{Internet}} of {{Things}}  - {{IoT}} 2019},
  author = {Papst, Franz and Saukh, Olga and R{\"o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J{\"u}rgen and {Egger-Danner}, Christa},
  year = {2019},
  pages = {1--4},
  publisher = {{ACM Press}},
  address = {{Bilbao, Spain}},
  doi = {10.1145/3365871.3365900},
  isbn = {978-1-4503-7207-7},
  language = {en}
}

@misc{PCAPFilesUS,
  title = {{{PCAP}} Files from the {{US National CyberWatch Mid}}-{{Atlantic Collegiate Cyber Defense Competition}} ({{MACCDC}})},
  abstract = {Network forensics, packet sniffers and IT security products. Download NetworkMiner and other free software for network security analysis.},
  file = {/home/pepper-jk/Zotero/storage/ZISDBKCN/www.netresec.com.html},
  howpublished = {https://www.netresec.com/?page=MACCDC},
  journal = {Netresec}
}

@inproceedings{pejoPricePrivacyCollaborative2018,
  title = {The {{Price}} of {{Privacy}} in {{Collaborative Learning}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Pej{\'o}, Bal{\'a}zs and Tang, Qiang and Bicz{\'o}k, Gergely},
  year = {2018},
  month = oct,
  pages = {2261--2263},
  publisher = {{ACM}},
  address = {{Toronto Canada}},
  doi = {10.1145/3243734.3278525},
  file = {/home/pepper-jk/Zotero/storage/5F29WBUD/Pejó et al. - 2018 - The Price of Privacy in Collaborative Learning.pdf},
  isbn = {978-1-4503-5693-0},
  language = {en}
}

@misc{phiIllustratedGuideLSTM2020,
  title = {Illustrated {{Guide}} to {{LSTM}}'s and {{GRU}}'s: {{A}} Step by Step Explanation},
  shorttitle = {Illustrated {{Guide}} to {{LSTM}}'s and {{GRU}}'s},
  author = {Phi, Michael},
  year = {2020},
  month = jun,
  abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I'm Michael, and I'm a Machine\ldots},
  file = {/home/pepper-jk/Zotero/storage/ST5ISC54/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html},
  howpublished = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
  journal = {Medium},
  language = {en}
}

@inproceedings{puriPrivacyPreservingPregnancy2019,
  title = {Privacy Preserving Pregnancy Weight Gain Management: Demo Abstract},
  shorttitle = {Privacy Preserving Pregnancy Weight Gain Management},
  booktitle = {Proceedings of the 17th {{Conference}} on {{Embedded Networked Sensor Systems}}},
  author = {Puri, Chetanya and Dolui, Koustabh and Kooijman, Gerben and Masculo, Felipe and Van Sambeek, Shannon and Boer, Sebastiaan Den and Michiels, Sam and Hallez, Hans and Luca, Stijn and Vanrumste, Bart},
  year = {2019},
  month = nov,
  pages = {398--399},
  publisher = {{ACM}},
  address = {{New York New York}},
  doi = {10.1145/3356250.3361941},
  isbn = {978-1-4503-6950-3},
  language = {en}
}

@article{pustozerovaInformationLeaksFederated,
  title = {Information {{Leaks}} in {{Federated Learning}}},
  author = {Pustozerova, Anastasia and Mayer, Rudolf},
  file = {/home/pepper-jk/Zotero/storage/8QZXM2E5/Pustozerova and Mayer - Information Leaks in Federated Learning.pdf}
}

@article{qayyumSecureRobustMachine2020,
  title = {Secure and {{Robust Machine Learning}} for {{Healthcare}}: {{A Survey}}},
  shorttitle = {Secure and {{Robust Machine Learning}} for {{Healthcare}}},
  author = {Qayyum, Adnan and Qadir, Junaid and Bilal, Muhammad and {Al-Fuqaha}, Ala},
  year = {2020},
  month = jan,
  abstract = {Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from onedimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.},
  archivePrefix = {arXiv},
  eprint = {2001.08103},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/9LRRN48H/Qayyum et al. - 2020 - Secure and Robust Machine Learning for Healthcare.pdf},
  journal = {arXiv:2001.08103 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@misc{rajahalmeIPv6FlowLabel,
  title = {{{IPv6 Flow Label Specification}}},
  author = {Rajahalme, Jarno and Conta, Alex and Deering, Steve and Carpenter, Brian E.},
  file = {/home/pepper-jk/Zotero/storage/JLZEY76R/rfc3697.html},
  howpublished = {https://tools.ietf.org/html/rfc3697},
  language = {en}
}

@article{rasouliFedGANFederatedGenerative2020,
  title = {{{FedGAN}}: {{Federated Generative Adversarial Networks}} for {{Distributed Data}}},
  shorttitle = {{{FedGAN}}},
  author = {Rasouli, Mohammad and Sun, Tao and Rajagopal, Ram},
  year = {2020},
  month = jun,
  abstract = {We propose Federated Generative Adversarial Network (FedGAN) for training a GAN across distributed sources of non-independent-and-identically-distributed data sources subject to communication and privacy constraints. Our algorithm uses local generators and discriminators which are periodically synced via an intermediary that averages and broadcasts the generator and discriminator parameters. We theoretically prove the convergence of FedGAN with both equal and two time-scale updates of generator and discriminator, under standard assumptions, using stochastic approximations and communication efficient stochastic gradient descents. We experiment FedGAN on toy examples (2D system, mixed Gaussian, and Swiss role), image datasets (MNIST, CIFAR-10, and CelebA), and time series datasets (household electricity consumption and electric vehicle charging sessions). We show FedGAN converges and has similar performance to general distributed GAN, while reduces communication complexity. We also show its robustness to reduced communications.},
  archivePrefix = {arXiv},
  eprint = {2006.07228},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/NMCKX494/Rasouli et al. - 2020 - FedGAN Federated Generative Adversarial Networks .pdf;/home/pepper-jk/Zotero/storage/I7XF34UA/2006.html},
  journal = {arXiv:2006.07228 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{remyPhilipperemyCondRnn2020,
  title = {Philipperemy/Cond\_rnn},
  author = {R{\'e}my, Philippe},
  year = {2020},
  month = aug,
  abstract = {Conditional RNNs made easy with Tensorflow and Keras.},
  copyright = {MIT License         ,                 MIT License}
}

@inproceedings{riaziPrivacypreservingDeepLearning2018,
  title = {Privacy-Preserving Deep Learning and Inference},
  booktitle = {Proceedings of the {{International Conference}} on {{Computer}}-{{Aided Design}}},
  author = {Riazi, M. Sadegh and Koushanfar, Farinaz},
  year = {2018},
  month = nov,
  pages = {1--4},
  publisher = {{ACM}},
  address = {{San Diego California}},
  doi = {10.1145/3240765.3274560},
  isbn = {978-1-4503-5950-4},
  language = {en}
}

@article{ringFlowbasedNetworkTraffic2019,
  title = {Flow-Based Network Traffic Generation Using {{Generative Adversarial Networks}}},
  author = {Ring, Markus and Schl{\"o}r, Daniel and Landes, Dieter and Hotho, Andreas},
  year = {2019},
  month = may,
  volume = {82},
  pages = {156--172},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2018.12.012},
  abstract = {Flow-based data sets are necessary for evaluating network-based intrusion detection systems (NIDS). In this work, we propose a novel methodology for generating realistic flow-based network traffic. Our approach is based on Generative Adversarial Networks (GANs) which achieve good results for image generation. A major challenge lies in the fact that GANs can only process continuous attributes. However, flow-based data inevitably contain categorical attributes such as IP addresses or port numbers. Therefore, we propose three different preprocessing approaches for flow-based data in order to transform them into continuous values. Further, we present a new method for evaluating the generated flow-based network traffic which uses domain knowledge to define quality tests. We use the three approaches for generating flow-based network traffic based on the CIDDS-001 data set. Experiments indicate that two of the three approaches are able to generate high quality data.},
  file = {/home/pepper-jk/Zotero/storage/3HK5PSJ6/Ring et al. - 2019 - Flow-based network traffic generation using Genera.pdf;/home/pepper-jk/Zotero/storage/EETRZ3TK/S0167404818308393.html},
  journal = {Computers \& Security},
  keywords = {GANs,Generation,IDS,NetFlow,TTUR WGAN-GP},
  language = {en}
}

@article{ringFlowbasedNetworkTraffic2019a,
  title = {Flow-Based Network Traffic Generation Using {{Generative Adversarial Networks}}},
  author = {Ring, Markus and Schl{\"o}r, Daniel and Landes, Dieter and Hotho, Andreas},
  year = {2019},
  month = may,
  volume = {82},
  pages = {156--172},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2018.12.012},
  abstract = {Flow-based data sets are necessary for evaluating network-based intrusion detection systems (NIDS). In this work, we propose a novel methodology for generating realistic flow-based network traffic. Our approach is based on Generative Adversarial Networks (GANs) which achieve good results for image generation. A major challenge lies in the fact that GANs can only process continuous attributes. However, flow-based data inevitably contain categorical attributes such as IP addresses or port numbers. Therefore, we propose three different preprocessing approaches for flow-based data in order to transform them into continuous values. Further, we present a new method for evaluating the generated flow-based network traffic which uses domain knowledge to define quality tests. We use the three approaches for generating flow-based network traffic based on the CIDDS-001 data set. Experiments indicate that two of the three approaches are able to generate high quality data.},
  file = {/home/pepper-jk/Zotero/storage/8A7IXMQ5/Ring et al. - 2019 - Flow-based network traffic generation using Genera.pdf;/home/pepper-jk/Zotero/storage/SAYCCDWJ/S0167404818308393.html},
  journal = {Computers \& Security},
  keywords = {GANs,Generation,IDS,NetFlow,TTUR WGAN-GP},
  language = {en}
}

@inproceedings{ringIP2VecLearningSimilarities2017,
  title = {{{IP2Vec}}: {{Learning Similarities Between IP Addresses}}},
  shorttitle = {{{IP2Vec}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Ring, Markus and Dallmann, Alexander and Landes, Dieter and Hotho, Andreas},
  year = {2017},
  month = nov,
  pages = {657--666},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2017.93},
  abstract = {IP Addresses are a central part of packet- and flow-based network data. However, visualization and similarity computation of IP Addresses are challenging to due the missing natural order. This paper presents a novel similarity measure IP2Vec for IP Addresses that builds on ideas from Word2Vec, a popular approach in text mining. The key idea is to learn similarities by extracting available context information from network data. IP Addresses are similar if they appear in similar contexts. Thus, IP2Vec is automatically derived from the given network data set. The proposed approach is evaluated experimentally on two public flow-based data sets. In particular, we demonstrate the effectiveness of clustering IP Addresses within a botnet data set. In addition, we use visualization methods to analyse the learned similarities in more detail. These experiments indicate that IP2Vec is well suited to capture the similarity of IP Addresses based on their network communications.},
  file = {/home/pepper-jk/Zotero/storage/P7PXYP6U/Ring et al. - 2017 - IP2Vec Learning Similarities Between IP Addresses.pdf;/home/pepper-jk/Zotero/storage/RGBE27ZE/8215725.html},
  keywords = {Biological neural networks,botnet data set,computer network security,data mining,Data mining,Feature extraction,flow based network data,Internet,Intrusion Detection,invasive software,IP Addresses,IP networks,IP2Vec,learning (artificial intelligence),network communications,Neurons,packet based network data,pattern clustering,similarities learning,Similarity Measure,text mining,Training,visualization,Vocabulary,Word2Vec}
}

@article{ryffelGenericFrameworkPrivacy2018,
  title = {A Generic Framework for Privacy Preserving Deep Learning},
  author = {Ryffel, Theo and Trask, Andrew and Dahl, Morten and Wagner, Bobby and Mancuso, Jason and Rueckert, Daniel and {Passerat-Palmbach}, Jonathan},
  year = {2018},
  month = nov,
  abstract = {We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.},
  archivePrefix = {arXiv},
  eprint = {1811.04017},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/MTSC3H2E/Ryffel et al. - 2018 - A generic framework for privacy preserving deep le.pdf;/home/pepper-jk/Zotero/storage/AP8R9MUU/1811.html},
  journal = {arXiv:1811.04017 [cs, stat]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{samantEnhancingNeuralNetwork2001,
  title = {Enhancing {{Neural Network Traffic Incident}}-{{Detection Algorithms Using Wavelets}}},
  author = {Samant, A. and Adeli, H.},
  year = {2001},
  volume = {16},
  pages = {239--245},
  issn = {1467-8667},
  doi = {10.1111/0885-9507.00229},
  abstract = {Researchers have presented freeway traffic incident-detection algorithms by combining the adaptive learning capability of neural networks with imprecision modeling capability of fuzzy logic. In this article it is shown that the performance of a fuzzy neural network algorithm can be improved through preprocessing of data using a wavelet-based feature-extraction model. In particular, the discrete wavelet transform (DWT) denoising and feature-extraction model proposed by Samant and Adeli (2000) is combined with the fuzzy neural network approach presented by Hsiao et al. (1994). It is shown that substantial improvement can be achieved using the data filtered by DWT. Use of the wavelet theory to denoise the traffic data increases the incident-detection rate, reduces the false-alarm rate and the incident-detection time, and improves the convergence of the neural network training algorithm substantially.},
  file = {/home/pepper-jk/Zotero/storage/3VSKPQQ7/Samant and Adeli - 2001 - Enhancing Neural Network Traffic Incident-Detectio.pdf;/home/pepper-jk/Zotero/storage/PXI9JYQ9/0885-9507.html},
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  language = {en},
  number = {4}
}

@inproceedings{scheffelWidgetWidgetYou2017,
  title = {Widget, Widget as You Lead, {{I}} Am Performing Well Indeed!: Using Results from an Exploratory Offline Study to Inform an Empirical Online Study about a Learning Analytics Widget in a Collaborative Learning Environment},
  shorttitle = {Widget, Widget as You Lead, {{I}} Am Performing Well Indeed!},
  booktitle = {Proceedings of the {{Seventh International Learning Analytics}} \& {{Knowledge Conference}}},
  author = {Scheffel, Maren and Drachsler, Hendrik and Kreijns, Karel and {de Kraker}, Joop and Specht, Marcus},
  year = {2017},
  month = mar,
  pages = {289--298},
  publisher = {{ACM}},
  address = {{Vancouver British Columbia Canada}},
  doi = {10.1145/3027385.3027428},
  isbn = {978-1-4503-4870-6},
  language = {en}
}

@misc{ScienceDirectChooseOrganization,
  title = {{{ScienceDirect Choose Organization}}},
  file = {/home/pepper-jk/Zotero/storage/NH27DDR4/chooseorg.html},
  howpublished = {https://www.sciencedirect.com/user/chooseorg?targetURL=\%2Fscience\%2Farticle\%2Fpii\%2FS0167404818308393}
}

@misc{ServiceNameTransport,
  title = {Service {{Name}} and {{Transport Protocol Port Number Registry}}},
  file = {/home/pepper-jk/Zotero/storage/RM4WBA6N/service-names-port-numbers.html},
  howpublished = {https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml}
}

@article{shenComputationOffloadingMultiple2020,
  title = {Computation {{Offloading}} with {{Multiple Agents}} in {{Edge}}-{{Computing}}\textendash{{Supported IoT}}},
  author = {Shen, Shihao and Han, Yiwen and Wang, Xiaofei and Wang, Yan},
  year = {2020},
  month = feb,
  volume = {16},
  pages = {1--27},
  issn = {1550-4859, 1550-4867},
  doi = {10.1145/3372025},
  journal = {ACM Transactions on Sensor Networks},
  language = {en},
  number = {1}
}

@inproceedings{shokriPrivacyPreservingDeepLearning2015,
  title = {Privacy-{{Preserving Deep Learning}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}} - {{CCS}} '15},
  author = {Shokri, Reza and Shmatikov, Vitaly},
  year = {2015},
  pages = {1310--1321},
  publisher = {{ACM Press}},
  address = {{Denver, Colorado, USA}},
  doi = {10.1145/2810103.2813687},
  isbn = {978-1-4503-3832-5},
  language = {en}
}

@inproceedings{solimanDIVaDecentralizedIdentity2015,
  title = {{{DIVa}}: {{Decentralized Identity Validation}} for {{Social Networks}}},
  shorttitle = {{{DIVa}}},
  booktitle = {Proceedings of the 2015 {{IEEE}}/{{ACM International Conference}} on {{Advances}} in {{Social Networks Analysis}} and {{Mining}} 2015 - {{ASONAM}} '15},
  author = {Soliman, Amira and Bahri, Leila and Carminati, Barbara and Ferrari, Elena and Girdzijauskas, Sarunas},
  year = {2015},
  pages = {383--391},
  publisher = {{ACM Press}},
  address = {{Paris, France}},
  doi = {10.1145/2808797.2808861},
  isbn = {978-1-4503-3854-7},
  language = {en}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  year = {2011},
  month = jun,
  pages = {1521--1528},
  publisher = {{IEEE}},
  address = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995347},
  abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
  file = {/home/pepper-jk/Zotero/storage/84K2ZYHA/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf},
  isbn = {978-1-4577-0394-2},
  language = {en}
}

@article{truexDemystifyingMembershipInference2018,
  title = {Towards Demystifying Membership Inference Attacks},
  author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Yu, Lei and Wei, Wenqi},
  year = {2018},
  file = {/home/pepper-jk/Zotero/storage/DQ2ECTLY/Truex et al. - 2018 - Towards demystifying membership inference attacks.pdf;/home/pepper-jk/Zotero/storage/YHQJWR33/1807.html},
  journal = {arXiv preprint arXiv:1807.09173}
}

@article{truexDemystifyingMembershipInference2019,
  title = {Demystifying Membership Inference Attacks in Machine Learning as a Service},
  author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Yu, Lei and Wei, Wenqi},
  year = {2019},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/EZMFLEB5/8634878.html},
  journal = {IEEE Transactions on Services Computing}
}

@inproceedings{truexHybridApproachPrivacyPreserving2019,
  title = {A {{Hybrid Approach}} to {{Privacy}}-{{Preserving Federated Learning}}},
  booktitle = {Proceedings of the 12th {{ACM Workshop}} on {{Artificial Intelligence}} and {{Security}}  - {{AISec}}'19},
  author = {Truex, Stacey and Baracaldo, Nathalie and Anwar, Ali and Steinke, Thomas and Ludwig, Heiko and Zhang, Rui and Zhou, Yi},
  year = {2019},
  pages = {1--11},
  publisher = {{ACM Press}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3338501.3357370},
  isbn = {978-1-4503-6833-9},
  language = {en}
}

@misc{UNSWNB15DataSet,
  title = {The {{UNSW}}-{{NB15}} Data Set Description},
  file = {/home/pepper-jk/Zotero/storage/2HMSCMCC/ADFA-NB15-Datasets.html},
  howpublished = {https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/}
}

@article{vinyalsShowTellNeural2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2015},
  month = apr,
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1411.4555},
  eprinttype = {arxiv},
  file = {/home/pepper-jk/Zotero/storage/D9VZPCQB/Vinyals et al. - 2015 - Show and Tell A Neural Image Caption Generator.pdf;/home/pepper-jk/Zotero/storage/HDQV9UY4/1411.html},
  journal = {arXiv:1411.4555 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{wainakhEnhancingPrivacyHierarchical2020,
  title = {Enhancing {{Privacy}} via {{Hierarchical Federated Learning}}},
  author = {Wainakh, Aidmar and Guinea, Alejandro Sanchez and Grube, Tim and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  file = {/home/pepper-jk/Zotero/storage/34AN5FXV/Wainakh et al. - 2020 - Enhancing Privacy via Hierarchical Federated Learn.pdf;/home/pepper-jk/Zotero/storage/65ZHYSEF/2004.html},
  journal = {arXiv preprint arXiv:2004.11361}
}

@article{wangEavesdropCompositionProportion2019,
  title = {Eavesdrop the {{Composition Proportion}} of {{Training Labels}} in {{Federated Learning}}},
  author = {Wang, Lixu and Xu, Shichao and Wang, Xiao and Zhu, Qi},
  year = {2019},
  file = {/home/pepper-jk/Zotero/storage/U3MUA2W4/Wang et al. - 2019 - Eavesdrop the Composition Proportion of Training L.pdf;/home/pepper-jk/Zotero/storage/YVCXCL95/1910.html},
  journal = {arXiv preprint arXiv:1910.06044}
}

@inproceedings{wangInferringClassRepresentatives2019,
  title = {Beyond Inferring Class Representatives: {{User}}-Level Privacy Leakage from Federated Learning},
  shorttitle = {Beyond Inferring Class Representatives},
  booktitle = {{{IEEE INFOCOM}} 2019-{{IEEE Conference}} on {{Computer Communications}}},
  author = {Wang, Zhibo and Song, Mengkai and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
  year = {2019},
  pages = {2512--2520},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/WDKSZF4G/Wang et al. - 2019 - Beyond inferring class representatives User-level.pdf;/home/pepper-jk/Zotero/storage/QLS2VGQL/8737416.html}
}

@article{wangMatrixSketchingSecure2019,
  title = {Matrix {{Sketching}} for {{Secure Collaborative Machine Learning}}},
  author = {Wang, Shusen},
  year = {2019},
  file = {/home/pepper-jk/Zotero/storage/SIDRQXXJ/Wang - 2019 - Matrix Sketching for Secure Collaborative Machine .pdf;/home/pepper-jk/Zotero/storage/2SA9FMT7/1909.html},
  journal = {arXiv preprint arXiv:1909.11201}
}

@misc{WhenYouTry,
  title = {When You Try to Connect from {{TCP}} Ports Greater than 5000 You Receive the Error '{{WSAENOBUFS}} (10055)'},
  file = {/home/pepper-jk/Zotero/storage/EKCH8JBV/when-you-try-to-connect-from-tcp-ports-greater-than-5000-you-receive-t.html},
  howpublished = {https://support.microsoft.com/en-us/help/196271/when-you-try-to-connect-from-tcp-ports-greater-than-5000-you-receive-t}
}

@misc{WiresharkDisplayFilter,
  title = {Wireshark {$\cdot$} {{Display Filter Reference}}: {{Internet Protocol Version}} 4},
  file = {/home/pepper-jk/Zotero/storage/53QDHFCQ/ip.html},
  howpublished = {https://www.wireshark.org/docs/dfref/i/ip.html}
}

@misc{wroclawskiAssuredForwardingPHB,
  title = {Assured {{Forwarding PHB Group}}},
  author = {Wroclawski, John and Weiss, Walter and Heinanen, Juha and Baker, Fred},
  file = {/home/pepper-jk/Zotero/storage/X3H9YUXB/rfc2597.html},
  howpublished = {https://tools.ietf.org/html/rfc2597},
  language = {en}
}

@inproceedings{xiePrivacyPreservingDistributedMultiTask2017,
  title = {Privacy-{{Preserving Distributed Multi}}-{{Task Learning}} with {{Asynchronous Updates}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Xie, Liyang and Baytas, Inci M. and Lin, Kaixiang and Zhou, Jiayu},
  year = {2017},
  month = aug,
  pages = {1195--1204},
  publisher = {{ACM}},
  address = {{Halifax NS Canada}},
  doi = {10.1145/3097983.3098152},
  isbn = {978-1-4503-4887-4},
  language = {en}
}

@inproceedings{xuHybridAlphaEfficientApproach2019,
  title = {{{HybridAlpha}}: {{An Efficient Approach}} for {{Privacy}}-{{Preserving Federated Learning}}},
  shorttitle = {{{HybridAlpha}}},
  booktitle = {Proceedings of the 12th {{ACM Workshop}} on {{Artificial Intelligence}} and {{Security}}  - {{AISec}}'19},
  author = {Xu, Runhua and Baracaldo, Nathalie and Zhou, Yi and Anwar, Ali and Ludwig, Heiko},
  year = {2019},
  pages = {13--23},
  publisher = {{ACM Press}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3338501.3357371},
  isbn = {978-1-4503-6833-9},
  language = {en}
}

@misc{YAFDocumentation,
  title = {{{YAF}} - {{Documentation}}},
  file = {/home/pepper-jk/Zotero/storage/D45ZA2DI/yafscii.html},
  howpublished = {https://tools.netsa.cert.org/yaf/yafscii.html}
}

@article{yangDefendingModelInversion2020,
  title = {Defending {{Model Inversion}} and {{Membership Inference Attacks}} via {{Prediction Purification}}},
  author = {Yang, Ziqi and Shao, Bin and Xuan, Bohan and Chang, Ee-Chien and Zhang, Fan},
  year = {2020},
  file = {/home/pepper-jk/Zotero/storage/M2ZH6GT7/Yang et al. - 2020 - Defending Model Inversion and Membership Inference.pdf;/home/pepper-jk/Zotero/storage/NMHDPV5Z/2005.html},
  journal = {arXiv preprint arXiv:2005.03915}
}

@article{yangFederatedMachineLearning2019,
  title = {Federated {{Machine Learning}}: {{Concept}} and {{Applications}}},
  shorttitle = {Federated {{Machine Learning}}},
  author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  year = {2019},
  month = feb,
  volume = {10},
  pages = {1--19},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3298981},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  language = {en},
  number = {2}
}

@misc{YouExperienceIssues,
  title = {You Experience Issues with {{UDP}}-Dependent Network Services after You Install {{DNS Server}} Service Security Update 953230 ({{MS08}}-037)},
  file = {/home/pepper-jk/Zotero/storage/GDECZI87/you-experience-issues-with-udp-dependent-network-services-after-you-in.html},
  howpublished = {https://support.microsoft.com/en-us/help/956188/you-experience-issues-with-udp-dependent-network-services-after-you-in}
}

@inproceedings{yuFairnessawareIncentiveScheme2020,
  title = {A {{Fairness}}-Aware {{Incentive Scheme}} for {{Federated Learning}}},
  booktitle = {Proceedings of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Yu, Han and Liu, Zelei and Liu, Yang and Chen, Tianjian and Cong, Mingshu and Weng, Xi and Niyato, Dusit and Yang, Qiang},
  year = {2020},
  month = feb,
  pages = {393--399},
  publisher = {{ACM}},
  address = {{New York NY USA}},
  doi = {10.1145/3375627.3375840},
  isbn = {978-1-4503-7110-0},
  language = {en}
}

@inproceedings{zaremoayediArimaModelNetwork2008,
  title = {Arima Model for Network Traffic Prediction and Anomaly Detection},
  booktitle = {2008 {{International Symposium}} on {{Information Technology}}},
  author = {Zare Moayedi, H. and {Masnadi-Shirazi}, M.A.},
  year = {2008},
  month = aug,
  volume = {4},
  pages = {1--6},
  issn = {2155-899X},
  doi = {10.1109/ITSIM.2008.4631947},
  abstract = {This paper presents the use of a basic ARIMA model for network traffic prediction and anomaly detection. Accurate network traffic modeling and prediction are important for network provisioning and problem diagnosis, but network traffic is highly dynamic. To achieve better modeling and prediction it is needed to isolate anomalies from normal traffic variation. Thus, we decompose traffic signals into two parts normal variations, that follow certain law and are predictable and, anomalies that consist of sudden changes and are not predictable. ARIMA analysis and modeling for network traffic prediction is able to detect and identify volume anomaly or outliers.},
  file = {/home/pepper-jk/Zotero/storage/NL24HUEF/Zare Moayedi and Masnadi-Shirazi - 2008 - Arima model for network traffic prediction and ano.pdf;/home/pepper-jk/Zotero/storage/4T34L5CB/4631947.html},
  keywords = {Autoregressive processes,Correlation,Data models,Mathematical model,Measurement uncertainty,Predictive models,Time series analysis}
}

@inproceedings{zhangPEFLPrivacyenhancedFederated2019,
  title = {{{PEFL}}: {{A}} Privacy-Enhanced Federated Learning Scheme for Big Data Analytics},
  shorttitle = {{{PEFL}}},
  booktitle = {2019 {{IEEE Global Communications Conference}} ({{GLOBECOM}})},
  author = {Zhang, Jiale and Chen, Bing and Yu, Shui and Deng, Hai},
  year = {2019},
  pages = {1--6},
  publisher = {{IEEE}},
  file = {/home/pepper-jk/Zotero/storage/QZYSVFNG/9014272.html}
}

@inproceedings{zhangPrivacyPreservingLearning2005,
  title = {Privacy Preserving Learning in Negotiation},
  booktitle = {Proceedings of the 2005 {{ACM}} Symposium on {{Applied}} Computing  - {{SAC}} '05},
  author = {Zhang, Sheng and Makedon, Fillia},
  year = {2005},
  pages = {821},
  publisher = {{ACM Press}},
  address = {{Santa Fe, New Mexico}},
  doi = {10.1145/1066677.1066865},
  isbn = {978-1-58113-964-8},
  language = {en}
}

@inproceedings{zhengChallengesPrivacyPreservingMachine2019,
  title = {Challenges of {{Privacy}}-{{Preserving Machine Learning}} in {{IoT}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Challenges}} in {{Artificial Intelligence}} and {{Machine Learning}} for {{Internet}} of {{Things}}  - {{AIChallengeIoT}}'19},
  author = {Zheng, Mengyao and Xu, Dixing and Jiang, Linshan and Gu, Chaojie and Tan, Rui and Cheng, Peng},
  year = {2019},
  pages = {1--7},
  publisher = {{ACM Press}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3363347.3363357},
  file = {/home/pepper-jk/Zotero/storage/VC4VC9BG/Zheng et al. - 2019 - Challenges of Privacy-Preserving Machine Learning .pdf},
  isbn = {978-1-4503-7013-4},
  language = {en}
}

@misc{ZoteroYourPersonal,
  title = {Zotero | {{Your}} Personal Research Assistant},
  file = {/home/pepper-jk/Zotero/storage/U8VRWRN9/www.zotero.org.html},
  howpublished = {https://www.zotero.org/}
}

@misc{zsebyRequirementsIPFlow,
  title = {Requirements for {{IP Flow Information Export}} ({{IPFIX}})},
  author = {Zseby, Tanja and Quittek, Juergen and Claise, Benoit and Zander, Sebastian},
  file = {/home/pepper-jk/Zotero/storage/9WQMHRIY/rfc3917.html},
  howpublished = {https://tools.ietf.org/html/rfc3917},
  language = {en}
}


